\documentclass[a4paper]{article}

\def\npart {IB}
\def\nterm {Michaelmas}
\def\nyear {2017}
\def\nlecturer {Dr. A. Keating (amk50@cam.ac.uk)}
\def\ncourse {Linear Algebra}

\include{header}

\begin{document}
\maketitle

\setcounter{section}{-1}
\section{Introduction}

   Linear algebra is an important component of undergraduate mathematics. At the practical level, matrix theory and the related vector-space concepts provide a language and a powerful computational framework for posing and solving important problems. 




Beyond this, elementary linear algebra is a valuable introduction to mathematical abstraction and logical reasoning because the theoretical development is self-contained, consistent, and accessible to most students.



\section{Vector Spaces}
\subsection{Vector Spaces}

\begin{defi}
	An $ \F $-Vector space (a vector space on $ \F $) is an abelian group $ (V, +) $ equipped with a function\footnote{scalar multiplication} $ F \times V \to V $, $ (\lambda,v) \mapsto \lambda V $
	
	\[ \lambda ( v_{1} + v_{2}) = \lambda v_{1} + \lambda v_{2} \]
	
	\[ (\lambda_{1} + \lambda_{2}) v = \lambda_{1} v + \lambda_{2} v \]
	
	\[ \lambda (\mu v) = \lambda \mu v \]
	
	\[ 1v = v \]
	
	\[ v + \mathbf{0} = v \]
	
	for all $ \lambda_{i}, \lambda, \mu \in F $, $ v_{i} \in V $
	
\end{defi}

Note that we will not be underlining our vectors, as this is cumbersome here. We will however be using $ \mathbf{0} $ to denote the zero vector. 

\begin{eg}
	For all $ n \in \N  $, $ \F^{n} = $ space of column vectors of length $ n $, entries in $ \F $. We understand the definition as entry-wise addition, entry-wise scalar multiplication
	  
\end{eg}


\begin{eg}
	$ M_{m,m}(\F) $, the set of $ m \times m $ matrices with entries in $ \F $
	
	\[ \begin{pmatrix}
	a & b \\
	c & d
	\end{pmatrix} + \begin{pmatrix}
	e & f \\
	g & h
	\end{pmatrix} = \begin{pmatrix}
	a + e & b + f\\
	c + g & d + h
	\end{pmatrix} \]
	again all operations defined entry-wise
\end{eg}

\begin{eg}
	For any set $ X $, $ \R^{X} = \{ f : X \to \R \}$
	Addition and scalar multiplication defined pointwise $ = f_{1}(x) + f_{2} (x) $.
\end{eg}


\begin{ex}
	Show that the above examples satisfy the axioms
\end{ex}

\begin{prop} 
	$ 0 v = \mathbf{0} $ for all $ v \in V $.
\end{prop}

\begin{proof}
	( $ (0 + 0)v = 0v \iff 0 v + 0v = 0v \iff 0v = \mathbf{0} $)
\end{proof}

\begin{ex}
	Show\footnote{Hint: Use the previous proposition} that $ (-1)v = -v $
\end{ex}

\begin{defi}
	Let V be an $ \F $-vector space. A subset $ U $ of $ V $ is a subspace ( $ U \leq V $) if: 
	\begin{enumerate}
		\item $ \mathbf{0} \in U $
		\item $ u_{1}, u_{2} \in U  \Rightarrow u_{1} + u_{2} \in U $ ``$ U $ is closed under addition...''
		\item $ u \in U $, any $ \lambda \in \F \Rightarrow \lambda u \in U$ ``...and scalar multiplication''
	\end{enumerate}
\end{defi}


\begin{ex}
	If $ U $ is a subspace of $ V $, then $ U $ is also an $ \F $-vector space.
\end{ex}

\begin{eg}
	Let $ V = \R^{\R} $, then $ f : R \to R $. The set of all continuous functions $ C(\R) $ are a subspace. An even smaller subspace is the set of all polynomials.
\end{eg}


\begin{ex} Define $ U \subseteq R^{3} $ as: 
	\[ \left\{   \begin{pmatrix}
a_{1} \\
a_{2} \\
a_{3}
\end{pmatrix} \; \; a_{1} + a_{2} + a_{3} = t \right\} \]
for some constant  $ t $. 
Check that this is a subspace of $ \R^{3} $ if and only if $ t = 0 $.
\end{ex}

\begin{prop} 
	Let $ V $ be an $ F $-vector space, $ U,W \leq V $. Then $ U \cap W \leq V $.	
\end{prop}

\begin{proof}
	\begin{enumerate}
		\item 	$ 0 \in U $, $ 0 \in W  \Rightarrow 0 \in U \cap W $ 
		\item Suppose $ u,v \in U \cap W $, $ \lambda, \mu \in F $.
		$ U $ is a subspace $ \Rightarrow  \lambda u + \mu v \in W $. Similarly $ \lambda u + \mu v \in U \in W $, so it is in the intersection. 
	\end{enumerate}
\end{proof}

\begin{eg}
	$ V = \R^{3} $, $ U = \left\{ \begin{pmatrix}
	x\\
	y\\
	z
	\end{pmatrix} \; | \; x = 0 \right\} $, $ V = \left\{ \begin{pmatrix}
	x\\
	y\\
	z
	\end{pmatrix} \; | \; y = 0 \right\}  $ then $ U \cap W = U = \left\{ \begin{pmatrix}
	x\\
	y\\
	z
	\end{pmatrix} \; | \; x = 0, y= 0 \right\}  $ (intersect along the $ z $-axis)
\end{eg}

Note: union of family of subspaces is almost never a subspace itself.


\begin{defi}
	Let $ V $ be an $ F $-vector space, $ U,W \leq V $. The \emph{sum} of $ U $ and $ W $ is the set:
	
	\[ U + W = \left\{  u + w \; | u \in U, w \in W    \right\}  \]
\end{defi}

\begin{prop} 
	$ U + W \leq V $
\end{prop}

\begin{proof}
	$ \mathbf{0} \in U,W \Rightarrow \mathbf{0} + \mathbf{0} = \mathbf{0} \in U + W $
	
	$ u_{1},u_{2} \in U $, $ w_{1},w_{2} \in W $, 
	
	\[ (u_{1} + w_{1}) + (u_{2} + w_{2}) = \underbrace{(u_{1} + u_{2})}_{\in U}  + \underbrace{(w_{1} + w_{2})}_{\in W} \]
	
	Similarly for scalar multiplication (ex.)
\end{proof}

Note: $ U + W $ is the smallest subspace containing both $ U $ and $ W $. (This is becaues all elements of the form $ u + w $ ae forced to be in such a subspace by the ``closed under addition'' axiom)


\begin{defi}
	$ V $ is an $ \F $-vector space, $ U \leq V $. The quotient space\footnote{think of this as the collection of cosets of $ U $ in $ V $ } $ V / U $ is the abelian group $ V / U $ equipped with scalar multiplication;
	
	\[ F \times V / U \to V / U \]
	
	\[ (\lambda, v + U) \mapsto \lambda v + U \]
\end{defi}

\begin{prop} 
	This is well-defined, and $ V/U $ is an $ F $-vector space.
\end{prop}

\begin{proof}
	Well-defined: Suppose $ v_{1} + U = v_{2} + U \in V / U $. $ \Rightarrow (v_{2} - v_{1}) \in U \Rightarrow ( \lambda v_{2} - \lambda v_{1}) \in U \Rightarrow \lambda v_{2} + U = \lambda v_{1} + U \in V / U $

To show that it is an $ \F $-vector space, we must show that the axioms hold. These follow from the axioms of $ V $.
$ \lambda ( \mu (v + U)) = \lambda ( \mu v + U) = \lambda(u v) + U = (\lambda u) v + U  = \lambda u (v \in U) $ (scalar multiplication on $ V / U $). 

Ex. Other axioms follow similarly from using vecton space axioms

\end{proof}

\subsection{Bases}

\begin{defi}
	$ V $ is an $ \F $-vector space, $ S \subset V $. The \emph{span} of $ S $ is denoted by 
	
	\[ <S> = \left\{ \sum_{s \in S}  \lambda_{s} s \; | \; \lambda_{s} \in \F \right\}  \]
	
	ie. the set of all finite linear combinations, all but finitely many of the $ \lambda_{s} $ are zero.
\end{defi}

Remark: $ <S> $ is the smallest subspace of $ V $ which contains\footnote{This is essentially a tautology} all of the elements of $ S $

Convention: $  < \emptyset > = \{ \mathbf{0} \} $.

\begin{eg}
	$ V = \R^{3} $, 
	
	\[ S = \left\{ \begin{pmatrix}
	1 \\
	0 \\
	0
	
	\end{pmatrix}, \begin{pmatrix}
	0\\
	1\\
	2
	
	\end{pmatrix}, \begin{pmatrix}
	3\\
	-2\\
	-4
	\end{pmatrix} \right\}  \]
	
	
	\[ <S> = \{ \begin{pmatrix}
	a\\
	b\\
	2b
	\end{pmatrix} \} \; | \; a,b \in \R \]
	
	ie. we have took linear combinations of the first two. We don't need the third one.
	
\end{eg}



\begin{eg}
	For $ X $ a set, define $ \delta_{x}(y) : X \to \F $ as 
	
	
	\[ \delta_{x}(y) = \begin{cases} 1  & \text{ if } x = y \\ 0  & \text{ if } x \neq y \end{cases}  \]
	
	\[ < \delta_{x} \; | \; x \in X > = \{  f \in \R^{X} \; | \; f \text{ has finite support} \}  \]
	
	\[ = <  x \in X \; | \; f(x) \neq 0 > \]
\end{eg}

\begin{defi}
	$ S $ \emph{spans} $ V $ if $ <S> = V $
\end{defi}

\begin{defi}
	$ V $ is \emph{finite dimensiona}l over $ \F $ if it is spanned by a set that is finite.
\end{defi}

\begin{defi}
	The vectors $ v_{1},\cdots,v_{n} $ are \emph{linearly indepedent} over $ \F $ if 
	
	\[ \sum_{i = 1}^{n} \lambda_{i} v_{i} = 0 \Rightarrow \lambda_{i} \text{ for all } i \]
	
	some coefficients $ \lambda_{i} \in \F  $. $ S \subset V $ is linearly independent if every finite subset of it is. 
\end{defi}

\begin{eg}
	The first example, $ u,v,w $ are not linearly independent\footnote{If \emph{not} linearly independent, say a set is linearly dependent.}, but the set $ \{  \delta_{X} \; | \; x \in X \} $ is linearly independent.
	
\end{eg}


A lesson to be learnt from our example is that a linearly dependent spanning set contains redundant information. In a sense, a linearly independent spanning set is a minimal spanning set and hence represents the most efficient way of characterizing the subspace. This idea leads to the following definition.

\begin{defi}
	$ \mathcal{B} $ is a \emph{basis} of $ V $ if it is linearly independent and spans $ V $
\end{defi}


\begin{eg}
	\begin{itemize}
	
	
	
	\item $ \F^{n}  $  standard basis: $ \left\{     e_{1},e_{2},\cdots,e_{n} \right\} $.

	\item $ V = \C $ over $ \C $ has natural basis $ \{ 1\} $, over $ \R $ has natural basis $ \{ 1,i \} $

	\item $ V = \mathcal{P}(\R) $ space of all polynomials, has natural basis 
	
	\[ \{ 1,x,x^{2},x^{3},\cdots \} \]
	
	
		
\end{itemize}
\end{eg}

\begin{ex}
	Check this carefully 
\end{ex}


\begin{lemma} 
	$ V $ is an $ \F $-vector space. The vectors $ v_{1}, \cdots, v_{n} $ form a basis of $ V $ iff each vector $ v \in V $ has a unique expression
	
	\[ v = \sum_{i=1}^{n} \lambda_{i}v_{i}, \text{ with } \lambda_{i} \in \F \] 
	
\end{lemma}

\begin{proof}
	$ (\Rightarrow) $ Fix $ v \in V$. The $ v_{i} $ span, so 
	
	\[ \exists \lambda_{i} \in \F \text{ s.t. } v = \sum \lambda_{i} v_{i} \]
	
	Suppose also $ v = \sum \mu_{i} v_{i} $ for some $ \mu_{i} \in \F $. $ \sum \left( \mu_{i} - \lambda_{i} \right)  v_{i} = \mathbf{0} $.
	
	The $ v_{i} $ are linearly independent so $ \mu_{i} - \lambda_{i} = 0 $ for all $ i $, $ \lambda_{i} = \mu_{i} $
	
	
	$ (\Leftarrow) $ The $ v_{i} $ span $ V $, since any $ v \in V $ is a linear combination of them.
	IF $ \sum_{i=1}^{n} \lambda_{i} v_{i} = \mathbf{0} $. Note that $ \mathbf{0} = \sum_{i=1}^{n} 0 v_{i} $. By uniqueness (applied to $ \mathbf{0} $), $ \lambda_{i}  = 0 $ for all $ i $. 
\end{proof} 


\begin{lemma} 
	If $ v_{1},\cdots,v_{n} $ span $ V $ (over $ \F $), then some subset of $ v_{1},\cdots,v_{n} $ is a basis for $ V $ (over $ \F $).	
\end{lemma}

\begin{proof}
		If $ v_{1},\cdots,v_{n} $ linearly independent, done.
		Otherwise for some $ l $, there exist $ \alpha_{1},\cdots,\alpha_{l-1} \in \F $ such that
		
		\[ v_{l} = \alpha_{1} v_{1} + \cdots + \alpha_{l-1}v_{l-1} \]
		
		
		( If $ \sum \lambda_{i} v_{i} = \mathbf{0}$, not all $ \lambda_{i} = 0 $. Take $ l $ maximaml with $ \lambda_{i} \neq  0$, just $ \alpha_{i} = - \lambda_{i} / \lambda_{l} $ ).
		
		Now $ v_{2}, \cdots, v_{l-1},v_{l+1},\cdots,v_{n} $ still span $ V $. Continue interatively until get linear independence. 
		
\end{proof}

\begin{thm} (Steinitz exchange lemma)
	Let $ V $ be a finite dimensional vector space over $ \F $. Take $ v_{1},\cdots,v_{m} $ to be linearly independent $ w_{1},\cdots,w_{n} $ to span $ V $. 
	
	Then $ m \leq n $, and reordering the spanning set if needed,
	
	\[ v_{1},\cdots, v_{m}, w_{m+1},\cdots,w_{n} \] span $ V $.
	
\end{thm}

\begin{proof} (Induction)
	Suppose that we've replaced $ l (\geq 0)$ of the $ w_{i} $. Reordering the $ w_{i} $ if needed, $ v_{1},\cdots,v_{l},w_{l+1},\cdots,w_{n} $ span $ V $. 
	
	If $ l = m $, done.
	
	If $ l < m $, then
	
	\[ v_{l+1} = \sum_{i=1}^{l} \alpha_{i} v_{i}  + \sum_{i > l} \beta_{i} w_{i} \]
	
	$ \alpha_{i}, \beta_{i} \in \F $. As the $ v_{i} $ are lin. indep, $ \beta_{i} \neq 0 $ for some $ i $. (After reordering, $ \beta_{l+1} \neq 0 $).
	
	\[ w_{l+1} = \frac{1}{\beta_{l+1}} \left( v_{l+1} - \sum_{i \leq l} \alpha_{i} v_{i}  - \sum_{i > l+1} \beta_{i} w_{i} \right)  \]
	
	This $ v_{1},\cdots,v_{l+1},w_{l+2},\cdots,w_{n} $ also spans $ V $. After $ m $ steps, $ w_{i} $ will have replaced $ m $ of the $ w_{i} $ by $ v_{i} $. Thus $ m \leq n $.
\end{proof}


\begin{thm} 
	If $ V $ is a finite dimensional vector space over $ \F $, then any two bases for $ V $ have the same number of elements. This is what we call the \emph{dimension} of $ V $, denoted $ \dim_{\F} V $.
\end{thm}

\begin{proof}
	If $ \{  v_{1},\cdots,v_{n} \} $ is a basis and $ w_{1},\cdots,w_{m} $ is another basis, the $ \{ v_{i} \} $ span and $ \{ w_{i} \} $ is linearly indepnedent' so by Steinitz $ m \leq n $. Likewise, $ n \leq m $.
\end{proof}


\begin{eg}
	$ \dim_{\C} \C = 1 $, $ \dim_{\R} \C = 2 $
\end{eg}

\begin{thm} 
	$ V $, finite dim, $ v $-space over $ \F $. If $ w_{1},\cdots,w_{l}$ is a linearly indepnedent set of vectors, we can extend it to a basis $ w_{1},\cdots,w_{l},v_{l+1},\cdots,v_{n} $
\end{thm}


\begin{proof}
	Apply Steinitiz to $ w_{1},\cdots,w_{l} $ (lin indep) and any basis $ v_{1},\cdots,v_{n} $.
	
	Or directrly, if $ V = <w_{1},\cdots,w_{l} > $, stop.
	
	Otherwise take $ v_{l+1} \in V \; \setminus <w_{1},\cdots,w_{l} > $, now $ w_{1},\cdots,w_{l},v_{l+1} $ is linearly indep. iterate
\end{proof}


\begin{cor} 
	Suppose $ V $ is a finite dimensional vector space, with dimension $ n $.
	
	\begin{enumerate}
		\item Any linearly independent set of vectors has at most $ n $ elements with equality iff it's a basis
		\item Any spanning set of vectors must have at least $ n $ elements, with equality if and only if it's a basis.
		
	\end{enumerate}
\end{cor}

Slogan ``Choose the best basis for the job''

\begin{thm} 
	Let $ U,W $ be subspaces of $ V $. If $ U $, $ W $ are finite dim, so is $ U + W $ and $ \dim (U + W)   = \dim U + \dim W - \dim (U \cap W)  $
\end{thm}

\begin{proof}
	Pick basis basis $ v_{1},\cdots,v_{l} $ of $ U \cap W $. Extend it to basis $ v_{1},\cdots,v_{l},u_{1},\cdots,u_{m} $ of $ U $.
	Extend it to basis $ v_{1},\cdots,v_{l},w_{1},\cdots,w_{n} $ of $ W $.
	
	Claim: $ v_{1},\cdots,v_{l},u_{1},\cdots,u_{m},w_{1},\cdots,w_{n} $ is a basis for $ U + W $.
	\begin{enumerate}
		\item Span: $ u \in U $, then $ u = \sum \alpha_{i} v_{i}  + \sum _{\beta_{i} u_{i}} $, $ \alpha_{i},\beta_{i} \in \F $
		$ w \in W $, then $ w = \sum \gamma_{i} v_{i}  + \sum _{\delta_{i} w_{i}} $, $ \gamma_{i},\delta_{i} \in \F $
		
		\[ u + w  = \sum  (\alpha_{i} + \gamma_{i})v_{i}   + \sum (\beta_{i} + \delta_{i} )u_{i} \]
		
		\item lin indep: $ u = \sum \alpha_{i} v_{i}  + \sum _{\beta_{i} u_{i}} + \sum \gamma_{i} w_{i} = \mathbf{0} $
		
		\[ \Rightarrow  u =  \underbrace{\sum \alpha_{i} v_{i}  + \sum \beta_{i} u_{i}}_{\in U} = \underbrace{- \sum \gamma_{i} w_{i} }_{\in W}  \in U \cap W  \]
		
		This is equal to $ \sum \delta_{i} v_{i} $ for some $ \delta_{i} \in \F $ because $ v_{i} $ are basis for $ U \cap W $.
		
		AS $ v_{i} $ and $ w_{i} $ are lin indep, $ (*) \Rightarrow \gamma_{i} = \delta_{i} = 0 $ for all $ i $.
		
		$ \Rightarrow \sum \alpha_{i} v_{i} + \sum \beta_{i} u_{i} = 0 \Rightarrow \alpha_{i} = \beta_{i} = 0  $ because $ v_{i} $ and $ u_{i} $ rom a basis for $ U $.
		
		
 		
	\end{enumerate}
\end{proof}


\begin{thm} 
	Let $ V $ be a finite dim $ \F $-vector space, $ U \leq V $, then $ U $ and $ V / U $ are also of finite dim, and
	
	\[ \dim V = \dim U + \dim V / U \]
\end{thm}

\begin{proof}
	\begin{ex}
		Show that $ U $ is finite dim.
	\end{ex}


Let $ u_{1},\cdots,u_{l} $ be a basis for $ U $. Extend it to a basis for $ V $. Say $ u_{1},\cdots,u_{l},w_{l+1},\cdots,w_{n} $ of $ V $.
\begin{ex}
	Check: $ w_{l+1} + U, \cdots, w_{m} + U $ form a basis for $ V / U $.
\end{ex}

\end{proof}

\begin{cor} 
	If $ U $ is a proper subspace of $ V $, $ V $ is finite dimensional, $ \dim U < \dim V $.
	
	\begin{proof}
		$ V / U \neq \{ \mathbf{0} \} \Rightarrow \dim V/U  > 0 \Rightarrow \dim U < \dim V$
	\end{proof}
\end{cor}

\begin{defi}
	Let $ V $ be an $ \F $-vector space, $ U,W \leq V $
	Then $ V = U + \oplus W $ ($ V $ is an internal direct sum of $ U $ and $ W $) if every element of $ V $ can be written as $ v  = u + w, w \in W, u \in U $, uniquely.
	
	$ W $ is a \emph{direct compliment} of $ U $ in $ V $
\end{defi}

\begin{lemma} 
	$ U,W \leq V $. The following are equivalent
	
	\begin{enumerate}
		\item $ V = U \oplus W $, ie. every element of $ V $ can be written uniquely as $ u + w, $ for  $u \in U, w \in W $
		\item $ V = U + W $ and $ U \cap W = \{ \mathbf{0} \} $
		\item $ B_{1} $ any basis of $ U $, $ B_{2} $ is any basis of $ W $, then $ B = B_{1} \cup B_{2} $ is a basis of $ V $.
		
	\end{enumerate}
\end{lemma}


\begin{proof}
	(ii) $ \Rightarrow $ (i). Any $ v \in V $ is $ u + w $ for some $ u \in U $, $ w in W $.
	
	Suppose that 
	

	\[ u_{1} + w_{1} = u_{2} + w_{2} \]
	
	
	Then 
	\[ \Rightarrow u_{1} - u_{2} = -w_{1} + w_{2} \in U \cap W = \{ \mathbf{0} \} \Rightarrow w_{1} = w_{2}, u_{1} = u_{2} \]
	
	Thus uniqueness of expressions.
	
	
	(i) $ \Rightarrow $ (iii) $ B $ spans, any $ v \in V $ is $ u + w $, for some $ u \in U $, $ w \in W $, write $ u $ in terms of $ B_{1} $, $ w $ in terms of $ B_{2} $, Then $ u + w $ is a lin comb. of elements of $ B $.
	
	$ B $ indep? \[ \sum_{v \in B} \lambda_{v} v = \mathbf{0} = \mathbf{0}_{v} + \mathbf{0}_{w} \]
	
	\[ \underbrace{\sum_{v \in B_{1}} \lambda_{v} v}_{\in U} + \sum_{v \in B_{2}} \lambda_{v} v \]
	
	%under, in U, W
	
	By uniqueness of expressions, 
	
	\[ \sum_{v \in B_{1}} \lambda_{v} v = \mathbf{0}_{U} \qquad \sum_{v \in B_{2}} \lambda_{v} v = \mathbf{0}_{W} \]
	
	AS $ B_{1} $ and $ B_{2} $ are basis, all of the $ \lambda_{v} $ are zero. 
	
	
	(iii) $ \Rightarrow $ (ii). If $ v \in V $, $ v = \sum_{x \in B}  \lambda_{x} x = \underbrace{\sum_{u \in B} \lambda_{u} u }_{\in U} + \underbrace{\sum_{w \in B_{2}}   \lambda_{w} w }_{\in W}  $
	
	$ \Rightarrow v \in U + W $.
	
	
	If $ v \in U \cap W $, $ v = \sum_{u \in B_{1}}  \lambda_{u} $ , $u = \sum_{w \in B_{2}}  \lambda_{w} w  \Rightarrow $ All $ \lambda_{u},\lambda_{w} $ are zero, because $ B_{1} \cup B_{2}  $ is lin. indep.
	
\end{proof}
	
	\begin{lemma} 
		Let $ V $ be an f-dim vector space. $ U \leq V $.
		Then there exists a direct compliment to $ U $ in $ V $
		
	\end{lemma}

\begin{proof}
	Let $ u_{1},\cdots,u_{l} $ be a basis for $ U $. Extend it to a basis for $ V $, 
	
	\[ u_{1},\cdots,u_{l},w_{l+1},\cdots,w_{n} \]
	
	Then $ <w_{l+1},\cdots,w_{n}>$ is a direct compliment of $ U $. 
\end{proof}

Note! Direct compliments are not at all unique. In general, if you pick different ways of extending this you will get different direct compliments. 

Pick $ V = \R^{2} $. Pick $ U $ as the $ y $-axis, then any one of the following green lines are direct compliments.:

\begin{defi}
	Def $ v_{1},\cdots,v_{l} \leq V $,
	
	\[ \sum V_{i} = V_{1} + \cdots + V_{l} = \{  v_{1} + \cdots + v_{l} \; | \; v_{i} \in V_{i} \} \]
	
	The sum is direct if
	
	\[ v_{1} + \cdots + v_{l} = v_{1}' + \cdots + v_{l}' \Rightarrow v_{i} = v_{i}' \text{ for all} l \] (``unique expressions'')
	
	Notation:
	
	\[ \bigoplus_{i=1}^{l} V_{i} \]
\end{defi}
	
\begin{ex}
	$ V_{1},\cdot,V_{l} \leq V $. TFAE
	
	\begin{enumerate}
		\item The sum $ \sum V_{i} $ is direct
		\item $ V_{i} \cap \sum_{j \neq i}   V_{j} = \{ \mathbf{0} \} $ for all $ i $
		\item The $ B_{i} $ are pairwise disjoint and their union is a basis for  $ \sum V_{i}  $
		
	\end{enumerate}


\end{ex}


We also discuss \emph{external} direct sums, though will not touch them much in this course. This is simply an internal direct sum $ U_{1} \oplus U_{2} $, except now the $ U_{i} $'s are not subspaces of $ V $, they can be any old vector space. 


\begin{defi}
	Let $ U,W $ be $ \F $-vector spaces.
	External direct sum
	
	\[ U \oplus V  = \{  (u,w) \; | \; u \in U, w \in W \}\]
	
	with $ (u,w) + (x,y) = (u + x, w + y) $,
	
	$ \lambda(u,w) = (\lambda u, \lambda w) $
\end{defi}

Note that when we talk about dimension in this course, we have not shown yet that the dimension of an \emph{infinite} vector space is well defined\footnote{It is!}. We will come to this later.



\section{Linear Maps}

\subsection{Linear Maps}

\begin{defi}
	$ V,W $ are $ \F $-vector spaces. A map $ \alpha: V \to W $ is linear if
	
	\begin{enumerate}
		\item $ \alpha(v_{1} + v_{2}) = \alpha(v_{1}) + \alpha(v_{2}) $
		\item $ \alpha(\lambda v) = \lambda \alpha(v) $
		
	\end{enumerate}

Can be combined concisely as: 

\[  \alpha(\lambda_{1} v_{1} + \lambda_{2} v_{2}) = \lambda_{1} \alpha(v_{1}) + \lambda_{2} \alpha(v_{2})  \quad \lambda_{i} \in \F, v_{i} \in V  \]
\end{defi}

\begin{eg}
	A $ n \times m $ matrix with coeff in $ \F $
	
	\[ \alpha: \F^{n} \to \F^{m} \]
	\[ v \mapsto A v \]
\end{eg}

\begin{eg}
	The set of all polynomials with real coefficients:
	\[ \mathcal{D} : \mathcal{P}(\R) \to \mathcal{P}(\R) \]
	
	\[ f \mapsto \frac{\d f}{\d x} \]
\end{eg}


\begin{eg}
	The set of continuous functions over $ [0,1] $
	\[ I: \mathcal{C}[0,1] \to \mathcal{C}[0,1] \]
	\[ f \mapsto I(f) \]
	
	where $ I(f)(x) = \int_{0}^{x} f(t) \; \d t $
\end{eg}

\begin{eg}
	Fix $ x \in [0,1] $
	
	\[ \mathcal{C}[0,1] \to \R \]
	\[ f \mapsto f(x) \]
\end{eg}

Notes: If $ U,V,W $ are v spaces over $ \F $, then

\begin{enumerate}
	\item The identity map id: $ V \to V $ is linear
	\item If \begin{tikzcd}
		U \ar[r, "\alpha"] & V \ar[r, "\beta"] & W 
	\end{tikzcd} with $ \alpha,\beta $ both linear, then $ \beta \circ \alpha $ is linear.
	
\end{enumerate}

\begin{lemma} 
	Let $ V,W $ be $ \F $-vector spaces, and let $ \mathcal{B} $ is a basis for $ V $.
	If $ \alpha_{0} : \mathcal{B} \to W $ is \emph{any} map, then there exists a unique linear map\footnote{ie. if I tell you \emph{any} mapping of the basis vectors $ \alpha_{0} $ (it could be a non-linear mapping), then you have enough information to construct a linear map from this. } $ \alpha : V \to W $ extending $ \alpha_{0} $, ie. 
	
	\[ \alpha(v) = \alpha_{0}(v) \]
	
	for any basis element $ v \in \mathcal{B} $. 

	
\end{lemma}


\begin{proof}
	Let $ v  \in V $. Then $ v = \sum \lambda_{i} v_{i} $, $ v_{i} \in B $, $ \lambda_{i} \in \F $, unique expression. 
	
	Now Linearity forces
	
	\begin{align*}
	\alpha(v) & = \alpha\left( \sum \lambda_{i} v_{i} \right) \\
	& = \sum \lambda_{i} \alpha(v_{i})\\
	& = \sum \lambda_{i} \alpha_{0}(v_{i}) \\
	\end{align*}
	
	linear, exists.
	expression forced to be unique. 
\end{proof}

Note

\begin{enumerate}
	\item True for infinite dimensional vector space also
	\item Very often, to define a linear map, define it on a basis and `extend linearly'
	\item Let $ \alpha_{1},\alpha_{2} : V \to W $ be linear maps. If they agree on any basis, then they are equal. 
\end{enumerate}


\begin{defi} (Isomorphism)
	
	Let $ V,W $ be vector spaces over $ F $. The map $ \alpha : V \to W $ is an \emph{isomorphism} if it is linear and bijective. 
	Notation: $ V \simeq W $
\end{defi}

\begin{lemma} 
	$ \simeq $ is an equivalence notation on the set (score out set and write class) of all vector spaces over $ \F $. That is,
	
	\begin{enumerate}
		\item $ i_{V} : V \to V $ is an iso
		\item If $ \alpha : V \to W $ is an iso, then the inverse map $ \alpha^{-1} : W \to V $ is also linear, hence an iso.
		\item If 
		
		
		
		 \begin{tikzcd}
			U \ar[r, "\beta"] & V \ar[r, "\alpha"] & W 
		\end{tikzcd}
		
	
		then 
		
		\begin{tikzcd}
			U \ar[r, "\beta \circ \alpha"] & W  
		\end{tikzcd}
		
		is also an iso 
		
	\end{enumerate}
	
\end{lemma}

\begin{proof}
	\begin{enumerate}
		\item immediate
		\item $ \alpha $ bijective $ \Rightarrow \alpha^{-1} $ exists. Check: linear. 
		$ w_{2} \in W, w_{2} = \alpha(v_{2}) $, $ v_{2} \in V $, unique.
		$ \alpha^{-1}(w_{1} + w_{2}) = \alpha^{-1}(\alpha(v_{1}) + \alpha(v_{2})  = \alpha^{-1} (\alpha(v_{1} + v_{2})) = v_{1} + v_{2} = \alpha^{-1}(w_{1})  + \alpha^{-1}(w_{2}) $.
		
		Similarly, $ \lambda \in \F $, $ w \in W $, 
		
		\[ \alpha^{-1} (\lambda w) = \lambda \alpha^{-1} (w) \]
		
		\item immediate
		
	\end{enumerate}
\end{proof}



\begin{thm} 
	If $ V $ vector space over $ \F $ of dimension $ n $, then $ V \simeq \F^{n} $.
	
\end{thm}

\begin{proof}
	Choose a basis $ \mathcal{B} $ for $ V $, say $ v_{1},\cdots,v_{n} $
	
	\[ V \to \F^{n} \]
	
	\[ \sum \lambda_{i}v_{i} \mapsto \begin{pmatrix}
	\lambda_{1} \\
	\vdots \\
	\lambda_{n}
	\end{pmatrix} \text{ is an iso}\]
\end{proof}

Remark: Choosing an iso $ V \simeq F^{n} $ is equivalent to choosing a basis for $ V $.

\begin{thm} 
	$ V,W $ $ v $ spaces over $ \F $, finite dim, are isomorphic iff they have the same dimension 
\end{thm}

\begin{proof}
	($ \Leftarrow $) Both $ V $ and $ W $ are isomorphic 
	
	\[ \F^{\dim V} = \F^{\dim W} \]
	
	($ \Rightarrow $) Let $ \alpha : V \to W $ iso, $ \mathcal{B} $ a basis for $ V $.
	
	Claim: $ \alpha(\mathcal{B}) $ is a basis for $ W $. 
	
	
	\begin{ex}
		$ \alpha(\mathcal{B}) $ spans $ W $ because of surjectivity of $ \alpha $.
	\end{ex}
	
	\begin{ex}
		$ \alpha(\mathcal{B}) $ lin indep: follows from injectivity of $ \alpha $.
	\end{ex}
\end{proof}

\begin{defi} (Null space/ Kernel of a linear map)
	Let $ \alpha : V \to W $ be a linear map,the \emph{null space} of $ \alpha $ is given by 
	\[ N(\alpha) = \ker \alpha = \{  v \in V \; | \; \alpha(v) = \mathbf{0} \} \leq V \]
\end{defi}	
	
\begin{defi} (Image of a linear map)
			Let $ \alpha : V \to W $ be a linear map, the \emph{image} of $ \alpha $ is defined as:
			  \[ \Im(\alpha) = \{ w \in W \; | \; w = \alpha(v), \text{ some } v \in V  \} \leq W   \]
\end{defi}


\begin{defi} (Injective map)
	$ \alpha $ is injective if and only if $ N(\alpha) = \{ \mathbf{0} \} $
\end{defi}

\begin{defi} (Surjective map)\footnote{I mean, all definitions are iff statements really. Sometimes we leave it out and just use `if'}
$ 	\alpha $  is surjective if and only if $ \Im(\alpha) = W $
\end{defi}

\begin{eg}
	Let $ \alpha : {\mathcal{C}^{\infty}} (\R) \to \mathcal{C}^{\infty} (\R) $ be defined by 
	
	\[ \alpha(f)(t) = f''(t) + 2f'(t) - 5f  \] 
	
	
	\[  \ker \alpha \text{ is solns to } f'' + 2f' + 5f = 0 \]
	
	\[ g \in \Im \alpha \text{ if } \; \exists \text{ soln} f \text{ to } f'' + 2f' + 5f = g  \]
\end{eg}

\subsection{The First Isomorphism Theorem}

\begin{thm} (First Isomorphism Theorem)
	Let $ \alpha : V \to W $ be a linear map. It induces an iso :
	
	\begin{center}
		\begin{tikzcd}
			V/ \ker \alpha  \ar[r, "\overline{\alpha}"] & \Im(\alpha) 
		\end{tikzcd}
	\end{center}

	defined by
	
	
	\[ \overline{\alpha} (v + \ker \alpha) = \alpha(v) \]
	
	
\end{thm}

\begin{proof}
	\begin{enumerate}
		\item  $ \overline{\alpha} $ is well defined:
	
	\begin{align*}
	& v + \ker \alpha = v' + \ker \alpha \\
	\iff &  v - v' \in \ker \alpha \Rightarrow \alpha(v) \\
	\Rightarrow &  \alpha(v) = \alpha(v')
	\end{align*}
	
	\item $ \overline{\alpha} $ is linear; immediate from linearity of $ \alpha $.
	
	\item $ \overline{\alpha} $ bijective? 
	
	\begin{align*}
	& \overline{\alpha}(v + \ker \alpha  ) = \mathbf{0} \\
	\Rightarrow & \alpha(v) = 0 \\
	\Rightarrow &  v \in \ker \alpha 
	\end{align*}
	
	
	\item surjective: by defn of $ \Im(\alpha) $.		
	\end{enumerate}


	
	
\end{proof}


\begin{defi} (Rank and Nullity of a linear map)
	The \emph{rank} of a linear map $  r(\alpha) = rk(\alpha) $ is given by $ \dim (\Im \alpha)  $, and the \emph{nullity} $ n(\alpha) $ is likewise given as $ \dim(N(\alpha)) $
\end{defi}

\begin{thm} (Rank-nullity theorem)
	Let $ U,V $ be vector spaces over $ \F $, $ \dim_{\F} U < \infty $.Let $ \alpha : U \to V $ linear. Then:
	
	\[ \dim U = r(\alpha) + n(\alpha) \]
\end{thm}


\begin{proof}
	\[ U / \ker \alpha \simeq \Im (\alpha) \Rightarrow \dim(U) - \dim (\ker \alpha) = \dim (\Im(\alpha) )\]
\end{proof}

\begin{lemma} 
	Let $ V,W $ be v spaces over $ \F $, of equal finite dim. Let $ \alpha : V \to W $ linear. 
	
	TFAE
	
	\begin{enumerate}
		\item $ \alpha $ injective
		\item $ \alpha $ surjective
		\item $ \alpha $ isomorphism
		
	\end{enumerate}
\end{lemma}

\begin{defi}
	The \emph{space of linear maps} from $ V $ to $ W $ is denoted by 
	
	\[ L(V,W) = \{ \alpha : V \to W \text{ linear}\} \]
\end{defi}

\begin{prop} 
	$ L(V,W) $ is a v-space over $ \F $ under operators
	
	\begin{itemize}
		\item $ (\alpha_{1} + \alpha_{2})(v) = \alpha_{1} (v) + \alpha_{2} v $ for all $ \alpha_{i} \in L(V,W) $
		\item $ (\lambda \alpha  ) (v) = \lambda ( \alpha (v)) $ for all $ v \in V $, $ \lambda \in \F $
	\end{itemize}


If both $ V $ and $ W $ are finite dim, then so is $ L(V,W) $ and $ \dim (L(V,W))  = \dim(V) \times \dim(W) $.

\end{prop}

\begin{proof}
	$ \alpha_{1} + \alpha_{2} $, $ \lambda \alpha $ defined above are well-defined linear maps. The v-space axioms are satisfied.
	
	Claim about finite dim: See later
\end{proof}

\subsection{Representation of Linear Maps by Matrices }


\begin{defi}
	An $ m \times n $ matrix over $ \F $ is an array with $ m $ rows and $ n $ columns, entries in $ \F $.
	
	\[ A = (a_{ij}), \quad a_{ij} \in F, \quad 1 \leq i \leq m, 1 \leq j \leq n \]
	
	$ M_{m,n}(\F) $ is the set of all such matrices
\end{defi}

\begin{prop} $ M_{m,n}(\F) $ is an $ \F $ vector space, under operations
	
	\item $ (a_{ij}) + (b_{ij}) = (a_{ij} + b_{ij}) $
	\item $ \lambda(a_{ij}) = (\lambda a_{ij}) $
	\item and $ \dim (M_{m,n}(\F)) = m \times n $
	
\end{prop}

\begin{proof}
	$ v $-space okay, see 1.1. And dim? A standard basis for $ M_{m,n}(\F) $ is 
	
	\[ E_{ij} =  \begin{pmatrix}
	0 & \cdots & 0 \\
	\vdots & 1 &  \vdots\\
	0 & \cdots & 0\\


	\end{pmatrix} \]
	
	(ie a matrix of zeroes, with 1 in $ i^{\text{th}}  $ row and $ j^{\text{th}}$ column )
	
	
	$ (a_{ij}) = \sum_{ij}  a_{ij} E_{ij} $, from which span and LI follows
	
	This basis has cardinality $ mn $
	
\end{proof}


\begin{defi} (Coordinate Vectors)
	
	Let $ V,W $ be v-spaces over $ \F $, of finite dim, with $ \alpha : V \to W $, linear. Basis $ \mathcal{B} $ for $ V $, $ v_{1},\cdots,v_{n} $ basis $ \mathcal{C} $ for $ W $, $ w_{1},\cdots,w_{n} $. If $  v \in V $, $ v = \sum \lambda_{i} v_{i} $, write $ [v]_{\mathcal{B}} = \begin{pmatrix}
\lambda_{1} \\
\vdots \\
\lambda_{n}
\end{pmatrix} \in \F$, called coordinate vector of $ v $ wrt $ \mathcal{B} $. Similarly, $ [w]_{\mathcal{C}} \in \F^{m} $.
\end{defi}

\begin{defi} (Matrix)
	$ [\alpha]_{\mathcal{B},\mathcal{C}} $ matrix of $ \alpha $ wrt $ \mathcal{B} $ and $ \mathcal{C} $
	
	
	\begin{align*}
	[\alpha]_{\mathcal{B},\mathcal{C}} & = \left(   [\alpha(v_{1})]_{\mathcal{C}} \; | \; [\alpha(v_{2})]_{\mathcal{C}} \; | \; \cdots \; | \; [\alpha(v_{n})]_{\mathcal{C}} \right) \in M_{m,n}(\F)  \\
	& = (a_{ij})
	\end{align*}
\end{defi}

The notation says $ \alpha(v_{j}) = \sum \alpha_{ij} w_{i} $

\begin{lemma} For any $ v \in V $, 
	
	\[ [\alpha(v)]_{\mathcal{C}} = [\alpha]_{\mathcal{B},\mathcal{C}} \cdot [v]_{\mathcal{B}} \]
	
	where the dot denotes matrix applied to vector
	
\end{lemma}

\begin{proof} Fix $ v \in V $, $ v = \sum_{j=1}^{n} \lambda_{j}v_{j} $, so $ [v]_{\mathcal{B}} = \begin{pmatrix}
	\lambda_{1} \\
	\vdots \\
	\lambda_{n}
	\end{pmatrix} $
	
	
	\begin{align*}
	\alpha(v) & = \alpha( \sum \lambda_{j} v_{j} )  = \sum \lambda_{j} \alpha(v_{j}) = \sum_{j} \lambda_{j} (\sum_{i} \alpha_{ij} w_{i}  )  \\
	& = \sum_{i} \underbrace{\left(  \sum_{j} \alpha_{ij} \lambda_{j}  \right)}_{i^{\text{th}} \text{ entry of } [\alpha]_{\mathcal{B},\mathcal{C}} [v]_{\mathcal{B}}  } w_{i}
	\end{align*}
	
\end{proof}

\begin{lemma} 
	Let $ \alpha $, $ \beta $ be linear maps, with
	\begin{tikzcd}
		U \ar[r, "\beta"] & V \ar[r, "\beta"] & W 
	\end{tikzcd}
	and $ \alpha \circ \beta $ linear. Let $ \mathcal{A},\mathcal{B},\mathcal{C} $ be basis for $ U,W,V $ reps. Then
	
	\[  [\alpha \circ \beta]_{\mathcal{A},\mathcal{C}} = \underbrace{[\alpha]_{\mathcal{B},\mathcal{C}}}_{= (a_{ij})} \circ \underbrace{[\beta]_{\mathcal{A},\mathcal{B}}}_{= (b_{ji})}\]

\end{lemma}

\begin{proof}
	\begin{align*}
	(\alpha \circ \beta) \overbrace{(u_{i}) }^{\text{in } \mathcal{A}} & = \alpha(\beta(u_{i})) = \alpha (\sum_{j} b_{ji}\overbrace{ v_{j}}^{\text{in } \mathcal{B}} ) \\
	& = \sum_{j} b_{ji} \alpha(v_{j})\\
	& = \sum_{j} b_{ji} \sum_{i} a_{ij} \overbrace{w_{i}}^{\text{in } C} \\
	& = \sum_{i} \underbrace{\left(  \sum_{j} a_{ij} b_{ji} \right)}_{(i,j)^{\text{th}} \text{ entry of } [\alpha]_{\mathcal{B},\mathcal{C}} [\beta]_{\mathcal{A},\mathcal{B}}  }    w_{i} 
	\end{align*}
\end{proof}


\begin{prop} 
	If $ V,W $ are v-spaces over $ \F $ with $ \dim V = n $, $ \dim W = m $, then $ L(V,W) \simeq M_{m,n}(\F)  $
\end{prop}

\begin{proof}
	Fix bases 
	\[ \mathcal{B} \text{ of } V: v_{1},v_{2},\cdots,v_{n}  \]
	\[ \mathcal{C} \text{ of } V: w_{1},v_{2},\cdots,v_{n}  \]
	
	Claim: 
	
	\[ L(v,w) \to M_{m,n}(\F) \]
	\[ \alpha \mapsto [\alpha]_{\mathcal{B},\mathcal{C}} \]
	
	is an iso. 
	
	\begin{itemize}
		\item $ \theta $ linear $ [\lambda_{1} \alpha_{1} + \lambda_{2}\alpha_{2}]_{\mathcal{B},\mathcal{C}} = \lambda_{1}[\alpha_{1}]_{\mathcal{B},\mathcal{C}} + \lambda_{2} [\alpha_{2}]_{\mathcal{B},\mathcal{C}}  $
		\item $ \theta $ surjective: given $ A = (a_{ij})$. Let $ \alpha : v_{j} \mapsto \sum_{i=1}^{m} a_{ij}w_{i} $, and extend linearly. Then $ \alpha \in L(V,W),b  \theta(\alpha) = A$.
		\item $ \theta $ injective, $ [\alpha]_{\mathcal{B},\mathcal{C}} = 0 $ matrix $ \Rightarrow \alpha $ is zero-map from $ V $ to $ W $.
	\end{itemize}
\end{proof}

\begin{cor} 
	\[ \dim(L(V,W)) = (\dim V)(\dim W) \]
\end{cor}

\begin{eg}
	$ \alpha : V \to W, Y \leq V, Z \leq W$. Say $ \alpha(Y) \subseteq Z $.
	
	Basis of $ V: $
	
	\[ \mathcal{B}: \; \underbrace{v_{1},\cdots,v_{k}}_{\text{Basis for } Y, \; \mathcal{B}'},v_{k+1},\cdots,v_{n}  \]
	
	
	Basis of $ W: $
	
	\[ \mathcal{C}: \; \underbrace{w_{1},\cdots,w_{k}}_{\text{Basis for } Z, \; \mathcal{C}'},w_{k+1},\cdots,w_{m}  \]
	
	Then 
	
	\[ [\alpha]_{\mathcal{B},\mathcal{C}}  = \begin{pmatrix}
	A & \cdots & B_{1} \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & C_{1}  
	\end{pmatrix}\]
	
	because for $ 1 \leq j \leq k $, $ \alpha(v_{j}) $ is a lin combo of $ w_{i} $, where $ 1 \leq i \leq l $. 
	
	And
	
	\[ [\alpha |_{y}]_{\mathcal{B}',\mathcal{C}'} = A_{1} \]
	
	Claim: $ \alpha $ induces 
	
	\[ \overline{\alpha} : V / Y \to W / Z \]
	
	\[ v + Y \mapsto  \alpha(v) + Z  \]
	
	Well defined? 
	
	\begin{align*}
	v_{1} + Y = v_{2} + Y & \Rightarrow v_{1} - v_{2} \in Y \\
	& \Rightarrow \alpha(v_{1} - v_{2}) \in Z \\
	& \Rightarrow \alpha(v_{1})  + Z = \alpha(v_{2}) + Z
	\end{align*}
	
\begin{ex}
		Linear from linearity of $  \alpha $
\end{ex}

Basis for $ Y / V $,

\[ \mathcal{B}'' : v_{k+1} + Y, \cdots, v_{n} + Y \]

Basis for $ W / Z $, 

\[ \mathcal{B}'' : v_{k+1} + Y, \cdots, v_{n} + Y \] 

\begin{ex}
	$ [\overline{\alpha}]_{\mathcal{B}'',\mathcal{C}''} $
\end{ex}

\end{eg}






\subsection{Change of Basis}

Let $ V $ and $ W $ be v-spaces over $ \F $ with the following basis


\[ V \qquad \qquad \qquad  W  \]
\[ \mathcal{B} = \{ v_{1},\cdots,v_{n} \} \quad C = \{ w_{1},\cdots,w_{m} \} \]
\[ \mathcal{B}' = \{ v_{1}',\cdots,v_{n}' \} \quad C' = \{ w_{1}',\cdots,w_{m}' \} \]

\begin{defi}
	The \emph{change of basis matrix} from $ \mathcal{B} $ to $ \mathcal{B}' $ is $ P = (p_{ij}) $ given by $ v_{j}' = \sum p_{ij}v_{i} $.
	
	Equivalently,
	
	\[ P = \begin{pmatrix}
	& & & & & & \\
	[v_{1}']_{\mathcal{B}} & \big| & [v_{2}']_{\mathcal{B}} & \cdots &  & \big| & [v_{n}']_{\mathcal{B}} \\
	& & & & & &
	\end{pmatrix} = [\id]_{\mathcal{B}',\mathcal{B}}  \]	
	
\end{defi}

\begin{lemma} 
	$ [v]_{\mathcal{B}} = P [v]_{\mathcal{B}'} $
\end{lemma}

\begin{proof}
	\[ P [v]_{\mathcal{B}'} = [\id]_{\mathcal{B}',\mathcal{B}}[v]_{\mathcal{B}'} = [v]_{\mathcal{B}} \] 
\end{proof}

\begin{lemma} 
	$ P $ is an invertible $ n \times n $ matrix, and $ P^{-1} $ is the change of basis matrix from $ \mathcal{B} $ to $ \mathcal{B}' $
\end{lemma}

\begin{proof}
	\[  [\id]_{\mathcal{B},\mathcal{B}'} [\id]_{\mathcal{B}',\mathcal{B}} =  [\id]_{\mathcal{B}',\mathcal{B}'}  = I_{n}\]
	
		\[  [\id]_{\mathcal{B'},\mathcal{B}} [\id]_{\mathcal{B},\mathcal{B}'} =  [\id]_{\mathcal{B},\mathcal{B}}  = I_{n}\]
	
\end{proof}

Let $ Q $ be the change of basis matrix from $ C' $ to $ C $. $ Q $ also invertible $ m \times m $.

\begin{prop} 
	Let $ \alpha : V \to W $ linear, $ A = [\alpha]_{\mathcal{B},\mathcal{C}} $, $ A' = [\alpha]_{\mathcal{B}',\mathcal{C}'}  $. Then
	
	\[ A' = Q^{-1} A P \]
\end{prop}

\begin{proof}
	\begin{align*}
	Q^{-1} A P & = [\id]_{\mathcal{C},\mathcal{C'}} [\alpha]_{\mathcal{B},\mathcal{C}} [\id]_{B',B}  \\
	& = [\id \circ \alpha \circ \id ]_{\mathcal{B}',\mathcal{C}'} \\
	& = A'
	\end{align*}
\end{proof}

\begin{defi}
	$ A,A' \in M_{m,n}(\F) $ are \emph{equivalent} if $ A' = Q^{-1} A P $ for some invertible $ P \in M_{n,n}(\F), Q \in 	M_{m,m} (\F) $
\end{defi}

Note: this defines an equivalence relation on $ M_{m,n}(\F) $, eg. $ A' = Q^{-1} A P, A'' = (Q^{-1})^{-1} A' P' \Rightarrow A'' = (Q Q^{-1})^{-1} A P P' $


\begin{prop} 
	Let $ V,W $ be $ \F $-vector spaces of $ \dim n $ and $ m $ resp. Let $ \alpha : V \to W $ be a linear map. Then there exists bases $ \mathcal{B} $ of $ V $ and $ \mathcal{C} $ of $ W $, and some $ r \leq m, n $ st. 
	
		\[ [\alpha]_{\mathcal{B},\mathcal{C}}  = \begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix}\]
	
	where $ I_{r} $ is the identity matrix. 
	
	Note: $ r = \text{rank}(\alpha) = r(\alpha) $
	
\end{prop}
	
	\begin{proof}
		Fix $ r $ st. $ N(\alpha) $ has $ \dim n - r $. Fix a basis for $ N(\alpha) $, say $ v_{r+1},v_{r+2},\cdots,v_{n} $. Extend this to a basis for $ V $, say $ \underbrace{v_{1},\cdots,v_{r}}_{\mathcal{B}},v_{r+1},\cdots,v_{n} $. Now $ \alpha(v_{1}),\cdots,\alpha(v_{r}) $ is a basis for $ \im(\alpha) $.
		
		\begin{itemize}
			\item span: $ \alpha(v_{1}),\cdots,\alpha(v_{r}),\underbrace{\alpha(v_{r+1})}_{=0},\cdots,\underbrace{\alpha(v_{n})}_{=0} $ certainly span $ \imath(\alpha) $
			
			\item LI: 
			
		\begin{align*}
			\sum_{i=1}^{r} \lambda_{i} \alpha(v_{i}) = \mathbf{0} & \Rightarrow \alpha \underbrace{\left(  \sum_{i=1}^{r} \lambda_{i} v_{i} \right)}_{\in \ker(\alpha)} = \mathbf{0}   \\
		& \Rightarrow \sum_{i=1}^{r} \lambda_{i}v_{i} = \sum_{j=r+1}^{n} \mu_{j} v_{j} \text{ some } \mu_{j} \in \F \\
		& \Rightarrow \text{ as } v_{1},\cdots,v_{n} \text{ LI }, \lambda_{i} = \mu_{j} = 0 \; \forall \; i,j 
		\end{align*}
		\end{itemize}
	
	Extend $ \alpha(v_{1}),\cdots,\alpha(v_{r}) $ to a basis of $ W $, say $ C $. By construction, 
	
		\[ [\alpha]_{\mathcal{B},\mathcal{C}}  = \begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix}\]
	
	Remark: didn't need to assume in the proof that $ r = r(\alpha) $. Can think of this as giving a different proof of the r-n theorem.
	
		
		
		 	

\end{proof}

\begin{cor} 
	Any $ m \times n $ matrix is equivalent to $ \begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix} $ for some $ r $.
\end{cor}

\begin{defi}
	Let $ A \in M_{m,n}(\F) $. The \emph{column rank} of $ A $ is the dimension of the subspace of $ \F^{m} $ spanned by the columns of $ A $. The \emph{row rank} of $ A $ is the column rank of $ A^{T} $ (the dimension of the subspace of $ \F^{n} $ spanned by the row vectors of $ A $).
\end{defi}

Note: if $ \alpha $ is a linear map represented by $ A $ wrt. any choice of basis, then $ r(\alpha) = r(A) $, ie column rank = rank.

\begin{prop} 
	Two $ m \times n $ matrices $ A,A' $ are equivalent iff $ r(A') = r(A) $. 
\end{prop}

\begin{proof}
	 ($ \Leftarrow $) Both $ A $ and $ A' $ are equivalent to $ \begin{pmatrix}
	 I_{r} & \cdots & 0 \\
	 \vdots & \ddots & \vdots \\
	 0 & \cdots & 0  
	 \end{pmatrix} $, $ r = r(A') = r(A) $ (this is a transitive relation)
	 
	 ($ \Rightarrow $) Let $ \alpha $ be the linear map: $ \alpha : \F^{n}  \to \F^{m} $ represented by $ A $ wrt. the standard basis $ A' = Q^{-1} A P$. $ P $ and $ Q $ invertible, so $ A' $ represents $ \alpha $ wrt. two other bases. $ r(\alpha) $ is defined in a basis invariant way, so $ r = r(\alpha) = r(A) = r(A') $

\end{proof}



\begin{thm} 
	$ r(A) = r(A^{T}) $ ("row rank = column rank").
\end{thm}

\begin{proof}
	$ Q^{-1} A P = \begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix}_{m,n} $ where $ Q,P $ invertible 
	
	Take transpose of whole equation:
	
	\begin{align*}
	\begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix}_{n,m} & = (Q^{-1} A P)^{T} \\
	& = P^{T} A^{T} (Q^{T})^{-1}
	\end{align*}
	
	so $ A^{T} $ equiv to $ 	\begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix}_{n,m} $. Thus $ r(A) = r(A^{T})  $.
	
	
	
\end{proof}



$ V = W $, $ \mathcal{C} = \mathcal{B} $, other basis $ \mathcal{B} $'. $ P $ change of basis matrix form $ \mathcal{B} $ to $ \mathcal{B}' $, $ \alpha \in L(V,V) $. 

\[ [\alpha]_{\mathcal{B}',\mathcal{B}'} = P^{-1} [\alpha]_{B,B}  P \]

\begin{defi}
	$ A,A' \in M_{n,n}(\F) $, $ A, A' $ are \emph{similar} (or \emph{conjugate}) if $ A'  = P^{-1}  A P $ for  some invertible $ P $.  
\end{defi}

\subsection{Elementary Matrices and Operations}

\begin{defi}
	\emph{Elementary column operators} on an $ m \times n $ matrix $ A $:
	
	\begin{enumerate}
		\item swap columns $ i $ and $ j $ (wlog $ i \neq j $)
		\item replace column $ i $ by $ \lambda $ (column $ i $), $ \lambda \neq 0 $
		\item add $ \lambda $(column $ i $) to column $ j $, $ i \neq j $, $ \lambda \neq 0$.
	\end{enumerate}
\emph{Elementary row operators} analogous (replace `column' by `row')
\end{defi}

Note: all of these operations are reversible. 

Corresponding elementary matrices: effect of performing the column operations on $ I_{n} = n \times n $ id.  For row operations, $ I_{m} $.

\begin{enumerate}
	\item $ \begin{pmatrix}
		1\\
		& \ddots\\
		& & 1\\
		& & & 0 & & & & 1\\
		& & & & 1\\
		& & & & & \ddots\\
		& & & & & & 1\\
		& & & 1 & & & & 0\\
		& & & & & & & & 1\\
		& & & & & & & & & \ddots\\
		& & & & & & & & & 1
	\end{pmatrix} $
	
	The zeros appear in row $ i $, row $ j $.
	
	\item $ \begin{pmatrix}
	1 & & & & & &\\
	& \ddots & & & & & \\
	& & 1 & & & & \\
	& & & \lambda & & & \\
	& & & & 1 & & \\
	& & & & & \ddots & \\
	& & & & & & 1
	\end{pmatrix} $
	
	with $ \lambda $ in the $ i^{\text{th}} $ row
	
	\item $ I_{n} + \lambda E_{ij}  $, where $ E_{ij} $ is defined as $ 1 $ in the $ (i,j) $ position and $ 0 $ everywhere else.
	
\end{enumerate}

An elementary column operation on $ A \in M_{m,n}(\F) $ can be performed by multiplying $ A $ by the corresponding elementary matrix on the right.

\begin{ex}
	\[ \begin{pmatrix}
	1 & 2\\
	3 & 4
	\end{pmatrix} \begin{pmatrix}
	0 & 1 \\
	1 & 0
	\end{pmatrix} = \begin{pmatrix}
	2 & 1\\
	4 & 3
	\end{pmatrix} \]
\end{ex}

For row operations, multiply on the left

\begin{ex}
\[ \begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix} \begin{pmatrix}
1 & 2\\
3 & 4
\end{pmatrix} = \begin{pmatrix}
3 & 4\\
1 & 2
\end{pmatrix} \]	
\end{ex}

 
\[ \begin{pmatrix}
I_{r} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & 0  \end{pmatrix} \]

\begin{proof}
	Start with $ A $. If all entries of $ A $ are $ 0 $, we're done ($ r = 0 $). If not, some $ a_{ij} = \lambda \neq 0 $.
	\begin{itemize}
		\item swap rows $ 1,i $
		\item swap columns $ 1,j $
		\item multiply column 1 by $ \frac{1}{\lambda} $
		
	\end{itemize}
to get $ 1 $ in position $ (1,1) $. Now

\begin{itemize}
	\item add $ (-a_{12})(\text{column } 1) $ to column $ 2 $. 
	\item Similarly clear out all other entries in row 1.
	\item Also use row operations to clear out all other entries in column 1
\end{itemize}

Upshot: get

$ \begin{pmatrix}
1 & 0 & \cdots & 0 \\
0 & & & &\\
\vdots & & \tilde{A} & \\
0 & & & &
\end{pmatrix} $, $ \tilde{A} \in M_{m-1,n-1}(\F) $

Now iterate, to get $ \begin{pmatrix}
I_{r} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & 0  \end{pmatrix} = \overbrace{\underbrace{E_{1}',\cdots,E_{k}'}_{\text{elem row}}}^{Q^{-1}} A \overbrace{\underbrace{E_{1},\cdots,E_{k}}_{\text{elem column}}}^{P}  $


Row/column ops are reversible $ \Rightarrow $ elem matrices are invertible. $ Q: m \times m $ invertible, $ P: n \times n $ invertible.

\end{proof}

Variations:

If you use elementary row operations, can get the row echelon form of a matrix. 

\[ \begin{pmatrix}
1 & 0 & 0 & 0 & a \\
& & 1 & 0 & b\\
& & & 1 & c  
\end{pmatrix} \]

How? Assume $ a_{i1}' = \lambda \neq 0 $ some $ i $.
\begin{itemize}
	\item swap rows 1 and $ i $ $ \Rightarrow $ get $ \lambda $ in $ (1,1) $
	\item divide row 1 by $ \lambda \; \Rightarrow \; 1 $ in $ (1,1) $
	\item use (iii)-type operation to clear out rest of column 1, then move on to second column etc.  
\end{itemize}

\begin{lemma} 
	If $ A $ is $ n \times n $ invertible, we can obtain $ I_{n} $ by using only elementary row operations (or elementary column operations).
\end{lemma}

\begin{proof}
	Induction on number of rows
	
	
	Suppose we have
	
	\[ \begin{pmatrix}
	1 & 0 & 0 & & &  \\
	& & 1 & & 0 & \\
	& & & 1 & & 
	\end{pmatrix} \]
	
	There exists $ j > k $ with $ a_{k+1,j} = \lambda \neq 0 $
	
	If not, 	\[ \begin{pmatrix}
	0 \\
	\vdots \\
	1 \\
	\vdots \\
	0
	\end{pmatrix} \] with 1 in the $ (k+1) $th entry would not lie in the span of the column vectors, which would contradict invertability
	
	
	\begin{itemize}
		\item Swap columns $ k+1 $ and $ j $
		\item Divide column $ k+1 $ by $ \lambda $
		\item Use type 3 operators to clear the other entries of the $ (k+1) $th row. 
		\item now proceed inductively
	\end{itemize}
	
	
	
\end{proof}

Upshot 

\[ A E_{1} E_{2} \cdots E_{l} = I_{n} \Rightarrow A^{-1} = E_{1} E_{2} \cdots E_{l} \]

one recipe for inverses.

\begin{prop} 
	Any invertible matrix can be written as a product of elementary matrices.
\end{prop}


\section{Dual Spaces and Dual Maps}
\subsection{Dual Vector Spaces}

\begin{defi}
	Let $ V $ be a vector space over $ \F $. The \emph{dual vector space} $ V^{*} $ of $ V $
	
	\[ V^{*} = L(V,F) = \{ \alpha : V \to F \text{ linear } \}  \]
\end{defi}

$ V^{*} $ is a vector space over $ \F $. Its elements are sometimes called linear functionals. 

\begin{eg}
	$ V = \R^{3} $,
	
	\[ \theta: V \to \R \]
	
	\[ \begin{pmatrix}
	a \\
	b\\
	c
	\end{pmatrix} \to a - c \qquad \theta \in V^{*} \]
\end{eg}

\begin{eg}
	\[ t_{n}:  M_{m,n}(\F) \to \F \]
	
	\[ A \mapsto \sum_{i} A_{ii}, \qquad t_{n} \in (M_{m,n}(\F))^{*} \]
\end{eg}

\begin{lemma} 
	Let $ V $ be a vector space over $ \F $ with finite basis $ \mathcal{B} = \{ e_{1},\cdots,e_{n} \} $. Then there is a basis for $ V^{*} $, given by $ \mathcal{B}^{*} = \{  \varepsilon_{1},\cdots,\varepsilon_{n} \}  $ where 
	
	\[ \varepsilon_{j} \underbrace{\left(   \sum_{i=1}^{m}  a_{i} e_{i}  \right) }_{\in V} = a_{j}  \qquad 1 \leq j \leq n \]
	
	$ \mathcal{B}^{*} $ is called the dual basis to $ \mathcal{B} $
\end{lemma}


\begin{proof}
\begin{itemize}
	\item LI
	
	\begin{align*}
	\sum_{j=1}^{n} \lambda_{j} \varepsilon_{j} = \mathbf{0} & \Rightarrow \left(  \sum_{j=1}^{n}  \lambda_{j} \varepsilon_{j} \right) e_{i} = \mathbf{0}   \\
	& = \sum_{j} \lambda_{j} \underbrace{\varepsilon_{j} (e_{i})}_{\delta_{ij}} 
	\end{align*}
	
	
	\[ \Rightarrow  \lambda_{i} = 0 \quad \forall \; i = 1,\cdots,n \]
	
	\item Span: If $ \alpha \in V^{*} $, then $ \alpha = \sum_{i=1}^{n} \alpha(e_{i}) \varepsilon_{i} $
	
	(``linear maps are determined by their action on a basis'')
	
\end{itemize}
\end{proof}

\begin{cor} 
	If $ V $ is finite dim, then $ \dim V  = \dim V^{*} $
\end{cor}
 
 
 Remark: Someties useful to think about $ (\F^{n})^{*} $ as the space of row vectors of length $ n $ over $ \F $. Suppose
 
 \[ V \text{ basis } e_{1},\cdots,e_{n}\]
 \[ V^{*} \text{ dual basis } \varepsilon_{1},\cdots,\varepsilon_{n} \]
 
\[   x = \sum x_{i} e_{i} \in V \] 
\[ \alpha = \sum a_{i} \varepsilon_{i} \in V^{*}  \]
 
 \[ \alpha(x) = \sum_{i=1}^{n} \alpha_{i} x_{i} =   \begin{pmatrix}
 a_{1} & \cdots & a_{n}
 \end{pmatrix}  \begin{pmatrix}
 x_{1} \\
 \vdots \\
 x_{n}
 \end{pmatrix}  \]
 
 
 \begin{defi}
 	If $ U \subseteq V $, 
 	
 	\[ U^{0} = \{  \alpha \in V^{*} \text{ st. } \alpha(u) = 0 \text{ for all }  u  \in U \} \] 
 	
 	is the \emph{annihilator} of $ U $
 	
 \end{defi}

 
 \begin{lemma} 
 	\begin{enumerate}
 		\item $ U^{0} \leq V^{*} $
 		\item If $ U \leq V $ and $ \dim V = n < \infty $, then
 		
 		\[ \dim V = \dim U + \dim U^{0} \] 
 		
 	\end{enumerate}
 \end{lemma}

\begin{proof}
	\begin{enumerate}
		\item $ 0 \in U^{0} $. If $ \alpha,\alpha' \in U^{0} $, then $ (\alpha + \alpha') = \alpha(u)  + \alpha'(u) = 0 + 0 = 0$, for $ u \in U $ thus $ \alpha + \alpha' \in U^{0} $ 
		
		Similarly, $ \lambda \alpha \in U^{0} $ for any $ \lambda \in \F $
		
		\item Let $ e_{1},\cdots,e_{k} $ be a basis for $ U $. Extend to a basis for $ V $. $ e_{1},\cdots,e_{k},e_{k+1},\cdots,e_{n} $.
		
		Let $ \mathcal{B}^{*} $ be the dual basis to this. $ \varepsilon_{1},\cdots,\varepsilon_{n} $
		
			Claim: $ \varepsilon_{k+1},\varepsilon_{k+2},\cdots,\varepsilon_{n} $ is a basis for $ U^{0} $
			
			\begin{itemize}
				\item If $ i > k $, $ \varepsilon_{i}(e_{j}) = 0 $ where $ j \leq k $, so $ \varepsilon_{i} $ (for $ i > k $) is in $ U^{0} $.
				\item LI comes from the fact that $ \mathcal{B}^{*} $ is a basis. (So any subset of it is LI).
				\item Span? If $ \alpha \in U^{0} $, then $ \sum_{i=1}^{n} \alpha_{i} \varepsilon_{i} $, some $ a_{i} \in \F $. 
				
				
				\[ \left(  \sum_{i=1}^{n} a_{i} \varepsilon_{i} \right) (e_{j}) = 0 \Rightarrow a_{j} = 0 , \text{ any } j \leq k\]
				
				where $ e_{j} $ is a basis element for $ U $, for $ j \leq k $
				
				\[ \Rightarrow \alpha \in < \varepsilon_{k+1},\cdots,\varepsilon_{n} \]
				
			\end{itemize}
		
	\end{enumerate}


\end{proof}


\subsection{Dual Maps}

\begin{lemma} 
	Let $ V,W $ be vector spaces over $ \F $. Let $ \alpha \in L(V,W) $. Then the map 
	
	\[ \alpha^{*} : W^{*} \to V^{*} \]
	\[ \varepsilon \mapsto \varepsilon \circ \alpha \text{ is linear } \]
	
	\begin{tikzcd}
		V \ar[r, "\alpha"] & W \ar[r, "\varepsilon"] & F
	\end{tikzcd}
	
	We'll call $ \alpha^{*} $ the \emph{dual} of $ \alpha $.
	
	
\end{lemma}


\begin{proof}
	\begin{itemize}
		\item $ \varepsilon \circ \alpha $ is linear, so in $ V^{*} $.
		\item $ \alpha^{*} $ linear? Fix $ \theta_{1},\theta_{2} \in W^{*} $
		
		\begin{align*}
		\alpha^{*}(\theta_{1} + \theta_{2}) & = (\theta_{1} + \theta_{2})\alpha \\
		& = \theta_{1} \circ \alpha + \theta_{2} \circ \alpha \\
		& = \alpha^{*} \theta_{1} + \alpha^{*} \theta_{2}
		\end{align*} 
	\end{itemize}
Similarly, $ \alpha^{*}(\lambda \theta)  = \lambda \alpha^{*} \theta $
\end{proof}


\begin{prop} 
	Let $ V,W $ be v-spaces over $ \F $, with basis $ \mathcal{B},\mathcal{C} $ respectively. Let $ \mathcal{B}^{*},\mathcal{C}^{*} $ be the dual basis. Consider $ \alpha \in L(V,W) $ with dual $ \alpha^{*} $ .
	
	
	\[ [\alpha^{*}]_{\mathcal{C}^{*},\mathcal{B}^{*}} = [\alpha]_{\mathcal{B},\mathcal{C}}^{T} \]
\end{prop}

\begin{proof}
	 Say $ \mathcal{B} = \{ b_{1},\cdots,b_{n} \} $, $ \mathcal{C} \{ c_{1},\cdots,c_{n} \} $
	 
	 $ \mathcal{B}^{*} = \{  \beta_{1},\cdots,\beta_{n} \} $, $ \mathcal{C}^{*} = \{  \gamma_{1},\cdots,\gamma_{n} \} $
	 
	 and $ [\alpha]_{\mathcal{B},\mathcal{C}} = (a_{ij}) \quad m \times n $


\begin{align*}
\alpha^{*}(\gamma_{r}) (b_{s}) & = \gamma_{i} \circ \alpha(b_{s}) \\
& = \gamma_{r} (\alpha(b_{s})) \\
& = \gamma_{r} \left( \sum_{t} a_{ts}c_{t} \right) \\
& = \sum_{t} \alpha_{ts}\gamma_{r}(c_{t}) \\
& = a_{rs} \\
& = \left(  \sum_{i} \alpha_{ri} \beta_{i} \right)(b_{s})
\end{align*}

\[ \Rightarrow \alpha^{*}(\gamma_{r}) = \sum_{i} a_{r_{i}} \beta_{i} \]

\[ \Rightarrow [\alpha^{*}]_{\mathcal{C}^{*},\mathcal{B}^{*}} = [\alpha]_{\mathcal{B},\mathcal{C}}^{T} \]

\end{proof}


Let $ V $ be a finite dim $ \F $ vector space.

Bases $ \varepsilon = \{   e_{1},\cdots,e_{n} \} $, $ F = \{ f_{1},\cdots,f_{n} \} $

Dual bases $ \varepsilon^{*} = \{  \varepsilon_{1},\cdots,\varepsilon_{n} \} $, $ \mathcal{F} = \{ \eta_{1},\cdots,\eta_{n} \} $

And let us condisder $ P = [\id]_{\mathcal{F}\mathcal{E}} $

\begin{lemma} 
	Change of basis matrix from $ \mathcal{F}^{*} $ to $ \mathcal{E}^{*} $ is $ (P^{-1})^{T} $
\end{lemma}

\begin{proof}
	\[ [\id]_{\mathcal{F}^{*},\mathcal{E}^{*}} = [\id]_{\mathcal{E}\mathcal{F}}^{T} = ([\id]_{\mathcal{F}\mathcal{E}}^{-1})^{T}  \]
\end{proof}

CAUTION: $ V \simeq V^{*} $ only if $ V $ is finite dimensional.

Let $ V = \mathcal{P} $, the space of all real polynomials, with basis

\[ p_{j},j = 0,1,2,\cdots \qquad p_{j}(t) = t^{j} \]

Ex sheet 2 Q 9:

\[ P^{*} \simeq \R^{\N} \]
\[ \varepsilon \mapsto (\varepsilon(p_{0}), \varepsilon(p_{1}),\cdots  \]

Ex sheet 1, Q3 g) $ P \not \simeq \R^{\N} $ does NOT have a countable basis

\begin{lemma} 
	Let $ V,W $ be vector spaces over $ \F $. Fix $ \alpha \in L(V,W) $, let $ \alpha^{*} \in L(W^{*},V^{*}) $ be the dual map. Then
	\begin{enumerate}
		\item $ N(\alpha^{*})  = (\Im(\alpha))^{0} $ ie. $ \alpha^{*} $ injective iff $ \alpha $ is surjective
		\item $ \Im(\alpha^{*}) \leq (N(\alpha))^{0} $, with equality if $ V $
and $ W $ are finite dimensional. ie. $ \alpha^{*} $ surjective iff $ \alpha $ is injective	
	\end{enumerate}
\end{lemma}


\begin{proof}
	\begin{enumerate}
		\item Let $ \varepsilon \in W^{*} $. Then 
		
		
		
		\begin{align*}
		\varepsilon \in N(\alpha^{*}) & \iff \alpha^{*} \varepsilon = 0 \\
		& \iff \varepsilon \circ \alpha = 0  \\
		& \iff \varepsilon(\mu) = 0 \text{ for all } u \in \Im \alpha\\
		&  \iff \varepsilon \in \left(  \Im(\alpha) \right)^{0}
		\end{align*}
		
		\item Let $ \varepsilon \in \Im \alpha^{*} $. Then $ \varepsilon = \alpha^{*} \varphi $, for some $ \varphi \in W^{*} $.
		
		For any $ u \in N(\alpha) $,
		
		\begin{align*}
		\varepsilon(u) & = (\alpha^{*}  \varphi )(u) \\
		& = (\varphi \circ \alpha) (u)\\
		& = \varphi(\alpha(u)) \\
		& = \varphi(0) \\
		& = \mathbf{0}
		\end{align*}
		
		So $ \varepsilon \in N(\alpha^{0}) $
		
		
		Now use the fact that $ \dim V $, $ \dim W $ are finite. 
		
		
		\begin{align*}
		\dim \Im(\alpha^{*}) & = r(\alpha^{*}) \\
		& = r(\alpha) \qquad \text{ as } r(A) = r(A^{T})  \\
		& =  \dim V - \dim N(\alpha) \qquad \text{ by R-N} \\
		& = \dim (N(\alpha))^{0}
		\end{align*}
		
	\end{enumerate}

\end{proof}


\subsection{Double Duals}

\begin{defi}
	Let $ V $ be an $ \F $ vector space, $ V^{*}  = L(V,\F) $ dual of $ V $. Then the \emph{double dual} of $ V $ is the dual of $ V^{*} $, given by
	
	\[ V^{**} = L(V^{*},\F) \] 
\end{defi}

\begin{thm} 
	If $ V $ is a finite dimensional vector space over $ \F $, then the map 
	
	\[ \hat{} \; : V \to V^{**} \]
	\[ v \mapsto \hat{v}, \quad \hat{v}(\varepsilon) = \varepsilon(v) \]
	
	is an isomorphism 
\end{thm}


\begin{proof}
	Firstly, for $ v \in V $, the map $ \hat{v} : V^{*} \to \F $ is linear, so $ \; \hat{} \;  $ does indeed give a map from $ V $ to $ V^{**} $
	
	\begin{itemize}
		\item $ \; \hat{} \; $ is linear. If $ v_{1},v_{2} \in V $, $ \lambda_{1},\lambda_{2} \in \F, \varepsilon \in V^{*} $.
		
		
		
		\begin{align*}
		(\lambda_{1} v_{1} + \lambda_{2} v_{2} ) (\varepsilon) & = \varepsilon(  \lambda_{1}v_{1} + \lambda_{2}v_{2} )\\
		& = \lambda_{1} \varepsilon(v_{1}) + \lambda_{2} \varepsilon(v_{2})\\
		& = \lambda_{1} \hat{v_{1}} (\varepsilon) + \lambda_{2} \hat{v_{2}} (\varepsilon)
		\end{align*}
		
		
		\item $ \; \hat{} \; $ is injective: Let $ e \in V \setminus \{ \mathbf{0} \} $. Extend it to a basis of $ V $, say $ e_{1},e_{2},\cdots,e_{n} $. Let $ \varepsilon_{1},\varepsilon_{2},\cdots,\varepsilon_{n} $ be the dual basis for $ V^{*} $.
		
		$ \hat{e}(\varepsilon) = \varepsilon(e) = 1 $. So $ \hat{e} \neq 0 $.
		
		Thus $ N(\; \hat{} \;) = \{ \mathbf{0} \} $, so $ \; \hat{} \; $ is injective.
		
		\item $ V $ is finite dim, so $ \dim V = \dim V^{*} = \dim V^{**} $. 
		
	\end{itemize}

Thus $ \; \hat{} \; $ is an isomorphism	
	
\end{proof}

\begin{lemma} 
	Let $ V $ be a finite dim vector space over $ \F $ and $ U \leq V $.
	
	Then $ \hat{U} = U^{00} $, so after identification of $ V $ with $ V^{**} $, we have that $ U^{00} = U$.  
\end{lemma}

\begin{proof}
	\begin{itemize}
		\item First show $ \hat{U} \leq U^{00} $.
		
		\begin{align*}
		u \in U & \Rightarrow \varepsilon(u) = 0 \qquad \forall \; \varepsilon \in U^{0} \\
		& = \hat{u}(\varepsilon) = 0 \\
		& \Rightarrow \hat{u} \in (U^{0})^{0} = U^{00}
		\end{align*}
		
		
		\begin{align*}
		\dim U^{00}& = \dim V^{*} - \dim U^{0} \\
		& = \dim V - \dim U^{0} \\
		& = \dim U
		\end{align*}
	\end{itemize}

Thus $ \hat{U} = U^{00} $

\end{proof}

\begin{lemma} 
	Let $ V $ be a finite dim vector space of $ \F $, Let $ U_{1},U_{2} \leq V $. Then
	
	
	\begin{enumerate}
		\item $ (U_{1} + U_{2})^{0} = U_{1}^{0} \cap U_{2}^{0} $
		\item $ (U_{1} \cap U_{2})^{0} = U_{1}^{0} + U_{2}^{0} $
		
	\end{enumerate}
\end{lemma}

\begin{proof}
	\begin{enumerate}
		\item Let $ \theta \in V^{*} $
		
		\begin{align*}
		\theta \in (U_{1} + U_{2})^{0} & \iff \theta(u_{1} + u_{2}) = 0 \text{ for all } u_{1} \in U_{1},u_{2} \in U_{2} \\
		& = \theta(u) = 0 \text{ for all } u \in U_{1} \cap U_{2} \\
		& \theta \in U_{1}^{0} \cap U_{2}^{0}
 		\end{align*}
		
		\item Apply annihilator to (i).
		
		\[ w_{i} = U_{i}^{0} \qquad u_{i} = W_{i}^{0} \]
		
		\[ (W_{1}^{0} + W_{2}^{0} )^{0} = W_{1} \cap W_{2} \]
		\[ W_{1}^{0} + W_{2}^{0} = (W_{1} \cap W_{2} )^{0} \]
		
	\end{enumerate}
\end{proof}



\end{document}