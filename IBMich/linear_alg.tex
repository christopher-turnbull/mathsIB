\documentclass[a4paper]{article}

\def\npart {IB}
\def\nterm {Michaelmas}
\def\nyear {2017}
\def\nlecturer {Dr. A. Keating (amk50@cam.ac.uk)}
\def\ncourse {Linear Algebra}

\include{header}

\begin{document}
\maketitle

\setcounter{section}{-1}
\section{Introduction}

   Linear algebra is an important component of undergraduate mathematics. At the practical level, matrix theory and the related vector-space concepts provide a language and a powerful computational framework for posing and solving important problems. 




Beyond this, elementary linear algebra is a valuable introduction to mathematical abstraction and logical reasoning because the theoretical development is self-contained, consistent, and accessible to most students.



\section{Vector Spaces}
\subsection{Vector Spaces}

\begin{defi}
	An $ \F $-Vector space (a vector space on $ \F $) is an abelian group $ (V, +) $ equipped with a function\footnote{scalar multiplication} $ F \times V \to V $, $ (\lambda,v) \mapsto \lambda V $
	
	\[ \lambda ( v_{1} + v_{2}) = \lambda v_{1} + \lambda v_{2} \]
	
	\[ (\lambda_{1} + \lambda_{2}) v = \lambda_{1} v + \lambda_{2} v \]
	
	\[ \lambda (\mu v) = \lambda \mu v \]
	
	\[ 1v = v \]
	
	\[ v + \mathbf{0} = v \]
	
	for all $ \lambda_{i}, \lambda, \mu \in F $, $ v_{i} \in V $
	
\end{defi}

Note that we will not be underlining our vectors, as this is cumbersome here. We will however be using $ \mathbf{0} $ to denote the zero vector. 

\begin{eg}
	For all $ n \in \N  $, $ \F^{n} = $ space of column vectors of length $ n $, entries in $ \F $. We understand the definition as entry-wise addition, entry-wise scalar multiplication
	  
\end{eg}


\begin{eg}
	$ M_{m,m}(\F) $, the set of $ m \times m $ matrices with entries in $ \F $
	
	\[ \begin{pmatrix}
	a & b \\
	c & d
	\end{pmatrix} + \begin{pmatrix}
	e & f \\
	g & h
	\end{pmatrix} = \begin{pmatrix}
	a + e & b + f\\
	c + g & d + h
	\end{pmatrix} \]
	again all operations defined entry-wise
\end{eg}

\begin{eg}
	For any set $ X $, $ \R^{X} = \{ f : X \to \R \}$
	Addition and scalar multiplication defined pointwise $ = f_{1}(x) + f_{2} (x) $.
\end{eg}


\begin{ex}
	Show that the above examples satisfy the axioms
\end{ex}

\begin{prop} 
	$ 0 v = \mathbf{0} $ for all $ v \in V $.
\end{prop}

\begin{proof}
	( $ (0 + 0)v = 0v \iff 0 v + 0v = 0v \iff 0v = \mathbf{0} $)
\end{proof}

\begin{ex}
	Show\footnote{Hint: Use the previous proposition} that $ (-1)v = -v $
\end{ex}

\begin{defi}
	Let V be an $ \F $-vector space. A subset $ U $ of $ V $ is a subspace ( $ U \leq V $) if: 
	\begin{enumerate}
		\item $ \mathbf{0} \in U $
		\item $ u_{1}, u_{2} \in U  \Rightarrow u_{1} + u_{2} \in U $ ``$ U $ is closed under addition...''
		\item $ u \in U $, any $ \lambda \in \F \Rightarrow \lambda u \in U$ ``...and scalar multiplication''
	\end{enumerate}
\end{defi}


\begin{ex}
	If $ U $ is a subspace of $ V $, then $ U $ is also an $ \F $-vector space.
\end{ex}

\begin{eg}
	Let $ V = \R^{\R} $, then $ f : R \to R $. The set of all continuous functions $ C(\R) $ are a subspace. An even smaller subspace is the set of all polynomials.
\end{eg}


\begin{ex} Define $ U \subseteq R^{3} $ as: 
	\[ \left\{   \begin{pmatrix}
a_{1} \\
a_{2} \\
a_{3}
\end{pmatrix} \; \; a_{1} + a_{2} + a_{3} = t \right\} \]
for some constant  $ t $. 
Check that this is a subspace of $ \R^{3} $ if and only if $ t = 0 $.
\end{ex}

\begin{prop} 
	Let $ V $ be an $ F $-vector space, $ U,W \leq V $. Then $ U \cap W \leq V $.	
\end{prop}

\begin{proof}
	\begin{enumerate}
		\item 	$ 0 \in U $, $ 0 \in W  \Rightarrow 0 \in U \cap W $ 
		\item Suppose $ u,v \in U \cap W $, $ \lambda, \mu \in F $.
		$ U $ is a subspace $ \Rightarrow  \lambda u + \mu v \in W $. Similarly $ \lambda u + \mu v \in U \in W $, so it is in the intersection. 
	\end{enumerate}
\end{proof}

\begin{eg}
	$ V = \R^{3} $, $ U = \left\{ \begin{pmatrix}
	x\\
	y\\
	z
	\end{pmatrix} \; | \; x = 0 \right\} $, $ V = \left\{ \begin{pmatrix}
	x\\
	y\\
	z
	\end{pmatrix} \; | \; y = 0 \right\}  $ then $ U \cap W = U = \left\{ \begin{pmatrix}
	x\\
	y\\
	z
	\end{pmatrix} \; | \; x = 0, y= 0 \right\}  $ (intersect along the $ z $-axis)
\end{eg}

Note: union of family of subspaces is almost never a subspace itself.


\begin{defi}
	Let $ V $ be an $ F $-vector space, $ U,W \leq V $. The \emph{sum} of $ U $ and $ W $ is the set:
	
	\[ U + W = \left\{  u + w \; | u \in U, w \in W    \right\}  \]
\end{defi}

\begin{prop} 
	$ U + W \leq V $
\end{prop}

\begin{proof}
	$ \mathbf{0} \in U,W \Rightarrow \mathbf{0} + \mathbf{0} = \mathbf{0} \in U + W $
	
	$ u_{1},u_{2} \in U $, $ w_{1},w_{2} \in W $, 
	
	\[ (u_{1} + w_{1}) + (u_{2} + w_{2}) = \underbrace{(u_{1} + u_{2})}_{\in U}  + \underbrace{(w_{1} + w_{2})}_{\in W} \]
	
	Similarly for scalar multiplication (ex.)
\end{proof}

Note: $ U + W $ is the smallest subspace containing both $ U $ and $ W $. (This is becaues all elements of the form $ u + w $ ae forced to be in such a subspace by the ``closed under addition'' axiom)


\begin{defi}
	$ V $ is an $ \F $-vector space, $ U \leq V $. The quotient space\footnote{think of this as the collection of cosets of $ U $ in $ V $ } $ V / U $ is the abelian group $ V / U $ equipped with scalar multiplication;
	
	\[ F \times V / U \to V / U \]
	
	\[ (\lambda, v + U) \mapsto \lambda v + U \]
\end{defi}

\begin{prop} 
	This is well-defined, and $ V/U $ is an $ F $-vector space.
\end{prop}

\begin{proof}
	Well-defined: Suppose $ v_{1} + U = v_{2} + U \in V / U $. $ \Rightarrow (v_{2} - v_{1}) \in U \Rightarrow ( \lambda v_{2} - \lambda v_{1}) \in U \Rightarrow \lambda v_{2} + U = \lambda v_{1} + U \in V / U $

To show that it is an $ \F $-vector space, we must show that the axioms hold. These follow from the axioms of $ V $.
$ \lambda ( \mu (v + U)) = \lambda ( \mu v + U) = \lambda(u v) + U = (\lambda u) v + U  = \lambda u (v \in U) $ (scalar multiplication on $ V / U $). 

Ex. Other axioms follow similarly from using vecton space axioms

\end{proof}

\subsection{Bases}

\begin{defi}
	$ V $ is an $ \F $-vector space, $ S \subset V $. The \emph{span} of $ S $ is denoted by 
	
	\[ <S> = \left\{ \sum_{s \in S}  \lambda_{s} s \; | \; \lambda_{s} \in \F \right\}  \]
	
	ie. the set of all finite linear combinations, all but finitely many of the $ \lambda_{s} $ are zero.
\end{defi}

Remark: $ <S> $ is the smallest subspace of $ V $ which contains\footnote{This is essentially a tautology} all of the elements of $ S $

Convention: $  < \emptyset > = \{ \mathbf{0} \} $.

\begin{eg}
	$ V = \R^{3} $, 
	
	\[ S = \left\{ \begin{pmatrix}
	1 \\
	0 \\
	0
	
	\end{pmatrix}, \begin{pmatrix}
	0\\
	1\\
	2
	
	\end{pmatrix}, \begin{pmatrix}
	3\\
	-2\\
	-4
	\end{pmatrix} \right\}  \]
	
	
	\[ <S> = \{ \begin{pmatrix}
	a\\
	b\\
	2b
	\end{pmatrix} \} \; | \; a,b \in \R \]
	
	ie. we have took linear combinations of the first two. We don't need the third one.
	
\end{eg}



\begin{eg}
	For $ X $ a set, define $ \delta_{x}(y) : X \to \F $ as 
	
	
	\[ \delta_{x}(y) = \begin{cases} 1  & \text{ if } x = y \\ 0  & \text{ if } x \neq y \end{cases}  \]
	
	\[ < \delta_{x} \; | \; x \in X > = \{  f \in \R^{X} \; | \; f \text{ has finite support} \}  \]
	
	\[ = <  x \in X \; | \; f(x) \neq 0 > \]
\end{eg}

\begin{defi}
	$ S $ \emph{spans} $ V $ if $ <S> = V $
\end{defi}

\begin{defi}
	$ V $ is \emph{finite dimensiona}l over $ \F $ if it is spanned by a set that is finite.
\end{defi}

\begin{defi}
	The vectors $ v_{1},\cdots,v_{n} $ are \emph{linearly indepedent} over $ \F $ if 
	
	\[ \sum_{i = 1}^{n} \lambda_{i} v_{i} = 0 \Rightarrow \lambda_{i} \text{ for all } i \]
	
	some coefficients $ \lambda_{i} \in \F  $. $ S \subset V $ is linearly independent if every finite subset of it is. 
\end{defi}

\begin{eg}
	The first example, $ u,v,w $ are not linearly independent\footnote{If \emph{not} linearly independent, say a set is linearly dependent.}, but the set $ \{  \delta_{X} \; | \; x \in X \} $ is linearly independent.
	
\end{eg}


A lesson to be learnt from our example is that a linearly dependent spanning set contains redundant information. In a sense, a linearly independent spanning set is a minimal spanning set and hence represents the most efficient way of characterizing the subspace. This idea leads to the following definition.

\begin{defi}
	$ \mathcal{B} $ is a \emph{basis} of $ V $ if it is linearly independent and spans $ V $
\end{defi}


\begin{eg}
	\begin{itemize}
	
	
	
	\item $ \F^{n}  $  standard basis: $ \left\{     e_{1},e_{2},\cdots,e_{n} \right\} $.

	\item $ V = \C $ over $ \C $ has natural basis $ \{ 1\} $, over $ \R $ has natural basis $ \{ 1,i \} $

	\item $ V = \mathcal{P}(\R) $ space of all polynomials, has natural basis 
	
	\[ \{ 1,x,x^{2},x^{3},\cdots \} \]
	
	
		
\end{itemize}
\end{eg}

\begin{ex}
	Check this carefully 
\end{ex}


\begin{lemma} 
	$ V $ is an $ \F $-vector space. The vectors $ v_{1}, \cdots, v_{n} $ form a basis of $ V $ iff each vector $ v \in V $ has a unique expression
	
	\[ v = \sum_{i=1}^{n} \lambda_{i}v_{i}, \text{ with } \lambda_{i} \in \F \] 
	
\end{lemma}

\begin{proof}
	$ (\Rightarrow) $ Fix $ v \in V$. The $ v_{i} $ span, so 
	
	\[ \exists \lambda_{i} \in \F \text{ s.t. } v = \sum \lambda_{i} v_{i} \]
	
	Suppose also $ v = \sum \mu_{i} v_{i} $ for some $ \mu_{i} \in \F $. $ \sum \left( \mu_{i} - \lambda_{i} \right)  v_{i} = \mathbf{0} $.
	
	The $ v_{i} $ are linearly independent so $ \mu_{i} - \lambda_{i} = 0 $ for all $ i $, $ \lambda_{i} = \mu_{i} $
	
	
	$ (\Leftarrow) $ The $ v_{i} $ span $ V $, since any $ v \in V $ is a linear combination of them.
	IF $ \sum_{i=1}^{n} \lambda_{i} v_{i} = \mathbf{0} $. Note that $ \mathbf{0} = \sum_{i=1}^{n} 0 v_{i} $. By uniqueness (applied to $ \mathbf{0} $), $ \lambda_{i}  = 0 $ for all $ i $. 
\end{proof} 


\begin{lemma} 
	If $ v_{1},\cdots,v_{n} $ span $ V $ (over $ \F $), then some subset of $ v_{1},\cdots,v_{n} $ is a basis for $ V $ (over $ \F $).	
\end{lemma}

\begin{proof}
		If $ v_{1},\cdots,v_{n} $ linearly independent, done.
		Otherwise for some $ l $, there exist $ \alpha_{1},\cdots,\alpha_{l-1} \in \F $ such that
		
		\[ v_{l} = \alpha_{1} v_{1} + \cdots + \alpha_{l-1}v_{l-1} \]
		
		
		( If $ \sum \lambda_{i} v_{i} = \mathbf{0}$, not all $ \lambda_{i} = 0 $. Take $ l $ maximaml with $ \lambda_{i} \neq  0$, just $ \alpha_{i} = - \lambda_{i} / \lambda_{l} $ ).
		
		Now $ v_{2}, \cdots, v_{l-1},v_{l+1},\cdots,v_{n} $ still span $ V $. Continue interatively until get linear independence. 
		
\end{proof}

\begin{thm} (Steinitz exchange lemma)
	Let $ V $ be a finite dimensional vector space over $ \F $. Take $ v_{1},\cdots,v_{m} $ to be linearly independent $ w_{1},\cdots,w_{n} $ to span $ V $. 
	
	Then $ m \leq n $, and reordering the spanning set if needed,
	
	\[ v_{1},\cdots, v_{m}, w_{m+1},\cdots,w_{n} \] span $ V $.
	
\end{thm}

\begin{proof} (Induction)
	Suppose that we've replaced $ l (\geq 0)$ of the $ w_{i} $. Reordering the $ w_{i} $ if needed, $ v_{1},\cdots,v_{l},w_{l+1},\cdots,w_{n} $ span $ V $. 
	
	If $ l = m $, done.
	
	If $ l < m $, then
	
	\[ v_{l+1} = \sum_{i=1}^{l} \alpha_{i} v_{i}  + \sum_{i > l} \beta_{i} w_{i} \]
	
	$ \alpha_{i}, \beta_{i} \in \F $. As the $ v_{i} $ are lin. indep, $ \beta_{i} \neq 0 $ for some $ i $. (After reordering, $ \beta_{l+1} \neq 0 $).
	
	\[ w_{l+1} = \frac{1}{\beta_{l+1}} \left( v_{l+1} - \sum_{i \leq l} \alpha_{i} v_{i}  - \sum_{i > l+1} \beta_{i} w_{i} \right)  \]
	
	This $ v_{1},\cdots,v_{l+1},w_{l+2},\cdots,w_{n} $ also spans $ V $. After $ m $ steps, $ w_{i} $ will have replaced $ m $ of the $ w_{i} $ by $ v_{i} $. Thus $ m \leq n $.
\end{proof}


\begin{thm} 
	If $ V $ is a finite dimensional vector space over $ \F $, then any two bases for $ V $ have the same number of elements. This is what we call the \emph{dimension} of $ V $, denoted $ \dim_{\F} V $.
\end{thm}

\begin{proof}
	If $ \{  v_{1},\cdots,v_{n} \} $ is a basis and $ w_{1},\cdots,w_{m} $ is another basis, the $ \{ v_{i} \} $ span and $ \{ w_{i} \} $ is linearly indepnedent' so by Steinitz $ m \leq n $. Likewise, $ n \leq m $.
\end{proof}


\begin{eg}
	$ \dim_{\C} \C = 1 $, $ \dim_{\R} \C = 2 $
\end{eg}

\begin{thm} 
	$ V $, finite dim, $ v $-space over $ \F $. If $ w_{1},\cdots,w_{l}$ is a linearly indepnedent set of vectors, we can extend it to a basis $ w_{1},\cdots,w_{l},v_{l+1},\cdots,v_{n} $
\end{thm}


\begin{proof}
	Apply Steinitiz to $ w_{1},\cdots,w_{l} $ (lin indep) and any basis $ v_{1},\cdots,v_{n} $.
	
	Or directrly, if $ V = <w_{1},\cdots,w_{l} > $, stop.
	
	Otherwise take $ v_{l+1} \in V \; \setminus <w_{1},\cdots,w_{l} > $, now $ w_{1},\cdots,w_{l},v_{l+1} $ is linearly indep. iterate
\end{proof}


\begin{cor} 
	Suppose $ V $ is a finite dimensional vector space, with dimension $ n $.
	
	\begin{enumerate}
		\item Any linearly independent set of vectors has at most $ n $ elements with equality iff it's a basis
		\item Any spanning set of vectors must have at least $ n $ elements, with equality if and only if it's a basis.
		
	\end{enumerate}
\end{cor}

Slogan ``Choose the best basis for the job''

\begin{thm} 
	Let $ U,W $ be subspaces of $ V $. If $ U $, $ W $ are finite dim, so is $ U + W $ and $ \dim (U + W)   = \dim U + \dim W - \dim (U \cap W)  $
\end{thm}

\begin{proof}
	Pick basis basis $ v_{1},\cdots,v_{l} $ of $ U \cap W $. Extend it to basis $ v_{1},\cdots,v_{l},u_{1},\cdots,u_{m} $ of $ U $.
	Extend it to basis $ v_{1},\cdots,v_{l},w_{1},\cdots,w_{n} $ of $ W $.
	
	Claim: $ v_{1},\cdots,v_{l},u_{1},\cdots,u_{m},w_{1},\cdots,w_{n} $ is a basis for $ U + W $.
	\begin{enumerate}
		\item Span: $ u \in U $, then $ u = \sum \alpha_{i} v_{i}  + \sum _{\beta_{i} u_{i}} $, $ \alpha_{i},\beta_{i} \in \F $
		$ w \in W $, then $ w = \sum \gamma_{i} v_{i}  + \sum _{\delta_{i} w_{i}} $, $ \gamma_{i},\delta_{i} \in \F $
		
		\[ u + w  = \sum  (\alpha_{i} + \gamma_{i})v_{i}   + \sum (\beta_{i} + \delta_{i} )u_{i} \]
		
		\item lin indep: $ u = \sum \alpha_{i} v_{i}  + \sum _{\beta_{i} u_{i}} + \sum \gamma_{i} w_{i} = \mathbf{0} $
		
		\[ \Rightarrow  u =  \underbrace{\sum \alpha_{i} v_{i}  + \sum \beta_{i} u_{i}}_{\in U} = \underbrace{- \sum \gamma_{i} w_{i} }_{\in W}  \in U \cap W  \]
		
		This is equal to $ \sum \delta_{i} v_{i} $ for some $ \delta_{i} \in \F $ because $ v_{i} $ are basis for $ U \cap W $.
		
		AS $ v_{i} $ and $ w_{i} $ are lin indep, $ (*) \Rightarrow \gamma_{i} = \delta_{i} = 0 $ for all $ i $.
		
		$ \Rightarrow \sum \alpha_{i} v_{i} + \sum \beta_{i} u_{i} = 0 \Rightarrow \alpha_{i} = \beta_{i} = 0  $ because $ v_{i} $ and $ u_{i} $ rom a basis for $ U $.
		
		
 		
	\end{enumerate}
\end{proof}


\begin{thm} 
	Let $ V $ be a finite dim $ \F $-vector space, $ U \leq V $, then $ U $ and $ V / U $ are also of finite dim, and
	
	\[ \dim V = \dim U + \dim V / U \]
\end{thm}

\begin{proof}
	\begin{ex}
		Show that $ U $ is finite dim.
	\end{ex}


Let $ u_{1},\cdots,u_{l} $ be a basis for $ U $. Extend it to a basis for $ V $. Say $ u_{1},\cdots,u_{l},w_{l+1},\cdots,w_{n} $ of $ V $.
\begin{ex}
	Check: $ w_{l+1} + U, \cdots, w_{m} + U $ form a basis for $ V / U $.
\end{ex}

\end{proof}

\begin{cor} 
	If $ U $ is a proper subspace of $ V $, $ V $ is finite dimensional, $ \dim U < \dim V $.
	
	\begin{proof}
		$ V / U \neq \{ \mathbf{0} \} \Rightarrow \dim V/U  > 0 \Rightarrow \dim U < \dim V$
	\end{proof}
\end{cor}

\begin{defi}
	Let $ V $ be an $ \F $-vector space, $ U,W \leq V $
	Then $ V = U + \oplus W $ ($ V $ is an internal direct sum of $ U $ and $ W $) if every element of $ V $ can be written as $ v  = u + w, w \in W, u \in U $, uniquely.
	
	$ W $ is a \emph{direct compliment} of $ U $ in $ V $
\end{defi}

\begin{lemma} 
	$ U,W \leq V $. The following are equivalent
	
	\begin{enumerate}
		\item $ V = U \oplus W $, ie. every element of $ V $ can be written uniquely as $ u + w, $ for  $u \in U, w \in W $
		\item $ V = U + W $ and $ U \cap W = \{ \mathbf{0} \} $
		\item $ B_{1} $ any basis of $ U $, $ B_{2} $ is any basis of $ W $, then $ B = B_{1} \cup B_{2} $ is a basis of $ V $.
		
	\end{enumerate}
\end{lemma}


\begin{proof}
	(ii) $ \Rightarrow $ (i). Any $ v \in V $ is $ u + w $ for some $ u \in U $, $ w in W $.
	
	Suppose that 
	

	\[ u_{1} + w_{1} = u_{2} + w_{2} \]
	
	
	Then 
	\[ \Rightarrow u_{1} - u_{2} = -w_{1} + w_{2} \in U \cap W = \{ \mathbf{0} \} \Rightarrow w_{1} = w_{2}, u_{1} = u_{2} \]
	
	Thus uniqueness of expressions.
	
	
	(i) $ \Rightarrow $ (iii) $ B $ spans, any $ v \in V $ is $ u + w $, for some $ u \in U $, $ w \in W $, write $ u $ in terms of $ B_{1} $, $ w $ in terms of $ B_{2} $, Then $ u + w $ is a lin comb. of elements of $ B $.
	
	$ B $ indep? \[ \sum_{v \in B} \lambda_{v} v = \mathbf{0} = \mathbf{0}_{v} + \mathbf{0}_{w} \]
	
	\[ \underbrace{\sum_{v \in B_{1}} \lambda_{v} v}_{\in U} + \sum_{v \in B_{2}} \lambda_{v} v \]
	
	%under, in U, W
	
	By uniqueness of expressions, 
	
	\[ \sum_{v \in B_{1}} \lambda_{v} v = \mathbf{0}_{U} \qquad \sum_{v \in B_{2}} \lambda_{v} v = \mathbf{0}_{W} \]
	
	AS $ B_{1} $ and $ B_{2} $ are basis, all of the $ \lambda_{v} $ are zero. 
	
	
	(iii) $ \Rightarrow $ (ii). If $ v \in V $, $ v = \sum_{x \in B}  \lambda_{x} x = \underbrace{\sum_{u \in B} \lambda_{u} u }_{\in U} + \underbrace{\sum_{w \in B_{2}}   \lambda_{w} w }_{\in W}  $
	
	$ \Rightarrow v \in U + W $.
	
	
	If $ v \in U \cap W $, $ v = \sum_{u \in B_{1}}  \lambda_{u} $ , $u = \sum_{w \in B_{2}}  \lambda_{w} w  \Rightarrow $ All $ \lambda_{u},\lambda_{w} $ are zero, because $ B_{1} \cup B_{2}  $ is lin. indep.
	
\end{proof}
	
	\begin{lemma} 
		Let $ V $ be an f-dim vector space. $ U \leq V $.
		Then there exists a direct compliment to $ U $ in $ V $
		
	\end{lemma}

\begin{proof}
	Let $ u_{1},\cdots,u_{l} $ be a basis for $ U $. Extend it to a basis for $ V $, 
	
	\[ u_{1},\cdots,u_{l},w_{l+1},\cdots,w_{n} \]
	
	Then $ <w_{l+1},\cdots,w_{n}>$ is a direct compliment of $ U $. 
\end{proof}

Note! Direct compliments are not at all unique. In general, if you pick different ways of extending this you will get different direct compliments. 

Pick $ V = \R^{2} $. Pick $ U $ as the $ y $-axis, then any one of the following green lines are direct compliments.:

\begin{defi}
	Def $ v_{1},\cdots,v_{l} \leq V $,
	
	\[ \sum V_{i} = V_{1} + \cdots + V_{l} = \{  v_{1} + \cdots + v_{l} \; | \; v_{i} \in V_{i} \} \]
	
	The sum is direct if
	
	\[ v_{1} + \cdots + v_{l} = v_{1}' + \cdots + v_{l}' \Rightarrow v_{i} = v_{i}' \text{ for all} l \] (``unique expressions'')
	
	Notation:
	
	\[ \bigoplus_{i=1}^{l} V_{i} \]
\end{defi}
	
\begin{ex}
	$ V_{1},\cdot,V_{l} \leq V $. TFAE
	
	\begin{enumerate}
		\item The sum $ \sum V_{i} $ is direct
		\item $ V_{i} \cap \sum_{j \neq i}   V_{j} = \{ \mathbf{0} \} $ for all $ i $
		\item The $ B_{i} $ are pairwise disjoint and their union is a basis for  $ \sum V_{i}  $
		
	\end{enumerate}


\end{ex}


We also discuss \emph{external} direct sums, though will not touch them much in this course. This is simply an internal direct sum $ U_{1} \oplus U_{2} $, except now the $ U_{i} $'s are not subspaces of $ V $, they can be any old vector space. 


\begin{defi}
	Let $ U,W $ be $ \F $-vector spaces.
	External direct sum
	
	\[ U \oplus V  = \{  (u,w) \; | \; u \in U, w \in W \}\]
	
	with $ (u,w) + (x,y) = (u + x, w + y) $,
	
	$ \lambda(u,w) = (\lambda u, \lambda w) $
\end{defi}

Note that when we talk about dimension in this course, we have not shown yet that the dimension of an \emph{infinite} vector space is well defined\footnote{It is!}. We will come to this later.



\section{Linear Maps}

\subsection{Linear Maps}

\begin{defi}
	$ V,W $ are $ \F $-vector spaces. A map $ \alpha: V \to W $ is linear if
	
	\begin{enumerate}
		\item $ \alpha(v_{1} + v_{2}) = \alpha(v_{1}) + \alpha(v_{2}) $
		\item $ \alpha(\lambda v) = \lambda \alpha(v) $
		
	\end{enumerate}

Can be combined concisely as: 

\[  \alpha(\lambda_{1} v_{1} + \lambda_{2} v_{2}) = \lambda_{1} \alpha(v_{1}) + \lambda_{2} \alpha(v_{2})  \quad \lambda_{i} \in \F, v_{i} \in V  \]
\end{defi}

\begin{eg}
	A $ n \times m $ matrix with coeff in $ \F $
	
	\[ \alpha: \F^{n} \to \F^{m} \]
	\[ v \mapsto A v \]
\end{eg}

\begin{eg}
	The set of all polynomials with real coefficients:
	\[ \mathcal{D} : \mathcal{P}(\R) \to \mathcal{P}(\R) \]
	
	\[ f \mapsto \frac{\d f}{\d x} \]
\end{eg}


\begin{eg}
	The set of continuous functions over $ [0,1] $
	\[ I: \mathcal{C}[0,1] \to \mathcal{C}[0,1] \]
	\[ f \mapsto I(f) \]
	
	where $ I(f)(x) = \int_{0}^{x} f(t) \; \d t $
\end{eg}

\begin{eg}
	Fix $ x \in [0,1] $
	
	\[ \mathcal{C}[0,1] \to \R \]
	\[ f \mapsto f(x) \]
\end{eg}

Notes: If $ U,V,W $ are v spaces over $ \F $, then

\begin{enumerate}
	\item The identity map id: $ V \to V $ is linear
	\item If \begin{tikzcd}
		U \ar[r, "\alpha"] & V \ar[r, "\beta"] & W 
	\end{tikzcd} with $ \alpha,\beta $ both linear, then $ \beta \circ \alpha $ is linear.
	
\end{enumerate}

\begin{lemma} 
	Let $ V,W $ be $ \F $-vector spaces, and let $ \mathcal{B} $ is a basis for $ V $.
	If $ \alpha_{0} : \mathcal{B} \to W $ is \emph{any} map, then there exists a unique linear map\footnote{ie. if I tell you \emph{any} mapping of the basis vectors $ \alpha_{0} $ (it could be a non-linear mapping), then you have enough information to construct a linear map from this. } $ \alpha : V \to W $ extending $ \alpha_{0} $, ie. 
	
	\[ \alpha(v) = \alpha_{0}(v) \]
	
	for any basis element $ v \in \mathcal{B} $. 

	
\end{lemma}


\begin{proof}
	Let $ v  \in V $. Then $ v = \sum \lambda_{i} v_{i} $, $ v_{i} \in B $, $ \lambda_{i} \in \F $, unique expression. 
	
	Now Linearity forces
	
	\begin{align*}
	\alpha(v) & = \alpha\left( \sum \lambda_{i} v_{i} \right) \\
	& = \sum \lambda_{i} \alpha(v_{i})\\
	& = \sum \lambda_{i} \alpha_{0}(v_{i}) \\
	\end{align*}
	
	linear, exists.
	expression forced to be unique. 
\end{proof}

Note

\begin{enumerate}
	\item True for infinite dimensional vector space also
	\item Very often, to define a linear map, define it on a basis and `extend linearly'
	\item Let $ \alpha_{1},\alpha_{2} : V \to W $ be linear maps. If they agree on any basis, then they are equal. 
\end{enumerate}


\begin{defi} (Isomorphism)
	
	Let $ V,W $ be vector spaces over $ F $. The map $ \alpha : V \to W $ is an \emph{isomorphism} if it is linear and bijective. 
	Notation: $ V \simeq W $
\end{defi}

\begin{lemma} 
	$ \simeq $ is an equivalence notation on the set (score out set and write class) of all vector spaces over $ \F $. That is,
	
	\begin{enumerate}
		\item $ i_{V} : V \to V $ is an iso
		\item If $ \alpha : V \to W $ is an iso, then the inverse map $ \alpha^{-1} : W \to V $ is also linear, hence an iso.
		\item If 
		
		
		
		 \begin{tikzcd}
			U \ar[r, "\beta"] & V \ar[r, "\alpha"] & W 
		\end{tikzcd}
		
	
		then 
		
		\begin{tikzcd}
			U \ar[r, "\beta \circ \alpha"] & W  
		\end{tikzcd}
		
		is also an iso 
		
	\end{enumerate}
	
\end{lemma}

\begin{proof}
	\begin{enumerate}
		\item immediate
		\item $ \alpha $ bijective $ \Rightarrow \alpha^{-1} $ exists. Check: linear. 
		$ w_{2} \in W, w_{2} = \alpha(v_{2}) $, $ v_{2} \in V $, unique.
		$ \alpha^{-1}(w_{1} + w_{2}) = \alpha^{-1}(\alpha(v_{1}) + \alpha(v_{2})  = \alpha^{-1} (\alpha(v_{1} + v_{2})) = v_{1} + v_{2} = \alpha^{-1}(w_{1})  + \alpha^{-1}(w_{2}) $.
		
		Similarly, $ \lambda \in \F $, $ w \in W $, 
		
		\[ \alpha^{-1} (\lambda w) = \lambda \alpha^{-1} (w) \]
		
		\item immediate
		
	\end{enumerate}
\end{proof}



\begin{thm} 
	If $ V $ vector space over $ \F $ of dimension $ n $, then $ V \simeq \F^{n} $.
	
\end{thm}

\begin{proof}
	Choose a basis $ \mathcal{B} $ for $ V $, say $ v_{1},\cdots,v_{n} $
	
	\[ V \to \F^{n} \]
	
	\[ \sum \lambda_{i}v_{i} \mapsto \begin{pmatrix}
	\lambda_{1} \\
	\vdots \\
	\lambda_{n}
	\end{pmatrix} \text{ is an iso}\]
\end{proof}

Remark: Choosing an iso $ V \simeq F^{n} $ is equivalent to choosing a basis for $ V $.

\begin{thm} 
	$ V,W $ $ v $ spaces over $ \F $, finite dim, are isomorphic iff they have the same dimension 
\end{thm}

\begin{proof}
	($ \Leftarrow $) Both $ V $ and $ W $ are isomorphic 
	
	\[ \F^{\dim V} = \F^{\dim W} \]
	
	($ \Rightarrow $) Let $ \alpha : V \to W $ iso, $ \mathcal{B} $ a basis for $ V $.
	
	Claim: $ \alpha(\mathcal{B}) $ is a basis for $ W $. 
	
	
	\begin{ex}
		$ \alpha(\mathcal{B}) $ spans $ W $ because of surjectivity of $ \alpha $.
	\end{ex}
	
	\begin{ex}
		$ \alpha(\mathcal{B}) $ lin indep: follows from injectivity of $ \alpha $.
	\end{ex}
\end{proof}

\begin{defi} (Null space/ Kernel of a linear map)
	Let $ \alpha : V \to W $ be a linear map,the \emph{null space} of $ \alpha $ is given by 
	\[ N(\alpha) = \ker \alpha = \{  v \in V \; | \; \alpha(v) = \mathbf{0} \} \leq V \]
\end{defi}	
	
\begin{defi} (Image of a linear map)
			Let $ \alpha : V \to W $ be a linear map, the \emph{image} of $ \alpha $ is defined as:
			  \[ \Im(\alpha) = \{ w \in W \; | \; w = \alpha(v), \text{ some } v \in V  \} \leq W   \]
\end{defi}


\begin{defi} (Injective map)
	$ \alpha $ is injective if and only if $ N(\alpha) = \{ \mathbf{0} \} $
\end{defi}

\begin{defi} (Surjective map)\footnote{I mean, all definitions are iff statements really. Sometimes we leave it out and just use `if'}
$ 	\alpha $  is surjective if and only if $ \Im(\alpha) = W $
\end{defi}

\begin{eg}
	Let $ \alpha : {\mathcal{C}^{\infty}} (\R) \to \mathcal{C}^{\infty} (\R) $ be defined by 
	
	\[ \alpha(f)(t) = f''(t) + 2f'(t) - 5f  \] 
	
	
	\[  \ker \alpha \text{ is solns to } f'' + 2f' + 5f = 0 \]
	
	\[ g \in \Im \alpha \text{ if } \; \exists \text{ soln} f \text{ to } f'' + 2f' + 5f = g  \]
\end{eg}

\subsection{The First Isomorphism Theorem}

\begin{thm} (First Isomorphism Theorem)
	Let $ \alpha : V \to W $ be a linear map. It induces an iso :
	
	\begin{center}
		\begin{tikzcd}
			V/ \ker \alpha  \ar[r, "\overline{\alpha}"] & \Im(\alpha) 
		\end{tikzcd}
	\end{center}

	defined by
	
	
	\[ \overline{\alpha} (v + \ker \alpha) = \alpha(v) \]
	
	
\end{thm}

\begin{proof}
	\begin{enumerate}
		\item  $ \overline{\alpha} $ is well defined:
	
	\begin{align*}
	& v + \ker \alpha = v' + \ker \alpha \\
	\iff &  v - v' \in \ker \alpha \Rightarrow \alpha(v) \\
	\Rightarrow &  \alpha(v) = \alpha(v')
	\end{align*}
	
	\item $ \overline{\alpha} $ is linear; immediate from linearity of $ \alpha $.
	
	\item $ \overline{\alpha} $ bijective? 
	
	\begin{align*}
	& \overline{\alpha}(v + \ker \alpha  ) = \mathbf{0} \\
	\Rightarrow & \alpha(v) = 0 \\
	\Rightarrow &  v \in \ker \alpha 
	\end{align*}
	
	
	\item surjective: by defn of $ \Im(\alpha) $.		
	\end{enumerate}


	
	
\end{proof}


\begin{defi} (Rank and Nullity of a linear map)
	The \emph{rank} of a linear map $  r(\alpha) = rk(\alpha) $ is given by $ \dim (\Im \alpha)  $, and the \emph{nullity} $ n(\alpha) $ is likewise given as $ \dim(N(\alpha)) $
\end{defi}

\begin{thm} (Rank-nullity theorem)
	Let $ U,V $ be vector spaces over $ \F $, $ \dim_{\F} U < \infty $.Let $ \alpha : U \to V $ linear. Then:
	
	\[ \dim U = r(\alpha) + n(\alpha) \]
\end{thm}


\begin{proof}
	\[ U / \ker \alpha \simeq \Im (\alpha) \Rightarrow \dim(U) - \dim (\ker \alpha) = \dim (\Im(\alpha) )\]
\end{proof}

\begin{lemma} 
	Let $ V,W $ be v spaces over $ \F $, of equal finite dim. Let $ \alpha : V \to W $ linear. 
	
	TFAE
	
	\begin{enumerate}
		\item $ \alpha $ injective
		\item $ \alpha $ surjective
		\item $ \alpha $ isomorphism
		
	\end{enumerate}
\end{lemma}

\begin{defi}
	The \emph{space of linear maps} from $ V $ to $ W $ is denoted by 
	
	\[ L(V,W) = \{ \alpha : V \to W \text{ linear}\} \]
\end{defi}

\begin{prop} 
	$ L(V,W) $ is a v-space over $ \F $ under operators
	
	\begin{itemize}
		\item $ (\alpha_{1} + \alpha_{2})(v) = \alpha_{1} (v) + \alpha_{2} v $ for all $ \alpha_{i} \in L(V,W) $
		\item $ (\lambda \alpha  ) (v) = \lambda ( \alpha (v)) $ for all $ v \in V $, $ \lambda \in \F $
	\end{itemize}


If both $ V $ and $ W $ are finite dim, then so is $ L(V,W) $ and $ \dim (L(V,W))  = \dim(V) \times \dim(W) $.

\end{prop}

\begin{proof}
	$ \alpha_{1} + \alpha_{2} $, $ \lambda \alpha $ defined above are well-defined linear maps. The v-space axioms are satisfied.
	
	Claim about finite dim: See later
\end{proof}

\subsection{Representation of Linear Maps by Matrices }


\begin{defi}
	An $ m \times n $ matrix over $ \F $ is an array with $ m $ rows and $ n $ columns, entries in $ \F $.
	
	\[ A = (a_{ij}), \quad a_{ij} \in F, \quad 1 \leq i \leq m, 1 \leq j \leq n \]
	
	$ M_{m,n}(\F) $ is the set of all such matrices
\end{defi}

\begin{prop} $ M_{m,n}(\F) $ is an $ \F $ vector space, under operations
	
	\item $ (a_{ij}) + (b_{ij}) = (a_{ij} + b_{ij}) $
	\item $ \lambda(a_{ij}) = (\lambda a_{ij}) $
	\item and $ \dim (M_{m,n}(\F)) = m \times n $
	
\end{prop}

\begin{proof}
	$ v $-space okay, see 1.1. And dim? A standard basis for $ M_{m,n}(\F) $ is 
	
	\[ E_{ij} =  \begin{pmatrix}
	0 & \cdots & 0 \\
	\vdots & 1 &  \vdots\\
	0 & \cdots & 0\\


	\end{pmatrix} \]
	
	(ie a matrix of zeroes, with 1 in $ i^{\text{th}}  $ row and $ j^{\text{th}}$ column )
	
	
	$ (a_{ij}) = \sum_{ij}  a_{ij} E_{ij} $, from which span and LI follows
	
	This basis has cardinality $ mn $
	
\end{proof}


\begin{defi} (Coordinate Vectors)
	
	Let $ V,W $ be v-spaces over $ \F $, of finite dim, with $ \alpha : V \to W $, linear. Basis $ \mathcal{B} $ for $ V $, $ v_{1},\cdots,v_{n} $ basis $ \mathcal{C} $ for $ W $, $ w_{1},\cdots,w_{n} $. If $  v \in V $, $ v = \sum \lambda_{i} v_{i} $, write $ [v]_{\mathcal{B}} = \begin{pmatrix}
\lambda_{1} \\
\vdots \\
\lambda_{n}
\end{pmatrix} \in \F$, called coordinate vector of $ v $ wrt $ \mathcal{B} $. Similarly, $ [w]_{\mathcal{C}} \in \F^{m} $.
\end{defi}

\begin{defi} (Matrix)
	$ [\alpha]_{\mathcal{B},\mathcal{C}} $ matrix of $ \alpha $ wrt $ \mathcal{B} $ and $ \mathcal{C} $
	
	
	\begin{align*}
	[\alpha]_{\mathcal{B},\mathcal{C}} & = \left(   [\alpha(v_{1})]_{\mathcal{C}} \; | \; [\alpha(v_{2})]_{\mathcal{C}} \; | \; \cdots \; | \; [\alpha(v_{n})]_{\mathcal{C}} \right) \in M_{m,n}(\F)  \\
	& = (a_{ij})
	\end{align*}
\end{defi}

The notation says $ \alpha(v_{j}) = \sum \alpha_{ij} w_{i} $

\begin{lemma} For any $ v \in V $, 
	
	\[ [\alpha(v)]_{\mathcal{C}} = [\alpha]_{\mathcal{B},\mathcal{C}} \cdot [v]_{\mathcal{B}} \]
	
	where the dot denotes matrix applied to vector
	
\end{lemma}

\begin{proof} Fix $ v \in V $, $ v = \sum_{j=1}^{n} \lambda_{j}v_{j} $, so $ [v]_{\mathcal{B}} = \begin{pmatrix}
	\lambda_{1} \\
	\vdots \\
	\lambda_{n}
	\end{pmatrix} $
	
	
	\begin{align*}
	\alpha(v) & = \alpha( \sum \lambda_{j} v_{j} )  = \sum \lambda_{j} \alpha(v_{j}) = \sum_{j} \lambda_{j} (\sum_{i} \alpha_{ij} w_{i}  )  \\
	& = \sum_{i} \underbrace{\left(  \sum_{j} \alpha_{ij} \lambda_{j}  \right)}_{i^{\text{th}} \text{ entry of } [\alpha]_{\mathcal{B},\mathcal{C}} [v]_{\mathcal{B}}  } w_{i}
	\end{align*}
	
\end{proof}

\begin{lemma} 
	Let $ \alpha $, $ \beta $ be linear maps, with
	\begin{tikzcd}
		U \ar[r, "\beta"] & V \ar[r, "\beta"] & W 
	\end{tikzcd}
	and $ \alpha \circ \beta $ linear. Let $ \mathcal{A},\mathcal{B},\mathcal{C} $ be basis for $ U,W,V $ reps. Then
	
	\[  [\alpha \circ \beta]_{\mathcal{A},\mathcal{C}} = \underbrace{[\alpha]_{\mathcal{B},\mathcal{C}}}_{= (a_{ij})} \circ \underbrace{[\beta]_{\mathcal{A},\mathcal{B}}}_{= (b_{ji})}\]

\end{lemma}

\begin{proof}
	\begin{align*}
	(\alpha \circ \beta) \overbrace{(u_{i}) }^{\text{in } \mathcal{A}} & = \alpha(\beta(u_{i})) = \alpha (\sum_{j} b_{ji}\overbrace{ v_{j}}^{\text{in } \mathcal{B}} ) \\
	& = \sum_{j} b_{ji} \alpha(v_{j})\\
	& = \sum_{j} b_{ji} \sum_{i} a_{ij} \overbrace{w_{i}}^{\text{in } C} \\
	& = \sum_{i} \underbrace{\left(  \sum_{j} a_{ij} b_{ji} \right)}_{(i,j)^{\text{th}} \text{ entry of } [\alpha]_{\mathcal{B},\mathcal{C}} [\beta]_{\mathcal{A},\mathcal{B}}  }    w_{i} 
	\end{align*}
\end{proof}


\begin{prop} 
	If $ V,W $ are v-spaces over $ \F $ with $ \dim V = n $, $ \dim W = m $, then $ L(V,W) \simeq M_{m,n}(\F)  $
\end{prop}

\begin{proof}
	Fix bases 
	\[ \mathcal{B} \text{ of } V: v_{1},v_{2},\cdots,v_{n}  \]
	\[ \mathcal{C} \text{ of } V: w_{1},v_{2},\cdots,v_{n}  \]
	
	Claim: 
	
	\[ L(v,w) \to M_{m,n}(\F) \]
	\[ \alpha \mapsto [\alpha]_{\mathcal{B},\mathcal{C}} \]
	
	is an iso. 
	
	\begin{itemize}
		\item $ \theta $ linear $ [\lambda_{1} \alpha_{1} + \lambda_{2}\alpha_{2}]_{\mathcal{B},\mathcal{C}} = \lambda_{1}[\alpha_{1}]_{\mathcal{B},\mathcal{C}} + \lambda_{2} [\alpha_{2}]_{\mathcal{B},\mathcal{C}}  $
		\item $ \theta $ surjective: given $ A = (a_{ij})$. Let $ \alpha : v_{j} \mapsto \sum_{i=1}^{m} a_{ij}w_{i} $, and extend linearly. Then $ \alpha \in L(V,W),b  \theta(\alpha) = A$.
		\item $ \theta $ injective, $ [\alpha]_{\mathcal{B},\mathcal{C}} = 0 $ matrix $ \Rightarrow \alpha $ is zero-map from $ V $ to $ W $.
	\end{itemize}
\end{proof}

\begin{cor} 
	\[ \dim(L(V,W)) = (\dim V)(\dim W) \]
\end{cor}

\begin{eg}
	$ \alpha : V \to W, Y \leq V, Z \leq W$. Say $ \alpha(Y) \subseteq Z $.
	
	Basis of $ V: $
	
	\[ \mathcal{B}: \; \underbrace{v_{1},\cdots,v_{k}}_{\text{Basis for } Y, \; \mathcal{B}'},v_{k+1},\cdots,v_{n}  \]
	
	
	Basis of $ W: $
	
	\[ \mathcal{C}: \; \underbrace{w_{1},\cdots,w_{k}}_{\text{Basis for } Z, \; \mathcal{C}'},w_{k+1},\cdots,w_{m}  \]
	
	Then 
	
	\[ [\alpha]_{\mathcal{B},\mathcal{C}}  = \begin{pmatrix}
	A & \cdots & B_{1} \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & C_{1}  
	\end{pmatrix}\]
	
	because for $ 1 \leq j \leq k $, $ \alpha(v_{j}) $ is a lin combo of $ w_{i} $, where $ 1 \leq i \leq l $. 
	
	And
	
	\[ [\alpha |_{y}]_{\mathcal{B}',\mathcal{C}'} = A_{1} \]
	
	Claim: $ \alpha $ induces 
	
	\[ \overline{\alpha} : V / Y \to W / Z \]
	
	\[ v + Y \mapsto  \alpha(v) + Z  \]
	
	Well defined? 
	
	\begin{align*}
	v_{1} + Y = v_{2} + Y & \Rightarrow v_{1} - v_{2} \in Y \\
	& \Rightarrow \alpha(v_{1} - v_{2}) \in Z \\
	& \Rightarrow \alpha(v_{1})  + Z = \alpha(v_{2}) + Z
	\end{align*}
	
\begin{ex}
		Linear from linearity of $  \alpha $
\end{ex}

Basis for $ Y / V $,

\[ \mathcal{B}'' : v_{k+1} + Y, \cdots, v_{n} + Y \]

Basis for $ W / Z $, 

\[ \mathcal{B}'' : v_{k+1} + Y, \cdots, v_{n} + Y \] 

\begin{ex}
	$ [\overline{\alpha}]_{\mathcal{B}'',\mathcal{C}''} $
\end{ex}

\end{eg}






\subsection{Change of Basis}

Let $ V $ and $ W $ be v-spaces over $ \F $ with the following basis


\[ V \qquad \qquad \qquad  W  \]
\[ \mathcal{B} = \{ v_{1},\cdots,v_{n} \} \quad C = \{ w_{1},\cdots,w_{m} \} \]
\[ \mathcal{B}' = \{ v_{1}',\cdots,v_{n}' \} \quad C' = \{ w_{1}',\cdots,w_{m}' \} \]

\begin{defi}
	The \emph{change of basis matrix} from $ \mathcal{B} $ to $ \mathcal{B}' $ is $ P = (p_{ij}) $ given by $ v_{j}' = \sum p_{ij}v_{i} $.
	
	Equivalently,
	
	\[ P = \begin{pmatrix}
	& & & & & & \\
	[v_{1}']_{\mathcal{B}} & \big| & [v_{2}']_{\mathcal{B}} & \cdots &  & \big| & [v_{n}']_{\mathcal{B}} \\
	& & & & & &
	\end{pmatrix} = [\id]_{\mathcal{B}',\mathcal{B}}  \]	
	
\end{defi}

\begin{lemma} 
	$ [v]_{\mathcal{B}} = P [v]_{\mathcal{B}'} $
\end{lemma}

\begin{proof}
	\[ P [v]_{\mathcal{B}'} = [\id]_{\mathcal{B}',\mathcal{B}}[v]_{\mathcal{B}'} = [v]_{\mathcal{B}} \] 
\end{proof}

\begin{lemma} 
	$ P $ is an invertible $ n \times n $ matrix, and $ P^{-1} $ is the change of basis matrix from $ \mathcal{B} $ to $ \mathcal{B}' $
\end{lemma}

\begin{proof}
	\[  [\id]_{\mathcal{B},\mathcal{B}'} [\id]_{\mathcal{B}',\mathcal{B}} =  [\id]_{\mathcal{B}',\mathcal{B}'}  = I_{n}\]
	
		\[  [\id]_{\mathcal{B'},\mathcal{B}} [\id]_{\mathcal{B},\mathcal{B}'} =  [\id]_{\mathcal{B},\mathcal{B}}  = I_{n}\]
	
\end{proof}

Let $ Q $ be the change of basis matrix from $ C' $ to $ C $. $ Q $ also invertible $ m \times m $.

\begin{prop} 
	Let $ \alpha : V \to W $ linear, $ A = [\alpha]_{\mathcal{B},\mathcal{C}} $, $ A' = [\alpha]_{\mathcal{B}',\mathcal{C}'}  $. Then
	
	\[ A' = Q^{-1} A P \]
\end{prop}

\begin{proof}
	\begin{align*}
	Q^{-1} A P & = [\id]_{\mathcal{C},\mathcal{C'}} [\alpha]_{\mathcal{B},\mathcal{C}} [\id]_{B',B}  \\
	& = [\id \circ \alpha \circ \id ]_{\mathcal{B}',\mathcal{C}'} \\
	& = A'
	\end{align*}
\end{proof}

\begin{defi}
	$ A,A' \in M_{m,n}(\F) $ are \emph{equivalent} if $ A' = Q^{-1} A P $ for some invertible $ P \in M_{n,n}(\F), Q \in 	M_{m,m} (\F) $
\end{defi}

Note: this defines an equivalence relation on $ M_{m,n}(\F) $, eg. $ A' = Q^{-1} A P, A'' = (Q^{-1})^{-1} A' P' \Rightarrow A'' = (Q Q^{-1})^{-1} A P P' $


\begin{prop} 
	Let $ V,W $ be $ \F $-vector spaces of $ \dim n $ and $ m $ resp. Let $ \alpha : V \to W $ be a linear map. Then there exists bases $ \mathcal{B} $ of $ V $ and $ \mathcal{C} $ of $ W $, and some $ r \leq m, n $ st. 
	
		\[ [\alpha]_{\mathcal{B},\mathcal{C}}  = \begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix}\]
	
	where $ I_{r} $ is the identity matrix. 
	
	Note: $ r = \text{rank}(\alpha) = r(\alpha) $
	
\end{prop}
	
	\begin{proof}
		Fix $ r $ st. $ N(\alpha) $ has $ \dim n - r $. Fix a basis for $ N(\alpha) $, say $ v_{r+1},v_{r+2},\cdots,v_{n} $. Extend this to a basis for $ V $, say $ \underbrace{v_{1},\cdots,v_{r}}_{\mathcal{B}},v_{r+1},\cdots,v_{n} $. Now $ \alpha(v_{1}),\cdots,\alpha(v_{r}) $ is a basis for $ \im(\alpha) $.
		
		\begin{itemize}
			\item span: $ \alpha(v_{1}),\cdots,\alpha(v_{r}),\underbrace{\alpha(v_{r+1})}_{=0},\cdots,\underbrace{\alpha(v_{n})}_{=0} $ certainly span $ \imath(\alpha) $
			
			\item LI: 
			
		\begin{align*}
			\sum_{i=1}^{r} \lambda_{i} \alpha(v_{i}) = \mathbf{0} & \Rightarrow \alpha \underbrace{\left(  \sum_{i=1}^{r} \lambda_{i} v_{i} \right)}_{\in \ker(\alpha)} = \mathbf{0}   \\
		& \Rightarrow \sum_{i=1}^{r} \lambda_{i}v_{i} = \sum_{j=r+1}^{n} \mu_{j} v_{j} \text{ some } \mu_{j} \in \F \\
		& \Rightarrow \text{ as } v_{1},\cdots,v_{n} \text{ LI }, \lambda_{i} = \mu_{j} = 0 \; \forall \; i,j 
		\end{align*}
		\end{itemize}
	
	Extend $ \alpha(v_{1}),\cdots,\alpha(v_{r}) $ to a basis of $ W $, say $ C $. By construction, 
	
		\[ [\alpha]_{\mathcal{B},\mathcal{C}}  = \begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix}\]
	
	Remark: didn't need to assume in the proof that $ r = r(\alpha) $. Can think of this as giving a different proof of the r-n theorem.
	
		
		
		 	

\end{proof}

\begin{cor} 
	Any $ m \times n $ matrix is equivalent to $ \begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix} $ for some $ r $.
\end{cor}

\begin{defi}
	Let $ A \in M_{m,n}(\F) $. The \emph{column rank} of $ A $ is the dimension of the subspace of $ \F^{m} $ spanned by the columns of $ A $. The \emph{row rank} of $ A $ is the column rank of $ A^{T} $ (the dimension of the subspace of $ \F^{n} $ spanned by the row vectors of $ A $).
\end{defi}

Note: if $ \alpha $ is a linear map represented by $ A $ wrt. any choice of basis, then $ r(\alpha) = r(A) $, ie column rank = rank.

\begin{prop} 
	Two $ m \times n $ matrices $ A,A' $ are equivalent iff $ r(A') = r(A) $. 
\end{prop}

\begin{proof}
	 ($ \Leftarrow $) Both $ A $ and $ A' $ are equivalent to $ \begin{pmatrix}
	 I_{r} & \cdots & 0 \\
	 \vdots & \ddots & \vdots \\
	 0 & \cdots & 0  
	 \end{pmatrix} $, $ r = r(A') = r(A) $ (this is a transitive relation)
	 
	 ($ \Rightarrow $) Let $ \alpha $ be the linear map: $ \alpha : \F^{n}  \to \F^{m} $ represented by $ A $ wrt. the standard basis $ A' = Q^{-1} A P$. $ P $ and $ Q $ invertible, so $ A' $ represents $ \alpha $ wrt. two other bases. $ r(\alpha) $ is defined in a basis invariant way, so $ r = r(\alpha) = r(A) = r(A') $

\end{proof}



\begin{thm} 
	$ r(A) = r(A^{T}) $ ("row rank = column rank").
\end{thm}

\begin{proof}
	$ Q^{-1} A P = \begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix}_{m,n} $ where $ Q,P $ invertible 
	
	Take transpose of whole equation:
	
	\begin{align*}
	\begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix}_{n,m} & = (Q^{-1} A P)^{T} \\
	& = P^{T} A^{T} (Q^{T})^{-1}
	\end{align*}
	
	so $ A^{T} $ equiv to $ 	\begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  
	\end{pmatrix}_{n,m} $. Thus $ r(A) = r(A^{T})  $.
	
	
	
\end{proof}



$ V = W $, $ \mathcal{C} = \mathcal{B} $, other basis $ \mathcal{B} $'. $ P $ change of basis matrix form $ \mathcal{B} $ to $ \mathcal{B}' $, $ \alpha \in L(V,V) $. 

\[ [\alpha]_{\mathcal{B}',\mathcal{B}'} = P^{-1} [\alpha]_{B,B}  P \]

\begin{defi}
	$ A,A' \in M_{n,n}(\F) $, $ A, A' $ are \emph{similar} (or \emph{conjugate}) if $ A'  = P^{-1}  A P $ for  some invertible $ P $.  
\end{defi}

\subsection{Elementary Matrices and Operations}

\begin{defi}
	\emph{Elementary column operators} on an $ m \times n $ matrix $ A $:
	
	\begin{enumerate}
		\item swap columns $ i $ and $ j $ (wlog $ i \neq j $)
		\item replace column $ i $ by $ \lambda $ (column $ i $), $ \lambda \neq 0 $
		\item add $ \lambda $(column $ i $) to column $ j $, $ i \neq j $, $ \lambda \neq 0$.
	\end{enumerate}
\emph{Elementary row operators} analogous (replace `column' by `row')
\end{defi}

Note: all of these operations are reversible. 

Corresponding elementary matrices: effect of performing the column operations on $ I_{n} = n \times n $ id.  For row operations, $ I_{m} $.

\begin{enumerate}
	\item $ \begin{pmatrix}
		1\\
		& \ddots\\
		& & 1\\
		& & & 0 & & & & 1\\
		& & & & 1\\
		& & & & & \ddots\\
		& & & & & & 1\\
		& & & 1 & & & & 0\\
		& & & & & & & & 1\\
		& & & & & & & & & \ddots\\
		& & & & & & & & & 1
	\end{pmatrix} $
	
	The zeros appear in row $ i $, row $ j $.
	
	\item $ \begin{pmatrix}
	1 & & & & & &\\
	& \ddots & & & & & \\
	& & 1 & & & & \\
	& & & \lambda & & & \\
	& & & & 1 & & \\
	& & & & & \ddots & \\
	& & & & & & 1
	\end{pmatrix} $
	
	with $ \lambda $ in the $ i^{\text{th}} $ row
	
	\item $ I_{n} + \lambda E_{ij}  $, where $ E_{ij} $ is defined as $ 1 $ in the $ (i,j) $ position and $ 0 $ everywhere else.
	
\end{enumerate}

An elementary column operation on $ A \in M_{m,n}(\F) $ can be performed by multiplying $ A $ by the corresponding elementary matrix on the right.

\begin{ex}
	\[ \begin{pmatrix}
	1 & 2\\
	3 & 4
	\end{pmatrix} \begin{pmatrix}
	0 & 1 \\
	1 & 0
	\end{pmatrix} = \begin{pmatrix}
	2 & 1\\
	4 & 3
	\end{pmatrix} \]
\end{ex}

For row operations, multiply on the left

\begin{ex}
\[ \begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix} \begin{pmatrix}
1 & 2\\
3 & 4
\end{pmatrix} = \begin{pmatrix}
3 & 4\\
1 & 2
\end{pmatrix} \]	
\end{ex}




\begin{thm} Any $ m \times n $ matrix is equivalent to \[ \begin{pmatrix}
	I_{r} & \cdots & 0 \\
	\vdots & \ddots & \vdots \\
	0 & \cdots & 0  \end{pmatrix} \]  for some $ r $
	
\end{thm}

\begin{proof}
	Start with $ A $. If all entries of $ A $ are $ 0 $, we're done ($ r = 0 $). If not, some $ a_{ij} = \lambda \neq 0 $.
	\begin{itemize}
		\item swap rows $ 1,i $
		\item swap columns $ 1,j $
		\item multiply column 1 by $ \frac{1}{\lambda} $
		
	\end{itemize}
to get $ 1 $ in position $ (1,1) $. Now

\begin{itemize}
	\item add $ (-a_{12})(\text{column } 1) $ to column $ 2 $. 
	\item Similarly clear out all other entries in row 1.
	\item Also use row operations to clear out all other entries in column 1
\end{itemize}

Upshot: get

$ \begin{pmatrix}
1 & 0 & \cdots & 0 \\
0 & & & &\\
\vdots & & \tilde{A} & \\
0 & & & &
\end{pmatrix} $, $ \tilde{A} \in M_{m-1,n-1}(\F) $

Now iterate, to get $ \begin{pmatrix}
I_{r} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & 0  \end{pmatrix} = \overbrace{\underbrace{E_{1}',\cdots,E_{k}'}_{\text{elem row}}}^{Q^{-1}} A \overbrace{\underbrace{E_{1},\cdots,E_{k}}_{\text{elem column}}}^{P}  $


Row/column ops are reversible $ \Rightarrow $ elem matrices are invertible. $ Q: m \times m $ invertible, $ P: n \times n $ invertible.

\end{proof}

Variations:

If you use elementary row operations, can get the row echelon form of a matrix. 

\[ \begin{pmatrix}
1 & 0 & 0 & 0 & a \\
& & 1 & 0 & b\\
& & & 1 & c  
\end{pmatrix} \]

How? Assume $ a_{i1}' = \lambda \neq 0 $ some $ i $.
\begin{itemize}
	\item swap rows 1 and $ i $ $ \Rightarrow $ get $ \lambda $ in $ (1,1) $
	\item divide row 1 by $ \lambda \; \Rightarrow \; 1 $ in $ (1,1) $
	\item use (iii)-type operation to clear out rest of column 1, then move on to second column etc.  
\end{itemize}

\begin{lemma} 
	If $ A $ is $ n \times n $ invertible, we can obtain $ I_{n} $ by using only elementary row operations (or elementary column operations).
\end{lemma}

\begin{proof}
	Induction on number of rows
	
	
	Suppose we have
	
	\[ \begin{pmatrix}
	1 & 0 & 0 & & &  \\
	& & 1 & & 0 & \\
	& & & 1 & & 
	\end{pmatrix} \]
	
	There exists $ j > k $ with $ a_{k+1,j} = \lambda \neq 0 $
	
	If not, 	\[ \begin{pmatrix}
	0 \\
	\vdots \\
	1 \\
	\vdots \\
	0
	\end{pmatrix} \] with 1 in the $ (k+1) $th entry would not lie in the span of the column vectors, which would contradict invertability
	
	
	\begin{itemize}
		\item Swap columns $ k+1 $ and $ j $
		\item Divide column $ k+1 $ by $ \lambda $
		\item Use type 3 operators to clear the other entries of the $ (k+1) $th row. 
		\item now proceed inductively
	\end{itemize}
	
	
	
\end{proof}

Upshot 

\[ A E_{1} E_{2} \cdots E_{l} = I_{n} \Rightarrow A^{-1} = E_{1} E_{2} \cdots E_{l} \]

one recipe for inverses.

\begin{prop} 
	Any invertible matrix can be written as a product of elementary matrices.
\end{prop}


\section{Dual Spaces and Dual Maps}
\subsection{Dual Vector Spaces}

\begin{defi}
	Let $ V $ be a vector space over $ \F $. The \emph{dual vector space} $ V^{*} $ of $ V $
	
	\[ V^{*} = L(V,F) = \{ \alpha : V \to F \text{ linear } \}  \]
\end{defi}

$ V^{*} $ is a vector space over $ \F $. Its elements are sometimes called linear functionals. 

\begin{eg}
	$ V = \R^{3} $,
	
	\[ \theta: V \to \R \]
	
	\[ \begin{pmatrix}
	a \\
	b\\
	c
	\end{pmatrix} \to a - c \qquad \theta \in V^{*} \]
\end{eg}

\begin{eg}
	\[ t_{n}:  M_{m,n}(\F) \to \F \]
	
	\[ A \mapsto \sum_{i} A_{ii}, \qquad t_{n} \in (M_{m,n}(\F))^{*} \]
\end{eg}

\begin{lemma} 
	Let $ V $ be a vector space over $ \F $ with finite basis $ \mathcal{B} = \{ e_{1},\cdots,e_{n} \} $. Then there is a basis for $ V^{*} $, given by $ \mathcal{B}^{*} = \{  \varepsilon_{1},\cdots,\varepsilon_{n} \}  $ where 
	
	\[ \varepsilon_{j} \underbrace{\left(   \sum_{i=1}^{m}  a_{i} e_{i}  \right) }_{\in V} = a_{j}  \qquad 1 \leq j \leq n \]
	
	$ \mathcal{B}^{*} $ is called the dual basis to $ \mathcal{B} $
\end{lemma}


\begin{proof}
\begin{itemize}
	\item LI
	
	\begin{align*}
	\sum_{j=1}^{n} \lambda_{j} \varepsilon_{j} = \mathbf{0} & \Rightarrow \left(  \sum_{j=1}^{n}  \lambda_{j} \varepsilon_{j} \right) e_{i} = \mathbf{0}   \\
	& = \sum_{j} \lambda_{j} \underbrace{\varepsilon_{j} (e_{i})}_{\delta_{ij}} 
	\end{align*}
	
	
	\[ \Rightarrow  \lambda_{i} = 0 \quad \forall \; i = 1,\cdots,n \]
	
	\item Span: If $ \alpha \in V^{*} $, then $ \alpha = \sum_{i=1}^{n} \alpha(e_{i}) \varepsilon_{i} $
	
	(``linear maps are determined by their action on a basis'')
	
\end{itemize}
\end{proof}

\begin{cor} 
	If $ V $ is finite dim, then $ \dim V  = \dim V^{*} $
\end{cor}
 
 
 Remark: Someties useful to think about $ (\F^{n})^{*} $ as the space of row vectors of length $ n $ over $ \F $. Suppose
 
 \[ V \text{ basis } e_{1},\cdots,e_{n}\]
 \[ V^{*} \text{ dual basis } \varepsilon_{1},\cdots,\varepsilon_{n} \]
 
\[   x = \sum x_{i} e_{i} \in V \] 
\[ \alpha = \sum a_{i} \varepsilon_{i} \in V^{*}  \]
 
 \[ \alpha(x) = \sum_{i=1}^{n} \alpha_{i} x_{i} =   \begin{pmatrix}
 a_{1} & \cdots & a_{n}
 \end{pmatrix}  \begin{pmatrix}
 x_{1} \\
 \vdots \\
 x_{n}
 \end{pmatrix}  \]
 
 
 \begin{defi}
 	If $ U \subseteq V $, 
 	
 	\[ U^{0} = \{  \alpha \in V^{*} \text{ st. } \alpha(u) = 0 \text{ for all }  u  \in U \} \] 
 	
 	is the \emph{annihilator} of $ U $
 	
 \end{defi}

 
 \begin{lemma} 
 	\begin{enumerate}
 		\item $ U^{0} \leq V^{*} $
 		\item If $ U \leq V $ and $ \dim V = n < \infty $, then
 		
 		\[ \dim V = \dim U + \dim U^{0} \] 
 		
 	\end{enumerate}
 \end{lemma}

\begin{proof}
	\begin{enumerate}
		\item $ 0 \in U^{0} $. If $ \alpha,\alpha' \in U^{0} $, then $ (\alpha + \alpha') = \alpha(u)  + \alpha'(u) = 0 + 0 = 0$, for $ u \in U $ thus $ \alpha + \alpha' \in U^{0} $ 
		
		Similarly, $ \lambda \alpha \in U^{0} $ for any $ \lambda \in \F $
		
		\item Let $ e_{1},\cdots,e_{k} $ be a basis for $ U $. Extend to a basis for $ V $. $ e_{1},\cdots,e_{k},e_{k+1},\cdots,e_{n} $.
		
		Let $ \mathcal{B}^{*} $ be the dual basis to this. $ \varepsilon_{1},\cdots,\varepsilon_{n} $
		
			Claim: $ \varepsilon_{k+1},\varepsilon_{k+2},\cdots,\varepsilon_{n} $ is a basis for $ U^{0} $
			
			\begin{itemize}
				\item If $ i > k $, $ \varepsilon_{i}(e_{j}) = 0 $ where $ j \leq k $, so $ \varepsilon_{i} $ (for $ i > k $) is in $ U^{0} $.
				\item LI comes from the fact that $ \mathcal{B}^{*} $ is a basis. (So any subset of it is LI).
				\item Span? If $ \alpha \in U^{0} $, then $ \sum_{i=1}^{n} \alpha_{i} \varepsilon_{i} $, some $ a_{i} \in \F $. 
				
				
				\[ \left(  \sum_{i=1}^{n} a_{i} \varepsilon_{i} \right) (e_{j}) = 0 \Rightarrow a_{j} = 0 , \text{ any } j \leq k\]
				
				where $ e_{j} $ is a basis element for $ U $, for $ j \leq k $
				
				\[ \Rightarrow \alpha \in < \varepsilon_{k+1},\cdots,\varepsilon_{n} \]
				
			\end{itemize}
		
	\end{enumerate}


\end{proof}


\subsection{Dual Maps}

\begin{lemma} 
	Let $ V,W $ be vector spaces over $ \F $. Let $ \alpha \in L(V,W) $. Then the map 
	
	\[ \alpha^{*} : W^{*} \to V^{*} \]
	\[ \varepsilon \mapsto \varepsilon \circ \alpha \text{ is linear } \]
	
	\begin{tikzcd}
		V \ar[r, "\alpha"] & W \ar[r, "\varepsilon"] & F
	\end{tikzcd}
	
	We'll call $ \alpha^{*} $ the \emph{dual} of $ \alpha $.
	
	
\end{lemma}


\begin{proof}
	\begin{itemize}
		\item $ \varepsilon \circ \alpha $ is linear, so in $ V^{*} $.
		\item $ \alpha^{*} $ linear? Fix $ \theta_{1},\theta_{2} \in W^{*} $
		
		\begin{align*}
		\alpha^{*}(\theta_{1} + \theta_{2}) & = (\theta_{1} + \theta_{2})\alpha \\
		& = \theta_{1} \circ \alpha + \theta_{2} \circ \alpha \\
		& = \alpha^{*} \theta_{1} + \alpha^{*} \theta_{2}
		\end{align*} 
	\end{itemize}
Similarly, $ \alpha^{*}(\lambda \theta)  = \lambda \alpha^{*} \theta $
\end{proof}


\begin{prop} 
	Let $ V,W $ be v-spaces over $ \F $, with basis $ \mathcal{B},\mathcal{C} $ respectively. Let $ \mathcal{B}^{*},\mathcal{C}^{*} $ be the dual basis. Consider $ \alpha \in L(V,W) $ with dual $ \alpha^{*} $ .
	
	
	\[ [\alpha^{*}]_{\mathcal{C}^{*},\mathcal{B}^{*}} = [\alpha]_{\mathcal{B},\mathcal{C}}^{T} \]
\end{prop}

\begin{proof}
	 Say $ \mathcal{B} = \{ b_{1},\cdots,b_{n} \} $, $ \mathcal{C} \{ c_{1},\cdots,c_{n} \} $
	 
	 $ \mathcal{B}^{*} = \{  \beta_{1},\cdots,\beta_{n} \} $, $ \mathcal{C}^{*} = \{  \gamma_{1},\cdots,\gamma_{n} \} $
	 
	 and $ [\alpha]_{\mathcal{B},\mathcal{C}} = (a_{ij}) \quad m \times n $


\begin{align*}
\alpha^{*}(\gamma_{r}) (b_{s}) & = \gamma_{i} \circ \alpha(b_{s}) \\
& = \gamma_{r} (\alpha(b_{s})) \\
& = \gamma_{r} \left( \sum_{t} a_{ts}c_{t} \right) \\
& = \sum_{t} \alpha_{ts}\gamma_{r}(c_{t}) \\
& = a_{rs} \\
& = \left(  \sum_{i} \alpha_{ri} \beta_{i} \right)(b_{s})
\end{align*}

\[ \Rightarrow \alpha^{*}(\gamma_{r}) = \sum_{i} a_{r_{i}} \beta_{i} \]

\[ \Rightarrow [\alpha^{*}]_{\mathcal{C}^{*},\mathcal{B}^{*}} = [\alpha]_{\mathcal{B},\mathcal{C}}^{T} \]

\end{proof}


Let $ V $ be a finite dim $ \F $ vector space.

Bases $ \varepsilon = \{   e_{1},\cdots,e_{n} \} $, $ F = \{ f_{1},\cdots,f_{n} \} $

Dual bases $ \varepsilon^{*} = \{  \varepsilon_{1},\cdots,\varepsilon_{n} \} $, $ \mathcal{F} = \{ \eta_{1},\cdots,\eta_{n} \} $

And let us condisder $ P = [\id]_{\mathcal{F}\mathcal{E}} $

\begin{lemma} 
	Change of basis matrix from $ \mathcal{F}^{*} $ to $ \mathcal{E}^{*} $ is $ (P^{-1})^{T} $
\end{lemma}

\begin{proof}
	\[ [\id]_{\mathcal{F}^{*},\mathcal{E}^{*}} = [\id]_{\mathcal{E}\mathcal{F}}^{T} = ([\id]_{\mathcal{F}\mathcal{E}}^{-1})^{T}  \]
\end{proof}

CAUTION: $ V \simeq V^{*} $ only if $ V $ is finite dimensional.

Let $ V = \mathcal{P} $, the space of all real polynomials, with basis

\[ p_{j},j = 0,1,2,\cdots \qquad p_{j}(t) = t^{j} \]

Ex sheet 2 Q 9:

\[ P^{*} \simeq \R^{\N} \]
\[ \varepsilon \mapsto (\varepsilon(p_{0}), \varepsilon(p_{1}),\cdots  \]

Ex sheet 1, Q3 g) $ P \not \simeq \R^{\N} $ does NOT have a countable basis

\begin{lemma} 
	Let $ V,W $ be vector spaces over $ \F $. Fix $ \alpha \in L(V,W) $, let $ \alpha^{*} \in L(W^{*},V^{*}) $ be the dual map. Then
	\begin{enumerate}
		\item $ N(\alpha^{*})  = (\Im(\alpha))^{0} $ ie. $ \alpha^{*} $ injective iff $ \alpha $ is surjective
		\item $ \Im(\alpha^{*}) \leq (N(\alpha))^{0} $, with equality if $ V $
and $ W $ are finite dimensional. ie. $ \alpha^{*} $ surjective iff $ \alpha $ is injective	
	\end{enumerate}
\end{lemma}


\begin{proof}
	\begin{enumerate}
		\item Let $ \varepsilon \in W^{*} $. Then 
		
		
		
		\begin{align*}
		\varepsilon \in N(\alpha^{*}) & \iff \alpha^{*} \varepsilon = 0 \\
		& \iff \varepsilon \circ \alpha = 0  \\
		& \iff \varepsilon(\mu) = 0 \text{ for all } u \in \Im \alpha\\
		&  \iff \varepsilon \in \left(  \Im(\alpha) \right)^{0}
		\end{align*}
		
		\item Let $ \varepsilon \in \Im \alpha^{*} $. Then $ \varepsilon = \alpha^{*} \varphi $, for some $ \varphi \in W^{*} $.
		
		For any $ u \in N(\alpha) $,
		
		\begin{align*}
		\varepsilon(u) & = (\alpha^{*}  \varphi )(u) \\
		& = (\varphi \circ \alpha) (u)\\
		& = \varphi(\alpha(u)) \\
		& = \varphi(0) \\
		& = \mathbf{0}
		\end{align*}
		
		So $ \varepsilon \in N(\alpha^{0}) $
		
		
		Now use the fact that $ \dim V $, $ \dim W $ are finite. 
		
		
		\begin{align*}
		\dim (\Im(\alpha^{*})) & = r(\alpha^{*}) \\
		& = r(\alpha) \qquad \text{ as } r(A) = r(A^{T})  \\
		& =  \dim V - \dim N(\alpha) \qquad \text{ by R-N} \\
		& = \dim (N(\alpha))^{0}
		\end{align*}
		
	\end{enumerate}

\end{proof}


\subsection{Double Duals}

\begin{defi}
	Let $ V $ be an $ \F $ vector space, $ V^{*}  = L(V,\F) $ dual of $ V $. Then the \emph{double dual} of $ V $ is the dual of $ V^{*} $, given by
	
	\[ V^{**} = L(V^{*},\F) \] 
\end{defi}

\begin{thm} 
	If $ V $ is a finite dimensional vector space over $ \F $, then the map 
	
	\[ \hat{} \; : V \to V^{**} \]
	\[ v \mapsto \hat{v}, \quad \hat{v}(\varepsilon) = \varepsilon(v) \]
	
	is an isomorphism 
\end{thm}


\begin{proof}
	Firstly, for $ v \in V $, the map $ \hat{v} : V^{*} \to \F $ is linear, so $ \; \hat{} \;  $ does indeed give a map from $ V $ to $ V^{**} $
	
	\begin{itemize}
		\item $ \; \hat{} \; $ is linear. If $ v_{1},v_{2} \in V $, $ \lambda_{1},\lambda_{2} \in \F, \varepsilon \in V^{*} $.
		
		
		
		\begin{align*}
		\overbrace{(\lambda_{1} v_{1} + \lambda_{2} v_{2} )}^{\hat{}} (\varepsilon) & = \varepsilon(  \lambda_{1}v_{1} + \lambda_{2}v_{2} )\\
		& = \lambda_{1} \varepsilon(v_{1}) + \lambda_{2} \varepsilon(v_{2})\\
		& = \lambda_{1} \hat{v_{1}} (\varepsilon) + \lambda_{2} \hat{v_{2}} (\varepsilon)
		\end{align*}
		
		
		\item $ \; \hat{} \; $ is injective: Let $ e \in V \setminus \{ \mathbf{0} \} $. Extend it to a basis of $ V $, say $ e_{1},e_{2},\cdots,e_{n} $. Let $ \varepsilon_{1},\varepsilon_{2},\cdots,\varepsilon_{n} $ be the dual basis for $ V^{*} $.
		
		$ \hat{e}(\varepsilon) = \varepsilon(e) = 1 $. So $ \hat{e} \neq 0 $.
		
		Thus $ N(\; \hat{} \;) = \{ \mathbf{0} \} $, so $ \; \hat{} \; $ is injective.
		
		\item $ V $ is finite dim, so $ \dim V = \dim V^{*} = \dim V^{**} $. 
		
	\end{itemize}

Thus $ \; \hat{} \; $ is an isomorphism	
	
\end{proof}

\begin{lemma} 
	Let $ V $ be a finite dim vector space over $ \F $ and $ U \leq V $.
	
	Then $ \hat{U} = U^{00} $, so after identification of $ V $ with $ V^{**} $, we have that $ U^{00} = U$.  
\end{lemma}

\begin{proof}
	\begin{itemize}
		\item First show $ \hat{U} \leq U^{00} $.
		
		\begin{align*}
		u \in U & \Rightarrow \varepsilon(u) = 0 \qquad \forall \; \varepsilon \in U^{0} \\
		& = \hat{u}(\varepsilon) = 0 \\
		& \Rightarrow \hat{u} \in (U^{0})^{0} = U^{00}
		\end{align*}
		
		
		\begin{align*}
		\dim U^{00}& = \dim V^{*} - \dim U^{0} \\
		& = \dim V - \dim U^{0} \\
		& = \dim U
		\end{align*}
	\end{itemize}

Thus $ \hat{U} = U^{00} $

\end{proof}

\begin{lemma} 
	Let $ V $ be a finite dim vector space of $ \F $, Let $ U_{1},U_{2} \leq V $. Then
	
	
	\begin{enumerate}
		\item $ (U_{1} + U_{2})^{0} = U_{1}^{0} \cap U_{2}^{0} $
		\item $ (U_{1} \cap U_{2})^{0} = U_{1}^{0} + U_{2}^{0} $
		
	\end{enumerate}
\end{lemma}

\begin{proof}
	\begin{enumerate}
		\item Let $ \theta \in V^{*} $
		
		\begin{align*}
		\theta \in (U_{1} + U_{2})^{0} & \iff \theta(u_{1} + u_{2}) = 0 \text{ for all } u_{1} \in U_{1},u_{2} \in U_{2} \\
		& = \theta(u) = 0 \text{ for all } u \in U_{1} \cap U_{2} \\
		& \theta \in U_{1}^{0} \cap U_{2}^{0}
 		\end{align*}
		
		\item Apply annihilator to (i).
		
		\[ w_{i} = U_{i}^{0} \qquad u_{i} = W_{i}^{0} \]
		
		\[ (W_{1}^{0} + W_{2}^{0} )^{0} = W_{1} \cap W_{2} \]
		\[ W_{1}^{0} + W_{2}^{0} = (W_{1} \cap W_{2} )^{0} \]
		
	\end{enumerate}
\end{proof}



\section{Bilinear Forms I}

\begin{defi}
	Let $ U,V $ be vector spaces over $ \F $.
	
	\[ \varphi : U \times V \to \F \]
	
	is \emph{bilinear} or a \emph{bilinear form} if its linear in both arguments
	
	\[ \varphi(u,-) : V \to \F \quad \in V^{*} \; \forall u \in U \]
	\[ \varphi(-,v) : U \to \F \quad \in U^{*} \; \forall v \in V \]
\end{defi}

\begin{eg}
	\begin{enumerate}
		\item $ V \times V^{*} \to \F $ with $ (v,\theta) \mapsto \theta(v) $
		
		\item $ U = V = \R^{n} $ with $ \varphi(x,y) = \sum_{i=1}^{n} x_{i} y_{i} $ for $ x \in U, y \in V $
		\item $ A \in M_{m,n}(\F) $ with $ \varphi : \F^{m} \times \F^{n} \to \F $, $ (u,v) \mapsto u^{T} A v $
		
		\item (infinite dim) $ U = V = C( [0,1] ,\R ) $ with $ \varphi(f,g) = \int_{0}^{1} f(t) g(t) \; \d t $ for $ f \in U, g \in V $
		
	\end{enumerate}
\end{eg}


\begin{defi}
	$ \mathcal{B} = \{ e_{1},\cdots,e_{m} \} $ basis for $ U $,
	
	$ \mathcal{C} = \{ f_{1},\cdots,f_{n} \} $ basis for $ V $
	
	
	$ \varphi : U \times V \to \F $ bilinear,
	
	The matrix of $ \varphi $ wrt $ \mathcal{B} $ and $ \mathcal{C} $ 
	
	\[ [\varphi]_{\mathcal{B},\mathcal{C}} = \left(  \varphi(e_{i},f_{j}) \right)  \] $ m \times n $, $ i,j $th entry
\end{defi}


\begin{lemma} 
	\[ \varphi(u,v) = [u]_{\mathcal{B}}^{T} [\varphi]_{\mathcal{B},\mathcal{C}} [ v]_{\mathcal{C}} \]
\end{lemma}


\begin{proof}
	Say $ u = \sum \lambda_{i} e_{i} $, $ v = \sum \mu_{j} f_{j} $
	
	
	\begin{align*}
	\varphi(u,v) & = \varphi \left( \sum \lambda_{i} e_{i}, \sum \mu_{j} f_{j} \right)  \\
	& = \sum_{i} \lambda_{i} \varphi(e_{i},\sum_{j} \mu_{j}f_{j}) \\
	& = \sum_{i,j} \lambda_{i} \varphi(e_{i},f_{j}) \mu_{j} 
	\end{align*}
\end{proof}

Note: $ [\varphi]_{\mathcal{B},\mathcal{C}} $ is the unique representation with this property

Note: $ \varphi: U \times V \to \F $ bilinear, determines linear maps 

\[ \varphi_{L} : U \to V^{*} \qquad \text{ and } \qquad \varphi_{R} : V \to U^{*} \]

\[ \varphi_{L}(u)(v) = \varphi(u,v) \qquad \text{ and } \qquad \varphi_{R}(v)(u) = \varphi(u,v) \]


\begin{lemma} 
	$ \mathcal{B} = \{ e_{1},\cdots,e_{m} \} $ basis for $ U $,
	
	dual $ \mathcal{B}^{*} = \{ \varepsilon_{1},\cdots,\varepsilon_{m} \} $ basis for $ U^{*} $.
	
	Similarly, $ \mathcal{C} = \{ f_{1},\cdots,f_{n} \} $ for $ V $,
	$ \mathcal{C}^{*} = \{  \eta_{1},\cdots,\eta_{n} \} $ for $ V^{*} $
	                                                                                                  
	                                                                                                  
	If $ [ \varphi ]_{\mathcal{B},\mathcal{C}} = A $, then $ [ \varphi_{R} ]_{\mathcal{C},\mathcal{B}^{*}}  = A$,  $ [ \varphi_{L}]_{\mathcal{B},\mathcal{C}^{*}} = A^{T} $
	
	 
\end{lemma}


\begin{proof}
\[ 	 \varphi_{L}(e_{i})(f_{j}) = A_{ij} \Rightarrow \varphi_{L}(e_{i}) = \sum_{j} A_{ij} \eta_{j} \]
	
	
	\[ \varphi_{R}(f_{j})(e_{i}) = A_{ij} \Rightarrow \varphi_{R}(f_{j}) = \sum_{i} A_{ij} \varepsilon_{i}  \]
\end{proof}


\begin{defi}
	Left kernel of $ \varphi = \ker \varphi_{L} $, Right kernel of $ \varphi = \ker \varphi_{R} $
\end{defi}

\begin{defi}
	$ \varphi $ is \emph{non-degenerate} if $ \ker \varphi_{L} = 0 $ and $ \ker \varphi_{R} = 0 $. Otherwise $ \varphi $ is \emph{degenerate}
\end{defi}

\begin{lemma} 
	Let $ U, \mathcal{B} $, $ V,\mathcal{C} $ as before,
	
	\[ \varphi: U \times V \to \F \]
	
	\[ A = [ \varphi ]_{\mathcal{B},\mathcal{C}} \]
	 
	 assume $ \dim U, \dim V $ finite. Then
	 
	 \[ \varphi \text{ non-degenerate} \iff A \text{ invertible } \]
\end{lemma}

\begin{proof}
	
	
	\begin{align*}
	 \varphi \text{ non-degenerate } & \iff \ker \varphi_{L} = \mathbf{0} \text{ and } \ker \varphi_{R} = \{ \mathbf{0} \} \\
	& \iff n(A^{T}) = 0 \text{ and } n(A) = 0 \\
	& \iff r(A^{T}) = \dim V \text{ and } r(A) = \dim U \\
	& \iff A \text{ invertible } \quad \text{ (and neccessarily ) } \dim U = \dim V \end{align*}
	 
\end{proof}

\begin{cor} 
	If $ \varphi $ is non-degenerate and $ U $ and $ V $ are finite, then 
	
	\[ \dim U = \dim V \]
\end{cor}


\begin{cor} 
	When $ U $ and $ V $ are finite dim, choosing a non-degenerate bilinear form $ \varphi: U \times V \to \F $ is equivalent to picking an isomorphism $ \varphi_{L} : U \to V^{*} $
\end{cor}


\begin{defi}
	For $ T \subset U $, $ T^{\perp} = \{ v \in V \; | \; \varphi(t,v) = 0 \; \forall t \in T \} \leq V $
	
	For $ S \subset T $, $ ^{\perp}S = \{ u \in U \; | \; \varphi(u,s) = 0 \; \forall s \in S \} \leq U $ 
	
	(Generalisation of annihilators)
\end{defi}

\begin{prop} 
	$ U $ bases $ \mathcal{B},\mathcal{B'} $, $ P = [\id]_{\mathcal{B}',\mathcal{B}} $
	
	$ V $ bases $ \mathcal{C},\mathcal{C}' $ with $ Q =[ \id]_{\mathcal{C'},\mathcal{C}} $
	
	Let $ \varphi: U \times V  \to \F $ bilinear. Then
	
	
	\[ [\varphi]_{\mathcal{B}',\mathcal{C}'} = P^{T} [ \varphi]_{\mathcal{B},\mathcal{C}}Q \]
\end{prop}

\begin{proof}
	
	\begin{align*}
	\varphi_{u,v} & = [u]_{\mathcal{B}}^{T} [\varphi]_{\mathcal{B},\mathcal{C}} [v]_{\mathcal{C}} \\
	& = (P[u]_{\mathcal{B}'})^{T} [\varphi]_{\mathcal{B},\mathcal{C}} (Q[v]_{\mathcal{C}'}) \\
	& = [u]_{\mathcal{B}'}^{T} [\varphi]_{\mathcal{B},\mathcal{C}} [v]_{\mathcal{C}'}
	\end{align*}
	
\end{proof}


\begin{defi}
	The rank of $ \varphi $, $ r(\varphi) $ is the rank of any matrix representing it (well-def) by prev thm.
\end{defi}

Note: $ r(\varphi) = r(\varphi_{L}) = r(\varphi_{R}) $


\section{Determinant and Trace}

\subsection{Trace}

\begin{defi}
	For $ A \in M_{n}(\F) $ (this is $ M_{n,n}(\F) $), then
	
	\[ \text{tr}(A) = \sum_{i}A_{ii}  \]
	
	is the \emph{trace} of $ A $. This is a linear map.
\end{defi}

\begin{lemma} 
	For $ A,B \in M_{n}(\F) $, 
	\[ \text{tr}(AB) = \text{tr}(BA) \]
\end{lemma}

\begin{proof}
	\begin{align*}
	\text{tr}(AB) & = \sum_{i} \sum_{j} a_{ij} b_{ji}  \\
	& = \sum_{j} \sum_{i} b_{ji} a_{ij} \\
	& = \text{tr}(BA)
	\end{align*}
\end{proof}


\begin{lemma} 
	Similar (= conjugate) matrices have the same trace.
\end{lemma}

\begin{proof}
	$ B = P^{-1} A P $, $ A,B \in M_{n}(\F) $.
	
	
	\begin{align*}
	\text{tr}(B) & = \text{tr}(P^{-1}AP) \\
	& = \text{tr}(AP P^{-1}) \\
	& = \text{tr}A
	\end{align*}
\end{proof}

\begin{defi}
	If $  \alpha : V \to V $ linear, define $ \text{tr } \alpha = \text{tr}[\alpha]_{\mathcal{B},\mathcal{B}} $. By the above, this is well defined. 
\end{defi}

\begin{lemma}  
	Let $ \alpha : V \to V $ linear, and $ \alpha^{*} : V^{*} \to V^{*} $ its dual. Then
	
	\[ \text{tr }\alpha = \text{tr }\alpha^{*} \]
\end{lemma}

\begin{proof}
	\begin{align*}
	\text{tr }\alpha & = \text{tr}[\alpha]_{\mathcal{B},\mathcal{B}}  \\
	& = \text{tr}[\alpha]_{\mathcal{B},\mathcal{B}}^{T} \\
	& = \text{tr}[\alpha^{*}]_{\mathcal{B}^{*}} \\
	& = \text{tr } \alpha^{*	} 
	\end{align*}
\end{proof}

\subsection{Determinants}


\[ S_{n} = \text{group of permutations of } \{ 1,\cdots,n \} \]

Define $ \varepsilon_{n} : S_{n} \to \{ -1,1 \} $ as

\[ \varepsilon(\sigma) = \begin{cases} +1  & \text{ if } \sigma \text{ product of even no. of transposes} \\-1  & \text{ if } \sigma \text{ product of odd no. of transposes}\end{cases} \] 

\begin{defi}
	Let $ A \in M_{n}(\F), A = (a_{ij})$. Then
	
	\[ det(A) = \sum_{\sigma \in S} \varepsilon(\sigma) a_{\sigma(1)1}a_{\sigma(2)2}\cdots a_{\sigma(n)n} \]
\end{defi}

There are $ n! $ summands, each is sign $ \times $ product of $ n $ elements (one for each row and each column).

Eg $ n = 2 $,

\[ \det \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix} = \underbrace{a_{11}a_{22}}_{\sigma = \id } - \underbrace{a_{12}a_{21}}_{\sigma = (1 2)}  \]


\begin{lemma} 
	If $ A = (a_{ij}) $ is an upper triangular matrix (ie. $ a_{ij} = 0  $ if $ i > j $) then $ \det A = a_{11}a_{22}\cdots a_{nn} $. Similar for lower triangular matrices (ie. $ a_{ij} = 0 $ if $ i < j $).
\end{lemma}


\begin{proof}
	\[ \det(A) = \sum_{\sigma \in S_{n}}  \varepsilon(\sigma)a_{\sigma(1)1}\cdots a_{\sigma(n)n}  \]
	
	For a summand to be non-zero, need $ \sigma(j) \leq j \; \forall \; j $. Thus $ \sigma = \id $
\end{proof}

\begin{lemma} 
	\[ \det(A) = \det(A^{T}) \]
\end{lemma}
\begin{proof}
	\begin{align*}
	\det(A)  & = \sum_{\sigma \in S_{n}}  \varepsilon(\sigma) \times \prod_{i=1}^{n} a_{\sigma(i)i}   \\
	& = \sum_{\sigma \in S_{n}} \underbrace{\varepsilon(\sigma)}_{= \varepsilon^{-1}} \prod_{i=1}^{n} a_{i \sigma^{-1}(i)} \\
	& = \sum_{\tau \in S_{n}} \varepsilon(\tau) \prod_{i=1}^{n}a_{i \tau(i)} \qquad (\sigma^{-1} = \tau)\\
	& = \det (A^{T})
	\end{align*}
\end{proof}

\begin{defi}
	A \emph{volume form} on $ \F^{n} $ is a function:
	
	\[ d : \underbrace{\F^{n} \times \F^{n} \times \cdots \times \F^{n}}_{n \text{ copies}} \to \F  \] such that
	
	\begin{enumerate}
		\item $ d $ is \emph{multilinear}: for any $ i $ and $ v_{1},\cdots,v_{i-1},v_{i},v_{i+1},\cdots,v_{n} \in \F^{n} $, \[ d(v_{1},v_{2},\cdots,v_{i-1},-,v_{i+1},\cdots,v_{n}) \in (\F^{n})^{*} \]
		\item $ d $ is \emph{alternating}: if $ v_{i} = v_{j} $ for $ i \neq j $, then $ d(v_{1},\cdots,v_{n}) = 0$
		
	\end{enumerate}
\end{defi}

Note that the notation we will use will look like

\[ A = (a_{ij}) = (A^{(1)} \; | \; A^{(2)} \; | \; \cdots \; | A^{(n)}   ) \]

If $ \{ e_{i} \} $ is the standard basis for $ \F^{n} $ then 

\[ I = (  e_{1} \; | \; \cdots \; | \; e_{n} ) \]


\begin{lemma} 
	\[ \det : \underbrace{\F^{n} \times \cdots \times \F^{n}}_{n \text{ copies}} \to \F  \]
	
	\[ (A^{(1)},\cdots,A^{(n)} ) \mapsto \det (A) \text{ is a volume form} \]
	
\end{lemma}


\begin{proof}
	\begin{enumerate}
		\item Multilinear: For any fixed $ \sigma \in S_{n} $, $ \prod_{i=1}^{n} a_{\sigma(i)} $ contains exactly one term from each column, and so is multilinear. Now use the fact that the sum of multilinear functions is multilinear. 
		
		\item Alternating: Suppose $ A^{(k)} = A^{(J)} $, for $ J \neq k $. Let $ \tau = (k J) $ transpo $ a_{ij} = a_{i \tau(J)} \; \forall \; i,j \in \{ 1,\cdots,n \}$, $ S_{n} = A_{n} \sqcup \tau A_{n} $, where $ \sqcup $ is disjoint union. 
		
		\begin{align*}
		\det(A) & = \sum_{\sigma \in A_{n}} \prod a_{i\sigma(i)} - \sum_{\sigma \in A_{n}} \prod a_{i  \tau(\sigma(i))  } \\
		& = \sum_{\sigma \in A_{n}} \prod a_{i\sigma(i)} - \sum_{\sigma \in A_{n}} \prod a_{i\sigma(i)}\\
		& = 0
		\end{align*}
		
	\end{enumerate}
\end{proof}


\begin{lemma} 
	Let $ d $ be a volume form. Then swapping two entries changes the sign.
	
	\[ d(v_{1},\cdots,v_{i},\cdots,v_{j},\cdots,v_{n}) = -d(v_{1},\cdots,v_{j},\cdots, v_{i},\cdots,v_{n} ) \]
\end{lemma}

\begin{proof}
	\begin{align*}
	0 & = d(v_{1},\cdots,v_{i-1},v_{i} + v_{j},v_{i+1}\cdots,v_{j-1},v_{j} + v_{i},v_{j+1},\cdots,v_{n}) \\
	& = d(v_{1},\cdots,v_{i},\cdots,v_{i},\cdots,v_{n}) + d(v_{1},\cdots,v_{j},\cdots,v_{i},\cdots,v_{n}) \\
	& + d(v_{1},\cdots,v_{i},\cdots,v_{j},\cdots,v_{n})  + d(v_{1},\cdots,v_{j},\cdots,v_{j},\cdots,v_{n})  \\
	& = d(v_{1},\cdots,v_{j},\cdots,v_{i},\cdots,v_{n}) + d(v_{1},\cdots,v_{i},\cdots,v_{j},\cdots,v_{n}) 
	\end{align*}
\end{proof}

\begin{cor} 
	If $ \sigma \in S_{n} $, $ d(v_{\sigma(1)}, \cdots, v_{\sigma(n)} ) = \varepsilon(\sigma)d(v_{1},\cdots,v_{n})  $
	
\end{cor}


\begin{thm} 
	Let $ d $ be a volume form on $ \F^{n} $. $ A = (A^{(1)}  \; | \; \cdots \; | \; A^{(n)}  ) $. Then 
	
	\[ d(A^{(1)}, \cdots, A^{(n)} ) = \det(A) \times d(e_{1},\cdots,e_{n}) \]
\end{thm}	
	\begin{proof}
		\begin{align*}
		d(A_{1},\cdots,A^{n}) & = d \left(  \sum_{i=1}^{n} a_{ij} e_{i}, A^{(2)}, \cdots, A^{(n)} \right)  \\
		& = \sum_{i=1}^{n} a_{i1} d(e_{i}, A^{(2)},\cdots,A^{(n)} ) \\
		& = \sum_{i} \sum_{j} a_{i1}a_{j2} d(e_{i},e_{j},A^{(3)},\cdots,A^{(n)}) \\
		& = \sum_{i_{1},\cdots,i_{n}} \prod_{k=1}^{n} a_{ik} \underbrace{d(e_{i_{1}},e_{i_{2}}, \cdots, e_{i_{n}})}_{0 \text{ unless all of } i_{k} \text{ are distinct}\footnote{\text{ie. $ \sigma \in S_{n} $ s.t. $ i_{k} = \sigma(k)$}  }} \\
		& = \sum_{i_{1},\cdots,i_{n}} \prod_{k=1}^{n} A_{\sigma(k)k} \underbrace{d(e_{\sigma(1)}, \cdots, e_{\sigma(n)})}_{ = \varepsilon(\sigma) d(e_{1},\cdots,e_{n}) }
		\end{align*}
	\end{proof}


\begin{cor} det is the unique volume form s.t.
	
	\[ d(e_{1},\cdots,e_{n})  = 1\]
	
\end{cor}


Recall: 	\[ \det : \underbrace{\F^{n} \times \cdots \times \F^{n}}_{n \text{ copies}} \to \F  \]

\[ (v_{1},\cdots,v_{n}) \mapsto \det (v_{1} \; | \; \cdots \; | v_{n}) \text{ is a volume form} \]


\begin{prop} 
	Let $ A,B \in M_{n}(\F) $. Then $ \det(AB) = \det(A)\det(B) $
\end{prop}

\begin{proof}
	Let $ d_{A}: \underbrace{\F^{n} \times \cdots \times \F^{n}}_{n \text{ copies}} \to \F  $,
	\[ (v_{1},\cdots,v_{n}) \mapsto \det (Av_{1} \; | \; \cdots \; | Av_{n}) \]
	
	\begin{itemize}
		\item $ d_{A} $ is multilinear: $ v_{i} \mapsto Av_{i} $ linear, and $ d $ multilinear
		\item $ d_{A} $ is alternating: $ v_{i} = v_{j} \Rightarrow A v_{i} = A v_{j} $ and $ d $ is alternating 
	\end{itemize}
	Thus $ d_{A} $ is a volume form. 
	
	
	\begin{align*}
	d_{A} (Be_{1},\cdots,Be_{n}) & = \det B d_{A} (e_{1},\cdots,e_{n}) \quad (d_{A} \text{ a v.f.}) \\
	& = \det B \det A
	\end{align*}
	
	Also $ d_{A}(Be_{1},\cdots,Be_{n}) = \det(AB e_{1} | \; \cdots \; | A B e_{n}) = \det (AB) $. 
	
	
	
\end{proof}
	
	
\begin{defi}
	$ A \in M_{n}(\F) $ is singular if $ \det A = 0 $. Otherwise $ A $ is non-singular. 
\end{defi}

\begin{lemma} 
	if $ A $ is invertible, then $ A $ is non-singular, $ \det(A^{-1}) = \frac{1}{\det A} $
\end{lemma}

\begin{proof}
	\begin{align*}
	1 & = \det(I_{n}) \\
	& = \det(A A^{-1}) \\
	& = \det(A)\det(A^{-1}) \\
	& \Rightarrow \det(A) \neq 0, \det(A^{-1}) = (\det(A))^{-1}
	\end{align*}
\end{proof}

\begin{thm} 
	Let $ A \in M_{m,n}(\F) $. TFAE:
	\begin{enumerate}
		\item $ A $ is invertible
		\item $ A $ is non-singular
		\item $ r(A) = n $
	\end{enumerate}
\end{thm}

\begin{proof}
	\begin{itemize}
		\item (i) $ \Rightarrow $ (ii) done
		\item (ii) $ \Rightarrow $ (iii): Suppose that $ r(A) < n $. By rank-nullity, $ n(A) > 0 $, so $ \exists \; \lambda \in \F^{n} \setminus \{ \mathbf{0} \} $ st. $ A \lambda = \mathbf{0} $. Say $ \lambda = (\lambda_{i}) $, say $ \lambda_{k} \neq 0 $. Have $ \sum_{i=1}^{n} A^{(i)}\lambda_{i} = \mathbf{0} $.
		
		Let $ B = ( e_{1} \; | \; \cdots \; | \; e_{k-1} \; | \; \lambda \; | \; e_{k+1} | \; \cdots \; | e_{n}  ) $.
		
		\begin{align*}
		AB \text{ has } k^{\text{th}} \text{ column zero } \Rightarrow \det(AB) = 0 \\
		& = \det(A)\det(B) \\
		& = \det(A)\underbrace{\lambda_{k}}_{\neq 0}
		\end{align*}
		
		Thus $ \det A = 0 $
		
		\item (iii) $ \Rightarrow $ (i) by rank-nullity
	\end{itemize}
\end{proof}


\subsubsection{Determinants of Linear Maps}
\begin{lemma} 
	Conjugate matrices have the same determinant.
\end{lemma}

\begin{proof}
	Let $ B = P^{-1}AP $. Then
	
	\begin{align*}
	\det B & = \det(P^{-1}AP) \\
	& = \det(P^{-1})\det(A)\det(P) \\
	& = (\det P)^{-1}(\det A)(\det P) \\
	& = \det A
	\end{align*}
\end{proof}


\begin{defi}
	Let $ \alpha : V \to V $, $ V $ a finite-dim $ v $-space. Define $ \det \alpha = \det [\alpha]_{\mathcal{B},\mathcal{B}} $, where $ \mathcal{B} $ is any basis for $ V $. This is well-defined by the previous lemma.  
\end{defi}

\begin{thm} 
	$ \det: L(V,V) \to \F $ satisfies:
	\begin{enumerate}
		\item $ \det(I_{d}) = 1 $
		\item $ \det(\alpha \circ \beta) = \det(\alpha) \det(\beta) $
		\item $ \det(\alpha) \neq 0 \iff \alpha $ invertible, and if $ \alpha $ invertible then $ \det(\alpha^{-1}) = (\det \alpha)^{-1} $
	\end{enumerate}
\end{thm}

\subsubsection{Determinants of Block Triangular Matrices}

\begin{lemma} 
	$ A \in M_{k}(\F) $, $ B \in M_{l}(\F) $, $ C \in M_{k,l}(\F) $.
	
	\[ \det \begin{pmatrix}
	A & C\\
	0 & B
	\end{pmatrix}  = \det A \det B\]
\end{lemma}

\begin{proof}
	set $ n = k + l $. Let $ X = \begin{pmatrix}
	A & C\\
	0 & B
	\end{pmatrix} \in M_{n}(\F) $, $ X = (x_{ij}) $. 
	
	\[ \det(X) = \sum_{\sigma \in S_{n}} \varepsilon(\sigma) \prod_{i=1}^{n} x_{\sigma(i)i}  \]
	
	Note: $ x_{\sigma(i)i} = 0 $ if $ i \leq k $ and $ \sigma(i) > k $. Thus we're summing over all $ \sigma $ with 
	\begin{enumerate}
		\item if $ j \in [1,k], \sigma(j) \in [1,k] $ AND
		\item if $ j \in [k+1,n] $, $ \sigma(j) \in [k+1,n] $ 
 	\end{enumerate}
	this means
	\begin{enumerate}
		\item get $ x_{\sigma(i)i} = \underbrace{x_{\sigma_{1}(i)i}}_{=a_{\sigma_{1}(i)i}} $ where $ \sigma_{1} = $ restriction of $ \sigma $ to $ [1,k] $
		\item get $ x_{\sigma(i)i} = \underbrace{x_{\sigma_{2}(i)i}}_{=b_{\sigma_{2}(i)i}} $ where $ \sigma_{2} =  $ restriction of $ \sigma $ to $ [ k+1,n ] $. 
		
		
		\[ \sigma = \sigma_{1}\sigma_{2} \Rightarrow \varepsilon(\sigma) = \varepsilon(\sigma_{1})\varepsilon(\sigma_{2}) \]
		
		We get 
		
		\begin{align*}
		\det X & = \left(  \sum_{\sigma_{1} \in S_{k}} \varepsilon(\sigma_{1}) \prod_{j=1}^{k} a_{\sigma_{1}(j)j}  \right) \left(  \sum_{\sigma_{2} \in S_{l}} \varepsilon(\sigma_{2}) \prod_{j=k+1}^{n} b_{\sigma_{2}(j)j}  \right)   \\
		& = \det A \det B
		\end{align*}
	\end{enumerate}
\end{proof}

\begin{cor} 
	For square matrices $ A_{1},\alpha,A_{k} $, the upper-triangular matrix  with $ A_{1},\alpha,A_{k} $ along the diagonal has determinant $ = \prod_{i=1}^{k} \det A_{i} $. 
\end{cor}
\begin{proof}
	Apply lemma immediately.
\end{proof}

Caution: In general,

\[ \det \begin{pmatrix}
A & B \\
C & D
\end{pmatrix} \neq \det A \det D - \det B \det C \]

Aside: Volume Interpretation of Determinants:

$ \R^{2} $ $ \det( \mathbf{u} | \mathbf{v} ) $ is the signed area of parallelogram made by extending $ \mathbf{u} $ and $ \mathbf{v} $. 

$ \R^{3} $ $ \det( \mathbf{u} | \mathbf{v} | \mathbf{w}) = $ singed volume of parallelepiped.

There are analogous interpretations in higher dimensions. 

\subsubsection{Elementary Operations and Det}

\begin{enumerate}
	\item $ E_{1} $ swaps 2 columns/rows. $ \det E_{1}  = -1 $
	\item $ E_{2} $ multiplies a column/row by $ \lambda \neq 0 $. $ \det E_{2} = \lambda  $
	\item $ E_{3} $ add $ \lambda $(column $ i $) to column $ j $ (/rows). $ \det E_{3} = 1 $
	
\end{enumerate} 	

One could prove properties of $ \det $ (eg $ \det (AB) = \det A \det B $) by using the factorisation of matrices into products of $ E_{i} $.

\subsubsection{Column Expansion and Adjugate Matrices}
\begin{lemma} 
	Let $ A \in M_{n}(\F) $, $ A = (a_{ij}) $. Define $ A_{\hat{ij}} $ by deleting row $ i $ and col $ j $ from $ A $. Then
	\begin{enumerate}
		\item for a fixed $ j $, 
		
		\[ \det A = \sum_{i=1}^{n} (-1)^{i+j}a_{ij}\det A_{\hat{ij}} \]
		
		`expansion in column $ j $'
		\item for a fixed $ i $,
		
		\[ \det A = \sum_{j=1}^{n} (-1)^{i+j}a_{ij}\det A_{\hat{ij}} \]
		
		`expansion in row $ i $'
	\end{enumerate}
\end{lemma} 

Remark: could use 1) to define determinants iteratively, starting with $ \det a = a $ for $ n = 1 $.

\begin{eg}
\[ 	\begin{vmatrix}
		a & b & c\\
		d & e & f\\
		g & h & i
	\end{vmatrix} = a\begin{vmatrix}
	e & f\\
	h & i
	\end{vmatrix} - b \begin{vmatrix}
	d & f\\
	g & i
	\end{vmatrix} + c \begin{vmatrix}
	d & e\\
	g & h
	\end{vmatrix}  \]
\end{eg}	

\begin{proof}
	We will only prove (i), and get (ii) by transposition
	\begin{align*}
	\det(A) & = \det( A^{(1)} \; | \; A^{(2)} \; | \; \cdots\; | \; \sum_{i=1}^{n} a_{ij}e_{i} | \; \cdots\; A^{(n)} ) \\
	& = \sum_{i=1}^{n} a_{ij} \det( A^{(1)} \; | \; \cdots\; e_{i} \; | \; A^{(ji)} \; | \; \cdots \; | \; A^{(n)}  ) \\
	& = \sum_{i=1}^{n} \underbrace{a_{ij} (-1)^{(i-1) +(j-1)}}_{\text{row and col swaps}} \det \begin{pmatrix}
	1 & 0 \\
	0 & A_{\hat{ij}} \\
	& = \sum_{i=1}^{n} a_{ij} (-1)^{i+j}\det(A_{\hat{ij}})
	\end{pmatrix}
	\end{align*}
	
	
\end{proof}

\begin{defi}
	Let $ A \in M_{n}(\F) $. The \emph{adjugate matrix} of $ A $, $ \text{adj }(A) $, is the $ n \times n $ matrix
	
	\[ (\text{adj }A)_{ij} = (-1)^{i+j}\det(A_{\hat{ij}}) \]
	
	
\end{defi}


\begin{thm} 
	\begin{enumerate}
		\item 
		
		\[ (\text{adj }A) A = (\det A)I = \begin{pmatrix}
		\det A & \cdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \cdots & \det A
		\end{pmatrix} \]
		
		\item If $ A $ is invertible, then $ A^{-1} = \frac{1}{\det A}\text{adj}(A) $.
		
	\end{enumerate}

\end{thm}

\begin{proof}
	\begin{enumerate}
		\item $ \det A = \sum_{i}(\text{adj }A)_{ji}a_{ij} =  j^{\text{th}},j^{\text{th}}$ entry of $ (\text{adj }A)A $
		
		For $ j \neq k $, 
		
		\begin{align*}
		0 & = \det( A^{(1)} \; | \; \cdots \; \underbrace{| \; A^{k} \; |}_{j^{\text{th}} \text{ col}} \; \cdots ;\ | \; A^{k} \; | \; \cdots \; | \; A^{(n)}  \\
		& = \sum_{i} (\text{adj }A)_{ji} a_{ik} \\
		& = j,k^{\text{th}} \text{ entry of } (\text{adj }A)A
		\end{align*}
		\item If $ A $ invertible, then $\det A \neq 0  $, so $ I = \frac{\text{adj}(A)}{\det A} A $ 
	\end{enumerate}
\end{proof}

\subsection{Systems of Linear Equations}

\begin{itemize}
	\item $ A \mathbf{x} = \mathbf{b} $ is $ m $ equations in $ n $ unknowns ($ A: m \times n $ and $ \mathbf{b} : m \times 1 $ known, $ \mathbf{x} = (x_{1},\cdots,x_{n}) = n \times 1$ unknown  )
	
	\item $ A \mathbf{x} = \mathbf{b} $ has solution iff $ r(A) = r(A | b) $ where $ A | b $ is the augmented matrix: $ A  $ with extra column $ b $ (ie. iff $ \mathbf{b} $ is a linear combo of columns in $ A $).
	
	\item The solution is unique iff $ r(A) = n $
	
	\item Special case: $ m = n $. If $ A $ is non-singular then there is a unique solution $ \mathbf{x} = A^{-1} \mathbf{b} $.
\end{itemize}

\subsubsection{The Cramer Rule}


If $ A \in M_{n} (\F) $ invertible, the system $ A \mathbf{x} = \mathbf{b} $ has unique solution $ \mathbf{x} =  (x_{i}) $, 

\[ x_{i} = \frac{\det(A_{\hat{ib}})}{\det A} \]

where $ A_{\hat{ib}} $ is obtained from $ A $ by deleting $ i^{\text{th}} $ column and replacing it with $ \mathbf{b} $.

\begin{proof}
	Assume that $ \mathbf{x} $ is a solution of $ A \mathbf{x} = \mathbf{b} $.
	
	\begin{align*}
	\det(A_{\hat{ib}})& = \det (  A^{(1)} \; | \; \cdots \; | \; A^{(i-1)} \; | \; b \; | \; A^{(i+1)} \; | \; \cdots \; | \; A^{(n)} ) \\
	& = \det (  A^{(1)} \; | \; \cdots \; | \; A\mathbf{x} \; | \; \cdots \; | \; A^{(n)} ) \\
	& = \sum_{j=1}^{n} x_{j} \det (  A^{(1)} \; | \; \cdots \; | \; A^{(i-1)} \; | \; A^{(j)} \; | \; A^{(i+1)} \; | \; \cdots \; | \; A^{(n)} ) \; \text{ as } A\mathbf{x} = \sum_{j} A^{(j)}x_{j} \\ 
	& = x_{i} \det A
	\end{align*}	
\end{proof}

\begin{cor} 
	If $ A \in M_{n}(\Z) $ ie. $ (n \times n) $ with integer entries, with $ \det A = \pm 1 $, then 
	
	\begin{itemize}
		\item $ A^{-1} \in M_{n}(\Z) $ also, 
		
		\[ A^{-1} = \frac{\text{adj }A}{\pm 1} \qquad \text{ with adj } A \text{ entries in } \Z  \]
		
		\item $ \mathbf{b} \in \Z^{n} $, can solve $ A \mathbf{x} = \mathbf{b} $ for integer solution. 
	\end{itemize}
\end{cor}

	
	
\section{Endomorphisms}

Let $ V $ be a vector space over $ \F $, $ \dim V = n < \infty $, and $ \alpha \in L(V) = L(V,V) $, with $ \mathcal{B} = \{  v_{1},\cdots,v_{n} \} $ basis. 

Problem: Choose $ \mathcal{B} $ st. $ [\alpha]_{\mathcal{B}} ( = [\alpha])_{\mathcal{B},\mathcal{B}} $ has "nice from". $ \mathcal{B}' $ other basis, $ P $ change of basis matrix $ [ \alpha]_{\mathcal{B}} = P^{-1} [\alpha]_{\mathcal{B}'}P $.

Problem: $ A \in M_{n}(\F) $, want $ A' $ conjugate to it which has a nice from.


\begin{defi}
	$ \alpha \in L(V) $ is \emph{diagonalisable} if there exists $ \mathcal{B} $ st. 
	
	\[ [\alpha]_{\mathcal{B}} \text{ is diagonal} \]
	
\end{defi}

A weaker possibility is

\begin{defi}
	$ \alpha \in L(V) $ is \emph{triangulable} if $ \exists \; \mathcal{B} $ st. $ [\alpha]_{\mathcal{B}} $ is upper triangular
\end{defi}

($ A \in M_{n}(\F) $  is diagonalisable if its conjugate to a diagonal matrix, similarly for triangular.  )

\begin{defi}
\begin{enumerate}
	\item $ \lambda  \in \F $ is an \emph{eigenvalue} of $ \alpha $ if there exists some $ v  \in V \setminus \{ \mathbf{0} \} $ st. $ \alpha(v) = \lambda v $
	
	\item $ v \in V $ is an eigenvector for $ \alpha $ if $ \alpha(v) = \lambda v $ for some eigenvalue $ \lambda $
	
	\item $ V_{\lambda} = \{  v \in V \; | \; \alpha(v) = \lambda v \} $ $ \lambda $-eigenspace of $ \alpha $, Note $ V_{\lambda} \leq V $
	
\end{enumerate}
\end{defi}

Shorthand: evector, evalue, espace.

Remark: \begin{enumerate}
	\item $ \lambda $ evalue, $ \iff $ $ \alpha - \lambda \iota $ singular$ \iff \det(\alpha - \lambda \iota ) = 0$. 
	
	\[ V_{\lambda} = \ker(\alpha - \lambda \iota) \]
	
	Note $ \iota $ is the identity map.
	
	\item If $ \alpha(v_{j}) = \lambda v_{j} $, then $ j^{\text{th}} $ col of $ [\alpha]_{\mathcal{B}} $ is 
	
	\[ \begin{pmatrix}
	0\\
	\vdots\\
	\lambda\\
	\vdots\\
	0
	\end{pmatrix} \]
	
	($ j^{\text{th}} $ entry)
	
	\item  $ [\alpha]_{\mathcal{B}} $ diagonal $ \iff $ $ \mathcal{B} $ consists of evectors.
	
	$ [\alpha]_{\mathcal{B}} $ upper triangular $ \iff $ $ \alpha(v_{j}) \in < v_{1},\cdots,v_{j} >  $ for all $ j $. In particular, $ v_{1} $ is an eigenvector.
	
\end{enumerate}

\subsection{Aside on Polynomials}

\[ F[t] \{ \text{polys w/  coefficients in } \F \} \]

\begin{itemize}
	\item $ \text{deg}(f+g) \leq \max(\text{deg } f, \text{deg } g) $
	\item $ \text{deg } 0 = - \infty $
	\item $ \text{deg}(fg) = \text{deg } f + \text{deg } g $
	\item If $ \lambda \in \F $ is a root of $ f \in F[t] $ (ie. $ f(\lambda) = 0 $), then $ (t - \lambda) $ divides $ f $: 
	
	\[ f(t) = (t - \lambda) g(t), \qquad \text{some } g(t) \in F[t] \]
	
	\item We say $ \lambda $ is a root of $ f \in F[t] $ with multiplicity $ e (\in \N) $ if $ (t - \lambda)^{e} $ divides $ f $, but $ (t - \lambda)^{e + 1} $ does not. 
	\item A poly of degree $ n $ has at most $ n $ roots, counted with multiplicity 
\end{itemize}

\begin{thm} Fundamental Theorem of Algebra
	Any $ f \in \C [t] $ of positive degree has a root (hence deg $ f $ roots.)
\end{thm}

\begin{defi}
	The characteristic polynomial of $ \alpha: \chi_{\alpha}(t) = \det (\alpha - t \iota) $. ($ \alpha \in L(V), A \in M_{n}(\F)  $).
	
	
\end{defi}
Conjugate matrices have same characteristic poly.

\begin{thm} 
	$ \alpha $ triangulable iff $ \chi_{\alpha}(t) $ can be written as a product of linear factors \emph{over} $ \F $.
	
	In particular, if $ \F = \C $, every matrix is triangulable.  
\end{thm}

\begin{proof}
	$ (\Rightarrow) $ Suppose $ \alpha $ is triangulable, and represented by 
	
	\[ \begin{pmatrix}
	a_{1} &  & * \\
	 & \ddots &  \\
	0 &  & a_{n}
	\end{pmatrix} \]
	
	
	wrt. some basis.
	
	Then $ \chi_{\alpha}(t) = \det \begin{pmatrix}
	a_{1}  - t&  & * \\
	 & \ddots &  \\
	0 &  & a_{n} - t
	\end{pmatrix} = \prod_{i=1}^{n}(a_{i} - t) $
	
	
	$ (\Leftarrow) $ Induction on $ n = \dim V $
	
	\begin{itemize}
		\item $ n = 0 $ or $ 1 $ Done.
		\item Suppose $ n > 1 $, and the thm holds for all endomorphisms of spaces of smaller dimension.
	\end{itemize}

	By hypothesis, $ \chi_{\alpha}(t) $ has a root in $ \F $, say $ \lambda $. Let $ U : = V_{\lambda} $ ($ \neq \{ \mathbf{0} \} $) 
	
	\[ \alpha(U) \leq U \Rightarrow \alpha \text{ induces  } \bar{a} : V / U \to V / U  \]
	
	Pick basis $ v_{1} | \cdots | v_{k} $ for $ U $, extend it to basis 
	
	\[ \mathcal{B} = \{  v_{1}, \cdots, v_{n} \} \text{ for } V \]
	
	wrt $ \mathcal{B} $, $ \alpha $ is represented by:
	
	\[ \begin{pmatrix}
	\lambda I_{k} & * \\
	0 & C
	\end{pmatrix} \]
	
	
	where $ \lambda I_{k} $ is the matrix of $ \alpha $ restricted to $ U $, $ \alpha |_{U} $, and $ C $ represents $ \bar{\alpha} $ wrt $ v_{k+1} + U, \alpha, v_{n} + U $.
	
	\begin{align*}
	\chi_{\alpha}(t) & = \det(\alpha - t \iota) \\
	& = (\lambda - t)^{k} \chi_{\bar{\alpha}}(t)
	\end{align*}
	
	Thus $ \chi_{\alpha} $ is \emph{also} a product of linear factors. By induction hypothesis, (since $ \bar{\alpha} $ is acting on a lower dimensional vector space ) there is a basis for $ V / U $, say $ w_{k+1} + U,\cdots,w_{n} + U $ wrt. which $ \bar{\alpha} $ is represented by an upper-triangular matrix, say $ T $.
	
	wrt $ v_{1},\cdots,v_{k},w_{k+1},\cdots,w_{n} $, $ \alpha $ is represented by 
	
	\[ \begin{pmatrix}
	\lambda I_{k} & * \\
	0 & T
	\end{pmatrix} \] 
	
\end{proof}

\begin{eg}
	$ \F = \R $, $ V = \R^{2} $, $ \alpha $ rotation
	
	\[ \begin{pmatrix}
	\cos \theta & \sin \theta \\
	- \sin \theta & \cos \theta
	\end{pmatrix} \]
	
	$ \chi_{\alpha}(t) = t^{2} - 2 \cos \theta t + 1 $ NOT triangulable over $ \R $
	
	(Conjugate to a diagonal matrix over $ \C $).
\end{eg}

\begin{lemma} 
	Let $ V $ be $ n $-dim over $ \F $, $ \alpha \in L(V) $. 
	
	\[ \chi_{\alpha}(t) = (-1)^{n}t^{n} + c_{n-1}t^{n-1} + \cdots + c_{0} \]
	
	Then:
	
	\begin{itemize}
		\item $ c_{0} = \det \alpha $
		\item for $ \F $ in $ \R $ or $ \C $, $ c_{n-1} = (-1)^{n-1} \text{tr }\alpha $
	\end{itemize}
\end{lemma}

\begin{proof}
	\begin{itemize}
		\item 	$ c_{0} = \chi_{\alpha}(0) = \det(\alpha - 0) = \det \alpha $
		\item For $ \F = \R  $, $ [\alpha]_{\mathcal{B}} $ can be thought of as a matrix over $ \C $ that happens to have real coeffs.
		
		\[ \chi_{\alpha}(t) = \det \begin{pmatrix}
		a_{0} - t & & *\\
		& \ddots & \\
		0 & & a_{n} - t
		\end{pmatrix} = \prod_{i=1}^{n} (a_{i} - t)  \]
		
		$ \sum_{i=1}^{n} a_{i} = \text{tr } \alpha $
	\end{itemize}
\end{proof}


Notation: $ p(t) $ is a poly over $ \F $, $ p(t) = a_{n}t^{n} + \cdots + a_{0} $, $ a_{i} \in \F $. For $ A \in M_{n}(\F)$, define $ P(A) = a_{n}A^{n} + a_{n-1} A^{n-1} + \cdots+ a_{0} I $, $ \alpha \in L(V) $ (over $ \F $), $ p(\alpha) = a_{n}\alpha^{n} + \cdots + \alpha_{0} \iota \in L(V)  $. ( where $ \alpha^{n} $)

\begin{thm} $ V $ v space over $ \F $, $ \dim V < \infty $. Let $ \alpha \in L(V) $. Then $ \alpha $ is diagonalisable iff $ p(\alpha) = 0 $ for some poly $ p \in F[t] $ which is the product of distinct linear factors.
	
	
	
\end{thm}	
	\begin{proof}
		$ (\Leftarrow) $ Suppose $ \alpha $ is diagonalisable, distinct evalues, $ \lambda_{1},\cdots,\lambda_{k} $. Let $ p(t) = (t - \lambda_{1})\cdots(t - \lambda_{k}) $. Let $ \mathcal{B} $ be a basis of evectors. For $ v \in \mathcal{B} $, $ \alpha(v) = \lambda_{i} v $ for some $ i $. This means
		
		\[ \Rightarrow (\alpha - \lambda_{i} \iota ) v = 0 \Rightarrow p(\alpha)(v)  = 0 \]
		
		As this holds for all $ v \in \mathcal{B} $, we have $ p(\alpha) = 0 $, done.
		
		$ (\Rightarrow) $ Suppose $ p(\alpha) = 0 $, for $ p(t) = \prod_{i=1}^{k}  (t - \lambda_{i}) $ wlog $ p(t) $ monic.
		
		Claim: $ V = \bigoplus_{i=1}^{k} V_{\lambda_{i}} $
		
		Proof of claim: Let $ q_{j}(t) = \prod_{i \neq j}^{k} \frac{t - \lambda_{i}}{\lambda_{j} - \lambda_{i} }   $ for $ j = 1, \cdots, k $ and $ q_{j}(\lambda_{i}) = \delta_{ij} $
		
		Let $ q(t) : = q_{1}(t) + \cdots + q_{k}(t) $
		
		$ q(t) $ has degree at most $ k - 1 $ (each of the $ q_{i} $ have deg at most $ k - 1 $). $ q(\lambda_{i}) = 1 $ for all $ i = 1,\cdots,k $. The only possibility is $ q(t) = 1 $ (constant map)
		
		
		Let $ \pi_{j} = q_{j}(\alpha): V \to V $. By construction, $ \sum_{j=1}^{k} \pi_{j} = q(\alpha) = \iota \in L(V) $.
		
		Given $ v \in V $, $ v = q(\alpha)v = \sum_{j=1}^{k} \pi_{j}(v)  $.
		
		Also, \[ (\alpha - \lambda_{j}\iota) (\pi_{j}(v)) = (\alpha - \lambda_{j} \iota) (q_{j}(\alpha)) (v) = \frac{1}{\prod_{i \neq j}  (\lambda_{j} - \lambda_{i}) }  p(\alpha) v = \mathbf{0}  \]
		
		So
		
		\[ \pi_{j}(v) \in \ker(\alpha - \lambda_{j} \iota   )V_{\lambda_{j}}   \] 
		
		Thus $ V = \sum V_{\lambda_{j}} $.
		
		To see that the sum is direct, suppose
		
		\[ v \in V_{\lambda_{j}} \cap \left(  \sum_{j \neq i}^{k}V_{\lambda_{i}} \right) \text{ and apply } \pi_{j} \text{ to } v \]
		
		$ v \in V_{\lambda_{j}} \Rightarrow  \pi_{j}(v) = \prod_{j \neq i}^{k}  \frac{\lambda_{j}  - \lambda_{i} }{\lambda_{j} - \lambda_{i}} v = v \qquad \left(  \alpha v = \lambda_{j} v \right)  $
		
		$ v \in \sum_{i \neq j} V_{\lambda_{i}} \Rightarrow \pi_{j}(v) = 0  $. Thus $ v = 0 $ and the sum is direct. 
		
		
		Now take the union of bases for $ V_{\lambda_{i}} $ as a basis for $ V $
		
	\end{proof}
	


Remarks:

\begin{itemize}
	\item Morally speaking, $ \pi_{j} $ is `projecting' to the $ V_{\lambda_{j}} $
	
	\item Proof shows that for $ k $ distinct evalues $ \lambda_{1},\cdots,\lambda_{k} $ of $ \alpha $, the sum $ \sum V_{\lambda_{j}} $ is direct: $ \sum V_{\lambda_{j}} = \bigoplus V_{\lambda_{j}} $.
	
	\item The only way diagonalisation fails is if $ \sum V_{\lambda_{j}} $ is not a subspace of $ V $. $ (\not\leq) $
\end{itemize}


\begin{cor} 
	If $ A \in M_{n}(\C) $ has finite order, ($ A^{m} = I $ for some $ m $). Then $ A $ is diagonalisable. 
\end{cor}

\begin{proof}
	$ p(A) = 0 $ for $ p(t) = t^{m} - 1 =  \prod_{i=0}^{m-1} (t - \xi^{i}) $ where $ \xi $ is $ m^{\text{th}} $ root of $ 1 $. (Now over complex numbers)
\end{proof}

\begin{thm} 
	Simultaneous diagonalisation:
	Let $ \alpha,\beta \in L(V) $ diagonalisable. Then $ \alpha,\beta $ are simultaneously diagonalisable (there exists a basis wrt which they're both diagonal) iff $ \alpha $ and $ \beta $ commute.
\end{thm}

\begin{proof}
	($ \Rightarrow $) Suppose there is a basis $ \mathcal{B} $ st. $ A = [ \alpha ]_{\mathcal{B}} $ and $ B = [\beta]_{\mathcal{B}}  $ diagonal. Any two diagonal matrices commute, so $ AB = BA $, so $ \alpha \beta = \beta\alpha $.
	
	
	($ \Leftarrow $) Suppose $ \alpha,\beta $ commute, both diagonalisable. We have $ V = V_{1} \oplus \cdots \oplus V_{k} $, where $ V_{i} = \ker (\alpha - \lambda_{i} \iota ) $ ($ V_{i} $ is an eigenspace for $ \alpha $).
	
	Claim: $ \beta(V_{j}) \leq V_{j} $ (still lands inside)
	
	Suppose $ v \in V_{j} $, $ \alpha \beta(v) = \beta \alpha(v) = \beta \lambda_{j} v = \lambda_{j} \beta (v)  $.
	
	As $ \beta $ is diagonalisable, theres a poly $ p $ with distinct linear factors st. $ p(\beta) = 0 $. 
	
	Now $ p(\beta |_{V_{i}}) = p(\beta)|_{V_{i}} = 0 \Rightarrow \beta |_{V_{i}} \in L(V_{i}) $ is diagon. 
	
	Pick a basis $ \beta_{i} $ of $ V_{i} $ consisting of evectors for $ \beta $. By construction, these are real evectors for $ \alpha $, and wrt $ \mathcal{B} = \cup_{i} \mathcal{B}_{i}  $ both $ \alpha $ and $ \beta $ are diagonal. 
 	
	
\end{proof}


\begin{lemma} 
	Euclidean alg for polynomails:
	
	Given $ a,b \in \F[t] $, with $ b \neq 0 $, then there exists polynomials $ q,r \in F[t] $ with deg $ r  < \deg b$ and $ a = qb + r $ 
\end{lemma}

\begin{proof}
	exercise (induction on deg) or see GRM
\end{proof}

\begin{defi}
	$ \alpha \in L(V) $, $ \dim V < \infty $. The minimal poly of $ \alpha $, $ m_{\alpha} $, is the non zero monic poly of smallest deg st. $ m_{\alpha}(\alpha) = 0 $
\end{defi}


Remarks (Existence and Uniqueness)
\begin{itemize}
	\item Say $ \dim_{\F} V = n < \infty $, $ \dim L(V) = n^{2} $.
	
	So $ \iota, \alpha,\alpha^{2},\cdots,\alpha^{n^{2}} \in L(V) $ must be linearly dependent, so $ \alpha_{n^{2}}  \alpha^{n^{2}} + \cdots + \alpha_{1} \alpha  + \alpha_{0} \iota $ for some $ \alpha_{i} \in \F $ not all zero. So min poly existt.  
\end{itemize}

\begin{lemma} 
	Let $ \alpha \in L(V) $, $ p \in \F [t] $. Then $ p(\alpha) = 0 $ iff $ m_{\alpha}(t) \; | \; p(t) $.
	
\end{lemma}

\begin{proof}
	Have $ q,r \in \F [t] $ st. $ p(t) = m_{\alpha}(t) q(t) + r(t) ( \deg r < \deg m_{\alpha}). $
	
	\begin{align*}
	0 & =p(\alpha) \\
	& = \underbrace{m_{\alpha}(\alpha)} q(\alpha) + r(\alpha) \\
	& \Rightarrow r(\alpha) = 0 \in L(V)
	\end{align*}
	
	By minimality of $ \deg m_{\alpha} $, $ r(t) = 0 $
\end{proof}

\begin{cor}  $ m_{\alpha} $ is uniquely defined.
	
\end{cor}

\begin{proof}
	Say $ m_{1} $ and $ m_{2} $ both minimal. Then $ m_{1} | m_{2} $ and $ m_{2} | m_{1} $, both are monic, so $ m_{1} = m_{2}  $.
\end{proof}

\begin{thm} (Cayley Hamilton)
	Let $ V  $ v space over $ \F $, $ \dim V < \infty $. Let $ \alpha \in L(V) $. Then $ \chi_{\alpha}(\alpha) = 0 \in L(V) $.
\end{thm}


\begin{proof}
	\begin{itemize}
		\item $ \F = \C $
		
		For some basis $ \mathcal{B} = \{ v_{1},\cdots,v_{n} \} $, $ [\alpha]_{\mathcal{B}} = \begin{pmatrix}
		a_{1} & & & \\
		& a_{2} & & \\
		& & \ddots & \\
		0 & & a_{n}
		\end{pmatrix} $
		
		
		
		Let $ U_{j} := < v_{1},\cdots,v_{j} > \cdots $. Then $ (\alpha - a_{j} \iota ) U_{j} \leq U_{j-1} $. So
		
		\[ (\alpha - \alpha_{1} \iota)(\alpha - \alpha_{2} \iota)  \cdots  (\alpha - \alpha_{n-1} \iota)\underbrace{(\alpha - \alpha_{n} \iota) V}_{\leq U_{n-1}} \]
		
		also $ \underbrace{(\alpha - \alpha_{n-1} \iota) (\alpha - \alpha_{n} \iota) V}_{\leq U_{n-2}} $
		
		and so on, until the whole thing 
		
		\[ \underbrace{(\alpha - \alpha_{1} \iota)(\alpha - \alpha_{2} \iota)  \cdots  (\alpha - \alpha_{n-1} \iota)(\alpha - \alpha_{n} \iota) V}_{\leq  (\alpha - \alpha_{1} \iota)U_{1} = 
			\{ \mathbf{0} \}  }  \]
		
		So $ \xi_{\alpha}(\alpha) = 0 $
		
		\item General Field $ \F $
		
		$ A \in M_{n}(\F) $. 
		
		\begin{align*}
		\chi_{A}(t) (-1)^{n} & =  t^{n} + a_{n-1} t^{n-1} + \cdots + a_{1} t + a_{0} \\
		& = \det (t I - A)
		\end{align*}
		
		For any matrix $ B $, $ B \text{ adj } B = (\det B) I $.
		
		$ B = t I - A $ : adj ($ B $) matrix with entries in polys in $ t $, of degree $  < n $, ie. polynomials in $ t $ with coeffs in $ M_{n}(\F) $
		
		
		\[ \underbrace{B_{n-1}   t^{n-1}  + \cdots + B_{1} t + B_{0}  }_{\text{adj}(B), \text{some} B_{i} \in M_{n}(\F) } \]
		
		\[ = \underbrace{( t^{n}   + a_{n-1}  t^{n-1}  + \cdots + a_{0}   )}_{\det B} I \]
		
		
		Equate coeffs (powers of $ t $) :
		
		
		\begin{align*}
		I & = B_{n-1} \\
		a_{n-1} I & = B_{n-2} - A B_{n-1} \\
		\vdots \\
		a_{0}I & = - A B_{0} 
		\end{align*}
		
		
		Multiply the first equation by $ A^{n} $, the second by $ A^{n-1} $, $ \cdots $, and the last by $ A_{0} $. Then add all these, and this yields
		
		\[  A^{n} + a_{n-1} A^{n-1} + \cdots + a_{1} A + a_{0} I = 0  \]
		
		
		
	\end{itemize}
\end{proof}


\begin{defi}
	$ \lambda $ an evalue of $ \alpha \in L(V) $, $ \dim V < \infty $. 
	
	\[ \chi_{\alpha}(t) = (t - \lambda )^{\alpha_{\lambda}} q(t)  \]
	
	
	some $ q \in F[t] $, $ (t - \lambda) \not | q(t) $.
	 
	$ a_{\lambda} $ \emph{algebraic multiplicity} of $ \lambda $ a an e value of $ \alpha $.
	
	
	$ g_{\lambda} = n(\alpha - \lambda \iota) $ is the \emph{geometric multiplicity} of $ \lambda $ as an evalue of $ \alpha $. 
\end{defi}


\begin{lemma} If $ \lambda $ evalue, $ 1 \leq g_{\lambda} \leq \alpha_{\lambda} $. 
	
\end{lemma}

\begin{proof}
	\begin{itemize}
		\item $ 1 \leq g_{\lambda} $, since $ \alpha- \lambda \iota $ is singular
		
		\item $ g_{\lambda} \leq a_{\lambda} $ ? Let $ \mathcal{B} = \{ v_{1},\cdots,v_{n} \} $ a basis of $ V $ with $ \{  v_{1},\cdots,v_{g} \} $ a basis of $ N(\alpha - \lambda \iota) $, ($ g = g_{\lambda} $). (Note $ N(\alpha - \lambda \iota) $ is $ V_{\lambda} $)
		
		\[ [\alpha]_{\mathcal{B}} = \begin{pmatrix}
		\lambda I_{g} & * \\
		0 & A_{1}
		\end{pmatrix}, \text{ some } A_{1} \in M_{n-g}(\F) \]
		
		
		
		\[ \chi_{\alpha}(t) = (t - \lambda)^{g}  \chi_{A_{1}}(t), \text{ so } g_{\lambda} \leq a_{\lambda}   \]
		
		
	\end{itemize}
	
	
\end{proof}


\begin{lemma} 
	$ \lambda $ an evalue. Let $ c_{\lambda} $ be the multiplicity of $ \lambda $ as a root of $ m_{\alpha} $. Then $ 1 \leq c_{\lambda} \leq a_{\lambda} $. 
\end{lemma}

\begin{proof}
	\begin{itemize}
		\item $ m_{\alpha} | \chi_{\alpha}  $ (as both of them applied to $ \alpha  $ are zero ) $ \Rightarrow c_{\lambda} \leq a_{\lambda} $. 
		
		\item For $ 1 \leq c_{\lambda} $, $ \lambda $ an evalue, so $ \alpha v = \lambda v $ for some $ v \in V \setminus \{ \mathbf{0} \} $.
		
		Claim $ m_{\alpha}(\alpha) v = m_{\alpha}(\lambda) v $ as $ ( \forall \; p \in \F[t] , p(\alpha) v = p(\lambda) v )    $. This is also zero as it is the minimal poly. Hnec
		
		\[ m_{\alpha}(\lambda) = 0  \in \F  \quad (v \neq \mathbf{0} ) \]
		
		and
		
		\[ t - \lambda \; | \; m_{\alpha}(t) \]
		
		
		
	\end{itemize}
\end{proof}


\begin{eg}
	
	\[ A = \begin{pmatrix}
	1 & 0 & -2 \\
	0 & 1 & 1\\
	0 & 0 & 2
	\end{pmatrix} \]
	  
	  
	  \[ \chi_{A}(t)  = |  A - t I | = (2 - t)(1 - t)^{2}  \]
	
	Choices for $ m_{\alpha} :  $
	
	\begin{enumerate}[label = (\alph*)]
		\item $ (t - 2)(t - 1)^{2} $
		\item $ (t - 2)(t - 1) $
		
	\end{enumerate}	
	Check:
	
	\[ (A - I)(A - 2I) = 0 \]
	
	So (b) holds, so $ A $ diagonalisable.
\end{eg}

\begin{eg}
	$ A = \begin{pmatrix}
	\lambda^{1} & & 0 \\
	& \lambda^{1} & &
	& & \ddots & \\
	& 0 & & \lambda^{1} 
	\end{pmatrix} $
	
	
	Check $ g_{\lambda} = 1 $, $ a_{\lambda} = n $, $ c_{\lambda} = n $.
	
\end{eg}

\begin{lemma} $ (\F = \C) $
	$ \alpha \in L(V) $. TFAE
	\begin{enumerate}
		\item $ \alpha $ diagonalisable
		\item $ \alpha_{\lambda} = g_{\lambda} $ for all eigenvalue $ \lambda $
		\item $ c_{\lambda} = 1 $ for all eigenvalue $ \lambda $
		
	\end{enumerate}
\end{lemma}

\begin{proof}
	\begin{itemize}
		\item (i) $ \iff $ (ii): Let $ \lambda_{1}, \cdots, \lambda_{k} $ evalues of $ \alpha $. 
		
		\[ \alpha \text{ diagonalisable } \iff V = \bigoplus V_{\lambda_{k}} \]
		
		where with $ V $, $ \dim n  = \deg \chi_{\alpha} = a_{1} + \cdots + a_{k}$, and $ \dim  $ RHS $ = g_{1} + \cdots + g_{k} $
		
		fund theorem of algebra.
		
		$ g_{2} \leq a_{2} $ for all $ i $, so $ \alpha $ diagonalisable iff $ g_{i} = a_{i} $ for all $ i $.
		
		\item (ii) $ \iff $ (iii). By the fund theorem of alg, $ m_{\alpha} $ is a product of linear factors. 
		
		$ \alpha $ is diagonalisable iff all of these linear factors are distinct, ie. $ c_{\lambda} = 1 $ for all evalues $ \lambda $.  
		
		\item 
	\end{itemize}
\end{proof}

Remark: Over $ \C $,

\begin{align*}
\chi_{\alpha}(t) & = (\lambda_{1} - t)^{\alpha_{1}}\cdots (\lambda_{k} - t)^{\alpha_{k}} \quad \lambda_{i} \text{ all evalues } \\
m_{\alpha} (t) & = ( t - \lambda_{1})^{c_{1}} \cdots (t - \lambda_{k} )^{c_{k}} \quad \text{ with } 1 \leq c_{i} \leq a_{i}
\end{align*}


\begin{defi}
	$ A \in M_{n}(\C) $ is in \emph{Jordan Normal Form} (JNF) if it is a block diagonal matrix
	
	\[ A = \begin{pmatrix}
	J_{n_{1}}(\lambda_{1}) & & 0\\
	& J_{n_{2}}(\lambda_{2}) & 
	0 & & \cdots J_{n_{k}}(\lambda_{k})
	\end{pmatrix} \]
	
	
	where $ k \geq 1 $, $ n_{1},\cdots,n_{k} \in \N $, $ \sum n_{i} = n $, $ \lambda_{i} \in \F $ (needn't be distinct)
	
	
	
	\[ J_{m}(\lambda) = A = \begin{pmatrix}
	\lambda^{1} & 1 & & \\
	& \lambda^{1} & 1 & \\
	& & \ddots & \\
	& 0 & & \lambda^{1} 
	\end{pmatrix} \]
	
	where $ J_{m}(\lambda) \in M_{m} \C  $ is a Jordan block
	
\end{defi}
 
 
 \begin{thm} 
 	Every $ A \in M_{n}\C $ is similar to a matrix in JNF, unique up to reordering the Jordan black
 \end{thm}

\begin{proof}
	(Non-examinable) consequence of main thm on modules in GRM.
\end{proof}

\begin{eg}
	Possible JNFs for $ A \in M_{2} \C $
	
	
	
	\[ \begin{pmatrix}
	\lambda_{1} & \\
	& \lambda_{2}
	\end{pmatrix} \; \begin{pmatrix}
	\lambda & 0 \\
	0 & \lambda
	\end{pmatrix}\; \begin{pmatrix}
	\lambda & 1\\
	0 & \lambda
	\end{pmatrix}
	\]
	
	
	\[ m_{A} = (t - \lambda_{1})(t - \lambda_{2}) \quad (t - \lambda) \quad (t - \lambda)^{2} \]
	
\end{eg}


\begin{eg}
	Possible JNFs for $ A \in M_{3} \C $
	
	$ \lambda_{i} $ distinct gives
	
	\[ \begin{pmatrix}
	\lambda_{1} & & \\
	& \lambda_{2} & \\
	& & \lambda_{3}
	\end{pmatrix} \]
	
	with $ m_{A} = (t - \lambda_{1}) (t - \lambda_{2}) (t - \lambda_{3}) $
	
	
	or
	
	\[ \begin{pmatrix}
	\lambda_{1} & & \\
	& \lambda_{2} & \\
	& & \lambda_{2}
	\end{pmatrix} \]
	
	with $ m_{A} =  (t - \lambda_{1})(t - \lambda_{2})  $
	
	or 
	
		
	\[ \begin{pmatrix}
	\lambda & & \\
	& \lambda & \\
	& & \lambda
	\end{pmatrix} \]
	
	with $ m_{A} = (t - \lambda) $
	
	or 
	
		\[ \begin{pmatrix}
	\lambda_{1} & & \\
	& \lambda_{2} 1 \\
	& & \lambda_{2}
	\end{pmatrix} \]
	
	with $ (t - \lambda_{1})(t - \lambda_{2})^{2} $
	
	or 
	
	\[ \begin{pmatrix}
	\lambda & & \\
	& \lambda 1 \\
	& & \lambda
	\end{pmatrix} \]
	
	with $ (t - \lambda)^{2} $
	
	and 
	
	\[ \begin{pmatrix}
	\lambda 1 1 \\
	& \lambda 1 \\
	& & \lambda
	\end{pmatrix} \]
	
	with $ (t - \lambda )^{3} $
	
\end{eg}


\begin{thm} (Generalised eigenspace decomposition)
	
	$ V $ f dim v space over $ \C $, $ \alpha \in L(V ) $. Suppose that 
	
	\[ m_{\alpha} (t) = (t - \lambda_{1})^{c_{1}} \cdots (t - \lambda_{k})^{c_{k}}  \qquad \lambda_{i} \text{ distinct }\]
	
	Then
	
	\[ V = \bigoplus V_{j} \]
	
	where $ V_{j} = N(  ( \alpha - \lambda_{j} \iota )^{c_{j}} ) $
	
	generalised space.
	
\end{thm}

\begin{proof}(Sketch)
	
	Let \[ p_{j}(t) = \prod_{i \neq j} (t - \lambda_{i})^{c_{i}}   \]
	
	The $ p_{j} $ have no common factor, so by Euclid's algorithm we can find $ q_{1}, \cdots, q_{k} \in \C [t]  $ st. $ \sum p_{j}(t) q_{j}(t) = 1 $
	
	
	Let $ \pi_{j} = q_{j}(\alpha) p_{j}(\alpha) \in L(V)  $. Note $ \sum_{j=1}^{k} \pi_{j} = \iota$.
	
	These $ \pi_{j} $ are sort of like projection maps as before, now projecting to generalised eigenspaces
	
	\begin{itemize}
		\item As $ m_{\alpha}(\alpha) = 0 $, so 
		
		\[ (  \alpha - \lambda_{j} \iota )^{c_{j}} \pi_{j} = 0 \Rightarrow \Im \pi_{j} \leq V_{j}   \]
		
		
		\item Suppose $ v \in V $,
		
		\[ v = \iota(v) = \sum \pi_{j}(v) \Rightarrow V = \sum V_{j} \]

		\item Directness $ \pi_{i} \pi_{j} = 0 $ for $ i \neq j $
		
		\[ \Rightarrow \pi_{i} = \pi_{i} \left( \sum_{j=1}^{n} \pi_{j} \right)  = \pi_{i}^{2} \text{ projection}  \]
		
		and so
		
		\[ \pi_{i} |_{v_{j}} = \begin{cases} \text{Id}  & \text{ if } i = j \\ 0 & \text{ if } i \neq j \end{cases}  \]

		and directness follows
	\end{itemize}
	
\end{proof}

Remarks:

\begin{enumerate}
	\item Can use this to reduce the proof of JNF to a single eigenvalue.
	
	\item Considering $ \alpha - \lambda \iota  $ can reduce to the case of evalue 0.

	
\end{enumerate} 


\begin{lemma} 
	Let $ \alpha \in L(V) $ with JNF $ A \in M_{n} \C $.
	
	\begin{align*}
	& \text{number of } \{  \text{Jordan blocks } J_{l}(\lambda) \text{ of } A \text{ with } l \geq r \} \\
	& = n \left( ( \alpha - \lambda_{i})^{r}   \right) - n \left(  ( \alpha - \lambda_{i} )^{r - 1} \right)  
	\end{align*}
\end{lemma}

\begin{proof}
	Work blockwise
	
	\[ J_{s}(\lambda) = \begin{pmatrix}
	\lambda & 1 & 0 \\
	& \vdots &\\
	0 & & \lambda
	\end{pmatrix}_{S \times S}, \qquad J_{s}( \lambda) s \lambda I_{s} = \begin{pmatrix}
	0 & 1 & & 0\\
	& \vdots & & \\
	0 & & 0
	\end{pmatrix} \quad (r = 1, \text{nullity} = 1 ) \]
	
	
	\[ ( J_{s}( \lambda) s \lambda I_{s})^{2} = \begin{pmatrix}
	0 & 0 & 1 & & 0\\
	& \vdots & & \\
	0 & & 0
	\end{pmatrix} \text{ nullity } 2 \]
	
	Hence 
	
	\[ n( ( J_{s}( \lambda) s \lambda I_{s})^{k} ) \begin{cases}  k   & \text{ if } k \leq s \\ s & \text{ if } k \geq s \end{cases}\]
\end{proof}

\begin{eg}
	\[ A = \begin{pmatrix}
	0 & -1 \\
	1 & 2
	\end{pmatrix} 
	\]
	
	Want JNF and a basis $ \mathcal{B} = \{ v_{1},v_{2} \} $ wrt which $ A $ is in JNF.
	
	\begin{itemize}
		\item \[ \chi_{A}(t) = \begin{vmatrix}
		-t & -1 \\
		1 & 2 - t
		\end{vmatrix} = t^{2} - 2t + 1 = (t - 1)^{2} \]
		
		2 possibilities, either $ m_{A} = t-1 $ or $ m_{A} = (t - 1)^{2} $
		
		In each case,
		
		\[ \text{JNF } = \begin{pmatrix}
		1 & 0 \\
		0 & 1
		\end{pmatrix}
		\quad 
		\text{JNF } = \begin{pmatrix}
		1 & 1 \\
		0 & 1
		\end{pmatrix} \]
		
		Note: if $ A $ was conjugate to $ I $, then $ A = I $ ($ P^{-1} A P = I $ for any $ P $ invertible). So it is the second case!
		
		\item Espace
		
		\[ A - I = \begin{pmatrix}
		-1 & -1 \\
		1 & 1
		\end{pmatrix}, \text{ ker spanned } v_{1} \begin{pmatrix}
		1 \\
		-1
		\end{pmatrix} \]
		
		Aside: see Michaels Notes
		
		\item $ v_{2} $ satisfies $ ( A - I)v_{2} = v_{1} $. 
		
		\[ \begin{pmatrix}
		-1 & -1 \\
		1 & 1
		\end{pmatrix} v_{2} = \begin{pmatrix}
		1\\
		-1
		\end{pmatrix} \Rightarrow v_{2} = \begin{pmatrix}
		-1\\
		0
		\end{pmatrix} \text{ (NOT unique!)} \]
		
		
		\[ A = \begin{pmatrix}
		-1 & -1 \\
		-1 & 0
		\end{pmatrix}  \begin{pmatrix}
		1 & 1 \\
		0 & 1
		\end{pmatrix} \begin{pmatrix}
		-1 & -1 \\
		-1 & 0
		\end{pmatrix}^{-1} \]
		
		$ A = P^{-1} A P $
		
		
		
	\end{itemize}



Now, suppose we want to find some high power of $ A $. Can use JNF.

\begin{align*}
A^{n} & = ( P^{-1} J P )^{n} \\
& = P^{-1} J^{n} P \\
& = P^{-1} \begin{pmatrix}
1 & n \\
0 & 1
\end{pmatrix} P 
\end{align*}




	
\end{eg}


Remark: In JNF:

\begin{itemize}
	\item $ a_{\lambda} =  $ total number of times that $ \lambda $ appears in diagonal
	\item $ g_{\lambda} = $ number of $ \lambda $-Jordan blocks
	\item $ c_{\lambda} =  $ size of the largest $ \lambda $-Jordan Block
\end{itemize}


\section{Bilnear Forms II}

\[ \varphi : V \times V \to \F \]

This chapter: same basis for both factors of $ V $, say $ \mathcal{B} $. For $ \dim_{\F} V < \infty $, matrix representation $ [ \varphi ]_{\mathcal{B}} ( = [  \varphi_{\mathcal{B},\mathcal{B}} ] ) $


\begin{lemma} 
	$ \psi : V \times V \to \F, \dim_{\F} V < \infty $, $ \mathcal{B},\mathcal{B}' $ bases for $ V $. Let $ P = [\id]_{\mathcal{B},\mathcal{B}'} $. Then
	
	\[ [\psi]_{\mathcal{B'}} = P^{T} [\psi]_{\mathcal{B}} P \]
\end{lemma}

\begin{proof}
	Special case of L10.
\end{proof}


\begin{defi}
	$ A,B \in M_{n}(\F) $ are \emph{congruent} if $ A = P^{T} B P $ for some invertible $ P $. 
\end{defi}

Note: This is an equivalence relation

\begin{defi}
	A bilinear form on $ V $ is \emph{symmetric} if $ \psi(u,v) = \psi(v,u) $ for all $ u,v \in V $
\end{defi}

Note: $ A \in M_{n}(\F) $ is symmetric if $ A = A^{T} $.

$ \varphi $ is symmetric $ \iff $ $ [\varphi]_{\mathcal{B}} $ is symmetric for any basis $ \mathcal{B} $. (enough $ [\varphi]_{\mathcal{B}} $ symmetric for one $ \mathcal{B} $ ).

Note: To be able to represent $ \varphi $ by a diagonal matrix, $ \varphi $ needs to be symmetric. 

\[ P^{T} \underbrace{A}_{= [ \varphi ]_{\mathcal{B}}} P = D \]

where $ D $ is diagonal, so 

\[ \underbrace{\Rightarrow D^{T}}_{\text{because } D \text{ diagonal}} = P^{T} A^{T} P \Rightarrow A = A^{T}  \]

\begin{defi}
	A map $ Q : V \to \F $ is \emph{quadratic form} if there is a bilinear form $ \varphi: V \times V \to \F  $ s.t. $ Q(v)= \varphi(v,v)  $ for all vectors $ v \in V $.
\end{defi}


\begin{eg}
	$ V = \R^{2} $
	
	
	\[ \begin{pmatrix}
	x\\
	y
	\end{pmatrix}  \mapsto \begin{pmatrix}
	x & y
	\end{pmatrix} \begin{pmatrix}
	a & b\\
	c & d
	\end{pmatrix} \begin{pmatrix}
	x \\
	y
	\end{pmatrix} = ax^{2} + (b+c)xy + dy^{2}  \]
\end{eg}

Rk: Wouldn't change if replace $ A $ with $ \frac{1}{2} (A + A^{T})  $

\begin{prop} ( Assume $ 1 + 1 \neq 0 \in \F $)
	If $ Q : V \to \F $ is a quadratic form then there exists a unique symmetric bilinear form $ \varphi: V \times V \to \F $ st. $ Q(u) = \varphi(u,u)  $ for all $ u \in V $
\end{prop}

\begin{proof}
	\begin{itemize}
		\item Existance: Let $ \psi  $ bilinear form on $ V $ st. $ Q(u) = \psi(u,u) $. Let 
		
		\[ \varphi(u,v) = \frac{1}{2} (    \psi(u,v) + \psi(v,u ) \]
		
		\item bilinear, symmetric.
		\item $ \varphi(u,u) = \psi(u,u) = Q(u) $
		
		\item Uniqueness: Suppose $ \varphi $ is such a symmetric bilnear from. 
		
		
		\begin{align*}
		Q(u+v)& = \varphi(u+v,u+v)  \\
		& = \varphi(u,u) + \varphi(u,v) + \varphi(v,u) + \varphi(v,v) \\
		& = Q(u) + 2\varphi(u,v) + Q(v)
		\end{align*}
		so
		
		(POLARISATION IDENTITY)
		\[ \varphi(u,v) = \frac{1}{2} \left(  Q(u+v) - Q(u) - Q(v) \right)  \]

		Any such $ \varphi $ is determined by a starting $ Q $, so uniquely determined.
		
	\end{itemize}
\end{proof}

\begin{thm} 
	Let $ \varphi : V \times V \to \F $ symmetric bilinear from, assume $ 1 + 1 \neq 0 \in \F $ (eg $ \F = \R $ or $ \C $), $ \dim_{\F}V < \infty $.
	
	Then there's a basis $ \mathcal{B} $ of $ V $ st. $ [\varphi]_{\mathcal{B}} $ is diagonal
\end{thm}

\begin{proof}
	(Induction on the dimension of $ V $, a pretty common technique) ($ n = \dim V $)
	
	\begin{itemize}
		\item $ n = 0,1 $ Done.
		\item Suppose thm holds for all spaces of $ \dim < n $. If $ \varphi(u,u) = 0 $ for all $ u $, then by polarisation identity, $ \varphi $ is identically zero, done.
		Otherwise choose $ e_{1} \in V  $ s.t. $ \varphi(e_{1},e_{1}) \neq 0 $.
		
		Let 
		
	\[ U = < e_{1} >^{\perp} = \{  u \in V \; | \; \varphi(e_{1},e_{1}) = 0 \} \]
	
	\[ = \ker\{ \varphi(e_{1},-) \; | \; V \to \F  \} \]
	
	
	$ \dim U = n - 1$ by rank nullity. Moreover,
	
	\[ V = <e_{1}> \oplus \; U \]
	
	
	Note: $ <e_{1}>   \cap \; U = \{  \mathbf{0} \} $, $ \dim (  <e_{1}>  \oplus \; U ) = 1 + n - 1 = n$
	
	
	Consider $ \varphi \big|_{U} : U \times U \to \F  $, bilinear, symmetric. By the induction hypothesis, there is a basis of $ U $, say $ e_{2},\cdots,e_{n} $ wrt. which $ \varphi \big|_{U} $ is diagonal.
	
	Now $ \varphi $ is diagonal wrt $ e_{1},\cdots,e_{n} $
	
	\end{itemize}                                                   
\end{proof}     


\begin{eg},
	
	
	$ V = \R^{3} $, std $ e_{1},e_{2},e_{3} $
	
	
	\[ Q(\underbrace{x_{1},x_{2},x_{3}}_{\sum_{i=1}^{3}x_{i}e_{i}  }) = x_{1}^{2} + x_{2}^{2} + 2x_{3}^{2} + 2x_{1}x_{2} + 2 x_{1}x_{3} - 2 x_{2}x_{3}  \]
	
	Want a basis $ f_{1},f_{2},f_{3} $ of $ \R^{3} $ st. 
	
	\[ Q(af_{1} + b f_{2} + c f_{3})  = \lambda a^{2} + \mu b^{2} + \nu c^{2} \] 
	
	some $ \lambda,\mu, \nu  \in \R $. (diagonal entries)
	
	The matrix wrt. std basis for bilinear symmetric form is 
	
	\[ \begin{pmatrix}
	1 & 1 & 1\\
	1 & 1 & -1 \\
	1 & -1 & 2 
	\end{pmatrix} \]
	
	How to diagonalise?
	
	Method 1: Complete the square. Use up all terms in $ x_{1} $, then use up all terms in $ x_{2} $ or $ x_{3} $, whichever easier!
	
	\begin{align*}
	Q(x_{1},x_{2},x_{3})  & =  (x_{1} + x_{2} + x_{3} )^{2} + x_{3}^{2} - 2 x_{2}x_{3} - 2 x_{2}x_{3}  \\
	& =  (x_{1} + x_{2} + x_{3} )^{2} - (x_{3}  - 2x_{2} )^{2} - 4 x_{2}^{2}
	\end{align*}
	
	
	So for some $ P $, $ P^{T} A P =\begin{pmatrix}
	1 & & 0 \\
	& 1 & \\
	0 & & -1
	\end{pmatrix} $
	
	To find $ P $, notice that 
	
	\[ \begin{pmatrix}
	x_{1}' \\
	x_{2}' \\
	x_{3}'
	\end{pmatrix} = \begin{pmatrix}
	1 & 1 & 1 \\
	0 & -2 & 1\\
	0 & 2 & 0
	\end{pmatrix}\begin{pmatrix}
	x_{1} \\
	x_{2} \\
	x_{3}
	\end{pmatrix} \]
	
	where the matrix is $ P^{-1} $.
	
	Method 2: Follow steps in diag prof.
	
\end{eg}                                                 















\end{document}