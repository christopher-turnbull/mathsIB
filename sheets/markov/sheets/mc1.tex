\documentclass[a4paper]{article}
\usepackage{amsmath}
\def\npart {IA}
\def\nterm {Michaelmas}
\def\nyear {2017}
\def\nlecturer {Prof Weber (rrw1@cam.ac.uk)}
\def\ncourse {Markov Chains Example Sheet 1}

\input{header}

\newtheorem*{soln}{Solution}

\renewcommand{\thesection}{}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\makeatletter
\def\@seccntformat#1{\csname #1ignore\expandafter\endcsname\csname the#1\endcsname\quad}
\let\sectionignore\@gobbletwo
\let\latex@numberline\numberline
\def\numberline#1{\if\relax#1\relax\else\latex@numberline{#1}\fi}
\makeatother


\begin{document}
	
\maketitle

\section{QUESTION 1}

$ X = (X_{n}) $ is a Markov Chain, and so satisfies the Markov Property

\[ \P( X_{n+1} = i_{n+1} \; | \; X_{0} = i_{0}, \cdots, X_{n} = i_{n} ) = \P(X_{n+1} = i_{n+1} \; | \; X_{n} = i_{n} ) \]

Given that $ Z_{k} = X_{m+k} $, with $ k \geq 0 $, want to show that $ Z_{k} $ is a Markov Chain, ie.

\[ \P(Z_{n+1}  \; | \; Z_{0},\cdots,Z_{n} ) = \P(Z_{n+1} \; | \; Z_{n} ) \qquad (*) \]

Substituting in our definition of $ Z_{k} $,

\begin{align*}
\P(Z_{n+1}  \; | \; Z_{0},\cdots,Z_{n} ) & = \P(X_{m+n+1}|X_{m},\cdots,X_{m+n}) \\
& = \P(X_{m+n+1} | X_{m+n}) \qquad \text{ as } X \text{ is Markov}\\
& = \P(Z_{n+1} \; | \; Z_{n} )
\end{align*}

Hence $ Z_{k} $ satisfies the Markov property, and is thus a Markov Chain.

$ Z_{0} = X_{m} = i $, so $ Z_{k} $ has starting state $ i $.

\section{QUESTION 2}

Let $ X_{1},\cdots,X_{n} $ be independent random variables, so $ \P(X_{n+1} = i_{n+1} \; | \; X_{n} = i_{n} ) = \P(X_{n+1} = i_{n+1}  ) $. But then 

\[ \P( X_{n+1} = i_{n+1} \; | \; X_{0} = i_{0}, \cdots, X_{n} = i_{n} ) = \P(X_{n+1} = i_{n+1}) \] 

also by independence, so Markov property is satisfied.

Homogeneous if $ \P(X_{n+1} = i_{n+1} \; | \; X_{n} = i_{n}) $ does not depend on the value of $ n $, so must have $ X_{n} $ identically distributed. 



\section{QUESTION 3}

As $ X_{n} $ is the \emph{maximum} reading obtained after $ n $ throws, this depends only on the last maximum, so $ X_{n} $ is a Markov chain. In particular, the state transition matrix is


\[ \begin{pmatrix}
1/6 & 1/6 & 1/6 & 1/6 & 1/6 & 1/6  \\
0 & 2/6 & 1/6 & 1/6 & 1/6 & 1/6  \\
0 & 0 & 3/6 & 1/6 & 1/6 & 1/6  \\
0 & 0 & 0 & 4/6 & 1/6 & 1/6 \\
0 & 0 & 0 & 0 & 5/6 & 1/6  \\
0 & 0 & 0 & 0 & 0 & 6/6 
\end{pmatrix} \]


\section{QUESTION 4}

We have

\[ S_{n+1} = \begin{cases} S_{n} + 1  & \text{ with probability } p  \\ S_{n} - 1& \text{ with probability } q \end{cases} \]

and $ S_{0} = 0 $.

Let $ X_{n} = | S_{n} | $

Clearly $ X_{n} $ satisfies the Markov property (where the walk is now only depends on where it was one step before), so $ X $ is a Markov Chain with initial state $ X_{0} = 0 $. The transition probabilities are given by the transition matrix

\[ \begin{pmatrix}
0 & 1 & 0 & 0 & \cdots \\
q & 0 & p & 0 & \cdots \\
0 & q & 0 & p & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix} \]

Define $ M_{k} := \max \{ S_{k} \; | \; 0 \leq k \leq n \} $, and $ Y_{n} = M_{n} - S_{n} $

Not sure about this part.

\section{QUESTION 5}

$ X $ is a Markov chain, thus

\[ \P( X_{n+1} = i_{n+1} \; | \; X_{0} = i_{0}, \cdots, X_{n} = i_{n} ) = \P(X_{n+1} = i_{n+1} \; | \; X_{n} = i_{n} ) \]

And so

\begin{align*}
\P(Y_{r+1}  \; | \; Y_{0},\cdots,Y_{r} ) & = \P(X_{n_{r+1}} \; | \; X_{n_{0}},\cdots,X_{n_{r}}  ) \\
\end{align*}

If $ n_{r} = 2r $ and $ X $ is a simple random walk on $ Z $ st. 

\[ X_{n+1} = \begin{cases} X_{n} + 1 & \text{ with probability } p \\ X_{n} - 1 & \text{ with probability } q \end{cases} \] 

the allowed states are $ \{ \cdots,-2,-1,0,1,2,\cdots \} $. Of course, the probability of finding oneself in an odd state is always zero (we are always taking an even number of states).

It is difficult to visualise the state transition matrix as the state space is infinite, but a general row has the form

\[ \begin{pmatrix}
\cdots & 0 & q^{2} & 0 & 2pq & 0 & p^{2} & \cdots
\end{pmatrix} \]

With probability $ 2pq $ of returning to the current state, $ p^{2} $ moving two to the right and $ q^{2} $ moving two to the left. (Thus rows summing to $ (p + q)^{2} = 1 $)




\section{QUESTION 6}

I think the answer should be no. $ X $ and $ Y $ are both Markov chains, so the value of $ X_{n+1} + Y_{n+1} $ only depends on $ X_{n} $, and $ Y_{n} $. But we only have the value of $ X_{n} + Y_{n} $ (loss of information).

For an explicit counterexample, try to smuggle information about $ X_{1} $ into $ Y $, so that $ X_{n} + Y_{n} $ depends on $ X_{1} $ (contradicting the Markov property) but $ X_{n} $ and $ Y_{n} $ are both Markov.

Idea: Let $ X $ be a random walk on the integers, so $ X_{1} = \pm 1 $ with some probabilities. Define $ Y_{n} $ as a symmetric `boosted' random walk:
                                      
\[ Y_{n} = \begin{cases} Y_{n-1} + 1 + 3 X_{1}   & \text{ with probability } 1/2 \\  Y_{n-1} - 1 + 3 X_{1} & \text{ with probability } 1/2 \end{cases} \]

Then $ X_{n} + Y_{n}  $ depends on $ X_{1} $, not just $ X_{n-1},Y_{n-1} $


\section{QUESTION 7}

\begin{enumerate}
	\item Let the probability of the flea being on the original vertex after $ n $ hops be $ p_{n} $. Then

\[ p_{n} = \begin{cases} 0  & \text{ if flea on first vertex after } n-1 \text{ hops}  \\ \frac{1}{2} & \text{ otherwise } \end{cases} \]

Hence we set up the recurrence, with $ p_{0} = 1 $

\begin{align*}
p_{n} & = 0 \cdot p_{n-1} + \frac{1}{2} (1 - p_{n-1})  \\
& = \frac{1}{2} (1 - p_{n-1}) \\
\end{align*}

Solving this difference equation gives general solution $ p_{n} = \frac{1}{3} + \frac{2}{3}\left( - \frac{1}{2} \right)^{n}   $.

\item Let $ p_{n} $ be defined as before. This can be though of as a Markov chain on the state space of triangle vertices $ \{1,2,3\} $, with transition matrix

\[ P = \begin{pmatrix}
0 & \frac{2}{3} & \frac{1}{3} \\
\frac{1}{3} & 0 & \frac{2}{3} \\
\frac{2}{3} & \frac{1}{3} & 0
\end{pmatrix} \]
	
Given $ p_{1,1}(0) = 1 $ (the flea starts on vertex $ 1 $ ) we want to find $ p_{1,1}(n) $ The eigenvalues $ \kappa $ of $ P $ are the roots of the equation $ \det(P - \kappa I) $, with solutions

\[ \kappa_{1} = 1, \quad \kappa_{2}, \kappa_{3} = - \frac{1}{2} \pm \frac{i}{2\sqrt{3}} \]	



Therefore

\[ P = U^{-1} \begin{pmatrix}
1 & 0 & 0 \\
0 & \kappa_{2} & 0 \\
0 & 0 & \kappa_{3}
\end{pmatrix} U \]

for some invertible matrix $ U $. It follows that

\[ P^{n} = U^{-1} \begin{pmatrix}
1 & 0 & 0 \\
0 & \kappa_{2}^{n} & 0 \\
0 & 0 & \kappa_{3}^{n}
\end{pmatrix} U  \]

Now note that $ \kappa_{2},\kappa_{3} = \frac{1}{\sqrt{3}} e^{\pm i \pi / 6}  $, so 

\begin{align*}
 p_{1,1}(n) & = A + B e^{i \pi n / 6} + C e^{- i \pi n / 6}  \\
& = A + B' \left(  3^{-1/2} \right)^{n}  \cos( \pi n / 6) + C' \left(  3^{-1/2} \right)^{n} \sin ( \pi n / 6)
\end{align*}

Using the b.c.s,

$ p_{1,1}(0) = 1, p_{1,1}(1) = 0, p_{1,1}(2) = 4/9 $ and hence

\[ p_{1,1}(n) = \frac{1}{3} + \frac{2}{3} \left(  3^{-1/2} \right)^{n} \cos (\pi n / 6)  \]





\end{enumerate}





\section{QUESTION 8}

Let $ X_{n} $ be the score of the die on the $ n^{\text{th}} $ roll; this is a Markov Chain. Define $ Y_{n} $ as

\[ Y_{n} = \begin{cases} 1  & \text{ if } X_{n} \neq 6 \\ 2 & \text{ if } X_{n} = 6 \end{cases} \]

This has transition matrix

\[ P = \begin{pmatrix}
\frac{4}{5} & \frac{1}{5} \\
1 & 0
\end{pmatrix} \]

We wish to find $ P(Y_{n} = 1 \; | \; Y_{1} = 1) $. Diagonalising $ P $, we have

\[ P^{n} = U^{-1}  \begin{pmatrix}
1 & 0 \\
0 & (-1/5)^{n}
\end{pmatrix} U  \]

for some invertible matrix $ U $, hence

\[ p_{2,2}(n) = A + B \left( - \frac{1}{5} \right)^{n}  \]


for probability of 6 on the $ n^{\text{th}} $  throw.  Given $ p_{2,2}(0) = 1 $ and $ p_{2,2}(1) = 0 $, this yields


\[ p_{1,1}(n) = - \frac{1}{4} + \frac{5}{4} \left( - \frac{1}{5} \right)^{n} \]

Then $ p_{2,1}(n) = 1 - p_{2,2}(n) = 1/4  - (5/4) ( 5^{-1})^{n} $

Next part: Then all other scores have probability $ 1/4 $, and transition matrix becomes 

\[ P = \begin{pmatrix}
0 & 0 & 1/4 & 1/4 & 1/4 & 1/4 \\
1/4 & 0 & 0 & 1/4 & 1/4 & 1/4 \\
1/4 & 1/4 & 0 & 0 & 1/4 & 1/4 \\
1/4 & 1/4 & 1/4 & 0 & 0 & 1/4 \\
1/4 & 1/4 & 1/4 & 1/4 & 0 & 0 \\
0 & 1/4 & 1/4 & 1/4 & 1/4 & 0
\end{pmatrix} \]


I can't spot a way to simplify this. Want $ \{ 1,2,3,4 \}, \{ 5 \}, \{ 6 \}$ as states but not sure what the first row of the transition matrix is. 
\section{QUESTION 9}


\begin{enumerate}[label = (\alph*)]
	\item For $ p = 1/16 $, the characteristic equation $ \det(P - \kappa \iota) = 0 $  gives roots $ \kappa_{1} = 1 $, $ \kappa_{2} = - 1/4 $, $ \kappa_{3} = - 1/12 $.
	
	Thus
	
	\[ P^{n} = U^{-1}  \begin{pmatrix}
	1 & 0 & 0 \\
	0 & -1/4 & 0 \\
	0 & 0 & - 1/12 
	\end{pmatrix} U  \]
	
	for some invertible matrix $ U $, and so
	
	\[ p_{1,1}(n) = A + B\left(  - \frac{1}{4} \right)^{n} + C \left(  - \frac{1}{12} \right)^{n}  \]
	
	Using $ p_{1,1}(0) = 1 $, $ p_{1,1}(1) = 0 $, $ p_{1,1}(2) = 0 $
	
	This gives $ A = 1/65 $, $ B = - 2/5 $ and $ C = 18/13 $
	
	\item For $ p = 1/6 $, the characteristic equation $ \det(P - \kappa \iota) = 0 $  gives roots $ \kappa_{1} = 1 $, $ \kappa_{2}, \kappa_{3} = -1/6 \pm i/6 = \frac{1}{\sqrt{18}} e^{\pm i \pi / 4} $, so similarly,
	
	\[ p_{1,1}(n) = A + \left(  18^{-1/2} \right)^{n} B \cos ( \pi n / 4) + \left(  18^{-1/2} \right)^{n} C \sin ( \pi n / 4)    \]
	
	yielding $ A = 1/13,  B = 12/13 $, $ C = - 18/13 $
 	
	
\end{enumerate}




\section{QUESTION 10}

Possible transitions of the chain are illustrated below:
  \begin{center}
	\begin{tikzpicture}
	\node [mstate] (1) at (0, 0) {$1$};
	\node [mstate] (5) at (2, 0) {$5$};
	\node [mstate] (3) at (3, -1.4) {$3$};
	\node [mstate] (4) at (4, 0) {$4$};
	\node [mstate] (2) at (6, 0) {$2$};
	\draw (1) edge [loop below, ->] (1);
	\draw (2) edge [bend left, ->] (4);
	\draw (4) edge [bend left, ->] (2);
	\draw (4) edge [loop above, ->] (4);
	\draw (5) edge [loop below, ->] (6);
	\draw (1) edge [bend left, ->] (5);
	\draw (5) edge [bend left, ->] (1);
	\draw (4) edge [->] (5);
	\draw (4) edge [->] (3);
	\draw (3) edge [loop below, ->] (3);
	\end{tikzpicture}
\end{center}

The communicating classes are $ C_{1} = \{ 1,5 \}, C_{2} = \{ 3 \} $ and $ C_{3} = \{ 2,4 \} $. The classes $ C_{1} $ and $ C_{3} $ are not closed, but $ C_{2} $ is closed.

\section{QUESTION 11}

Suppose $ S $ contains no closed communicating classes. 

Pick some $ i_{0} \in S $. If $ i $ is a singleton communicating class, then $ \{i_{0} \} $ is closed. So $ \exists \; i_{1} \in S $ st. $ i_{0} \to i_{1} $. Similarly, $ \exists \; i_{2} \in S $ st. $ i_{1} \to i_{2} $. ($ i_{2} \neq i_{0} $, or $ \{i_{0},i_{1}\} $ is a closed communicating class). Continuing in this fashion, we eventually hit the `last' element in the state space $ i_{n} $, as it is finite. Then $ S = \{ i_{0},\cdots,i_{n} \} $ itself is a closed communicating class.

For a transition matrix with no communicating classes a simple example is:

\[ \begin{pmatrix}
0 & 1 & 0 & 0 &\cdots \\
0 & 0 & 1 & 0 & \cdots \\
0 & 0 & 0 & 1 & \ddots \\
\vdots &&\vdots && \ddots 
\end{pmatrix} \]

ie. $ i_{0} \to i_{1} \to i_{2} \to \cdots $


\section{QUESTION 12}

If $ X_{0} = 2 $, following the strategy we see the state space produced is 

$ \{  0,2,4,6,8,10 \} $. The transition matrix $ P $ is given by


\[ P = \begin{pmatrix}
1 & 0 & 0 & 0 & 0 & 0\\
1/2 & 0 & 1/2 & 0 & 0 & 0 \\
1/2 & 0 & 0 & 0 & 1/2 & 0 \\
0 & 1/2 & 0 & 0 & 0 & 1/2\\
0 & 0 & 0 & 1/2 & 0 & 1/2 \\
0 & 0 & 0 & 0 & 0 & 1

\end{pmatrix} \]

With $ \{0\} $ and $ \{10\} $ being absorbing states.
Want to show $ \P (  X_{n} = 10 | X_{0} = 2 ) = 1/5  $

Not sure about this bit.




\end{document}