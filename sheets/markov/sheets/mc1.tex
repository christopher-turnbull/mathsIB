\documentclass[a4paper]{article}
\usepackage{amsmath}
\def\npart {IA}
\def\nterm {Michaelmas}
\def\nyear {2017}
\def\nlecturer {Prof Weber (rrw1@cam.ac.uk)}
\def\ncourse {Markov Chains Example Sheet 1}

\input{header}

\newtheorem*{soln}{Solution}

\renewcommand{\thesection}{}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\makeatletter
\def\@seccntformat#1{\csname #1ignore\expandafter\endcsname\csname the#1\endcsname\quad}
\let\sectionignore\@gobbletwo
\let\latex@numberline\numberline
\def\numberline#1{\if\relax#1\relax\else\latex@numberline{#1}\fi}
\makeatother


\begin{document}
	
\maketitle

\section{QUESTION 1}

$ X = (X_{n}) $ is a Markov Chain, and so satisfies the Markov Property

\[ \P( X_{n+1} = i_{n+1} \; | \; X_{0} = i_{0}, \cdots, X_{n} = i_{n} ) = \P(X_{n+1} = i_{n+1} \; | \; X_{n} = i_{n} ) \]

Given that $ Z_{k} = X_{m+k} $, with $ k \geq 0 $, want to show that $ Z_{k} $ is a Markov Chain, ie.

\[ \P(Z_{n+1}  \; | \; Z_{0},\cdots,Z_{n} ) = \P(Z_{n+1} \; | \; Z_{n} ) \qquad (*) \]

Substituting in our definition of $ Z_{k} $,

\begin{align*}
\P(Z_{n+1}  \; | \; Z_{0},\cdots,Z_{n} ) & = \P(X_{m+n+1}|X_{m},\cdots,X_{m+n}) \\
& = \P(X_{m+n+1} | X_{m+n}) \qquad \text{ as } X \text{ is Markov}\\
& = \P(Z_{n+1} \; | \; Z_{n} )
\end{align*}

Hence $ Z_{k} $ satisfies the Markov property, and is thus a Markov Chain.

$ Z_{0} = X_{m} = i $, so $ Z_{k} $ has starting state $ i $.

\section{QUESTION 2}

Let $ X_{1},\cdots,X_{n} $ be independent random variables, so $ \P(X_{n+1} = i_{n+1} \; | \; X_{n} = i_{n} ) = \P(X_{n+1} = i_{n+1}  ) $. But then 

\[ \P( X_{n+1} = i_{n+1} \; | \; X_{0} = i_{0}, \cdots, X_{n} = i_{n} ) = \P(X_{n+1} = i_{n+1}) \] 

also by independence, so Markov property is satisfied.

Homogeneous if $ \P(X_{n+1} = i_{n+1} \; | \; X_{n} = i_{n}) $ does not depend on the value of $ n $, so must have $ X_{n} $ identically distributed. 



\section{QUESTION 3}

As $ X_{n} $ is the \emph{maximum} reading obtained after $ n $ throws, this depends only on the last maximum, so $ X_{n} $ is a Markov chain. In particular, the state transition matrix is


\[ \begin{pmatrix}
1/6 & 1/6 & 1/6 & 1/6 & 1/6 & 1/6  \\
0 & 2/6 & 1/6 & 1/6 & 1/6 & 1/6  \\
0 & 0 & 3/6 & 1/6 & 1/6 & 1/6  \\
0 & 0 & 0 & 4/6 & 1/6 & 1/6 \\
0 & 0 & 0 & 0 & 5/6 & 1/6  \\
0 & 0 & 0 & 0 & 0 & 6/6 
\end{pmatrix} \]


Now note that

\[ p_{i,j}(n) = \begin{cases} 0  & \text{ if } j < i \\ \left( \frac{1}{6} \right)^{n}  & \text{ if } j = i \\
\left(  \frac{j}{6} \right)^{n} - \left(  \frac{j-1}{6} \right)^{n} & \text{ if} j > i   \end{cases} \]


\section{QUESTION 4}

We have

\[ S_{n+1} = \begin{cases} S_{n} + 1  & \text{ with probability } p  \\ S_{n} - 1& \text{ with probability } q \end{cases} \]

and $ S_{0} = 0 $.

To show that $ X_{n} $ is a Markov chain, consider the probability 

\[ \P (  | S_{n+1} | = i + 1 \; | \; |  S_{n} | = i, H ) \]

for some past event $ H $. Conditioning, this is equal to

\begin{align*}
& = \underbrace{\P (  | S_{n+1} | = i + 1 \; | \;   S_{n} = i )}_{p} \times \underbrace{\P (  S_{n} = i \; | \ ; |  S_{n} | = i, H)}_{\pi_{1}} \\
& \quad + \underbrace{\P (  | S_{n+1} | = i + 1 \; | \;   S_{n} = - i )}_{q} \times \underbrace{\P (  S_{n} = - i \; | \ ; |  S_{n} | = i, H)}_{\pi_{2}} 
\end{align*}

Note that

\begin{align*}
\frac{\pi_{1}}{\pi_{2}} & = \frac{  p^{n/2 + i/2} q^{n/2 - i/2}  }{  q^{n/2 + i/2} p^{n/2 - i/2} } \\
& = \frac{p^{i}}{q^{i}}
\end{align*}

Hence $ \pi_{1} = \frac{p^{i}}{p^{i} + q^{i}} $



Define $ M_{k} := \max \{ S_{k} \; | \; 0 \leq k \leq n \} $, and $ Y_{n} = M_{n} - S_{n} $


\begin{center}
	\begin{tikzpicture}
	\draw (-3, 0) -- (3, 0);
	\draw node at (1,0) [below] {$ S_{n} $};
	\draw node at (2.5,0) [below] {$ M_{n} $};
	\end{tikzpicture}
\end{center}

with the distance between $ S_{n} $ and $ M_{n} $ being $ i $, we have:

\[  p_{ij} = \begin{cases}   i-1 & \text{ with probability } p \\ i + 1 & \text{ with probability } q \end{cases} \]

\[  p_{0j} = \begin{cases}   0 & \text{ with probability } p \\  1 & \text{ with probability } q \end{cases} \]

\section{QUESTION 5}

$ X $ is a Markov chain, thus

\[ \P( X_{n+1} = i_{n+1} \; | \; X_{0} = i_{0}, \cdots, X_{n} = i_{n} ) = \P(X_{n+1} = i_{n+1} \; | \; X_{n} = i_{n} ) \]

And so

\begin{align*}
\P(Y_{r+1}  \; | \; Y_{0},\cdots,Y_{r} ) & = \P(X_{n_{r+1}} \; | \; X_{n_{0}},\cdots,X_{n_{r}}  ) \\
\end{align*}

If $ n_{r} = 2r $ and $ X $ is a simple random walk on $ Z $ st. 

\[ X_{n+1} = \begin{cases} X_{n} + 1 & \text{ with probability } p \\ X_{n} - 1 & \text{ with probability } q \end{cases} \] 

the allowed states are $ \{ \cdots,-2,-1,0,1,2,\cdots \} $. Of course, the probability of finding oneself in an odd state is always zero (we are always taking an even number of states).

It is difficult to visualise the state transition matrix as the state space is infinite, but a general row has the form

\[ \begin{pmatrix}
\cdots & 0 & q^{2} & 0 & 2pq & 0 & p^{2} & \cdots
\end{pmatrix} \]

With probability $ 2pq $ of returning to the current state, $ p^{2} $ moving two to the right and $ q^{2} $ moving two to the left. (Thus rows summing to $ (p + q)^{2} = 1 $)




\section{QUESTION 6}

I think the answer should be no. $ X $ and $ Y $ are both Markov chains, so the value of $ X_{n+1} + Y_{n+1} $ only depends on $ X_{n} $, and $ Y_{n} $. But we only have the value of $ X_{n} + Y_{n} $ (loss of information).

For an explicit counterexample, try to smuggle information about $ X_{1} $ into $ Y $, so that $ X_{n} + Y_{n} $ depends on $ X_{1} $ (contradicting the Markov property) but $ X_{n} $ and $ Y_{n} $ are both Markov.

Idea: Let $ X $ be a random walk on the integers, so $ X_{1} = \pm 1 $ with some probabilities. Define $ Y_{n} $ as a symmetric `boosted' random walk:
                                      
\[ Y_{n} = \begin{cases} Y_{n-1} + 1 + 3 X_{1}   & \text{ with probability } 1/2 \\  Y_{n-1} - 1 + 3 X_{1} & \text{ with probability } 1/2 \end{cases} \]

Then $ X_{n} + Y_{n}  $ depends on $ X_{1} $, not just $ X_{n-1},Y_{n-1} $


Note that we can also make use of $ M_{n} $ as defined in question 4. 

\[ \P (M_{3} = 2 \; |  \; M_{2} = 1,M_{1} = 1 ) = 0  \]
\[ \P(M_{3} = 2 \; | \; M_{2} = 1,M_{1} = 0 ) = \frac{1}{2} \]

So $ M_{n} $ not M.C.

\section{QUESTION 7}

\begin{enumerate}
	\item Let the probability of the flea being on the original vertex after $ n $ hops be $ p_{n} $. Then

\[ p_{n} = \begin{cases} 0  & \text{ if flea on first vertex after } n-1 \text{ hops}  \\ \frac{1}{2} & \text{ otherwise } \end{cases} \]

Hence we set up the recurrence, with $ p_{0} = 1 $

\begin{align*}
p_{n} & = 0 \cdot p_{n-1} + \frac{1}{2} (1 - p_{n-1})  \\
& = \frac{1}{2} (1 - p_{n-1}) \\
\end{align*}

Solving this difference equation gives general solution $ p_{n} = \frac{1}{3} + \frac{2}{3}\left( - \frac{1}{2} \right)^{n}   $.

\item Let $ p_{n} $ be defined as before. This can be though of as a Markov chain on the state space of triangle vertices $ \{1,2,3\} $, with transition matrix

\[ P = \begin{pmatrix}
0 & \frac{2}{3} & \frac{1}{3} \\
\frac{1}{3} & 0 & \frac{2}{3} \\
\frac{2}{3} & \frac{1}{3} & 0
\end{pmatrix} \]
	
Given $ p_{1,1}(0) = 1 $ (the flea starts on vertex $ 1 $ ) we want to find $ p_{1,1}(n) $ The eigenvalues $ \kappa $ of $ P $ are the roots of the equation $ \det(P - \kappa I) $, with solutions

\[ \kappa_{1} = 1, \quad \kappa_{2}, \kappa_{3} = - \frac{1}{2} \pm \frac{i}{2\sqrt{3}} \]	



Therefore

\[ P = U^{-1} \begin{pmatrix}
1 & 0 & 0 \\
0 & \kappa_{2} & 0 \\
0 & 0 & \kappa_{3}
\end{pmatrix} U \]

for some invertible matrix $ U $. It follows that

\[ P^{n} = U^{-1} \begin{pmatrix}
1 & 0 & 0 \\
0 & \kappa_{2}^{n} & 0 \\
0 & 0 & \kappa_{3}^{n}
\end{pmatrix} U  \]

Now note that $ \kappa_{2},\kappa_{3} = \frac{1}{\sqrt{3}} e^{\pm i \pi / 6}  $, so 

\begin{align*}
 p_{1,1}(n) & = A + B e^{i \pi n / 6} + C e^{- i \pi n / 6}  \\
& = A + B' \left(  3^{-1/2} \right)^{n}  \cos( \pi n / 6) + C' \left(  3^{-1/2} \right)^{n} \sin ( \pi n / 6)
\end{align*}

Using the b.c.s,

$ p_{1,1}(0) = 1, p_{1,1}(1) = 0, p_{1,1}(2) = 4/9 $ and hence

\[ p_{1,1}(n) = \frac{1}{3} + \frac{2}{3} \left(  3^{-1/2} \right)^{n} \cos (\pi n / 6)  \]





\end{enumerate}





\section{QUESTION 8}


Note that

\[ p_{66}(n-1) = \frac{1}{5} (1 - p_{66}(n-2)    ) \]

Hence,

\[ p = \frac{1}{6} + \frac{5}{6}\left(  - \frac{1}{2} \right)^{n-1}   \]


For the next part, let $ Y_{n} $ denote the die now, and this is related to the previous die by

\[ Y_{n} - Y_{n-1} = X_{n} - X_{n-1} \quad (\text{mod } 6)   \]

Thus

\[ Y_{n}  = X_{n} + (n-1) \quad \text{mod } 6  \]

Thus $ n-1 = 0 $ mod 6 $ \Rightarrow  p_{66} (n-1) = p   $, and
$ n -1 \neq 0  $ mod 6 $ \Rightarrow \frac{1}{5} (1-p)  $




\section{QUESTION 9}


\begin{enumerate}[label = (\alph*)]
	\item For $ p = 1/16 $, the characteristic equation $ \det(P - \kappa \iota) = 0 $  gives roots $ \kappa_{1} = 1 $, $ \kappa_{2} = - 1/4 $, $ \kappa_{3} = - 1/12 $.
	
	Thus
	
	\[ P^{n} = U^{-1}  \begin{pmatrix}
	1 & 0 & 0 \\
	0 & (-1/4)^{n} & 0 \\
	0 & 0 & (- 1/12)^{n} 
	\end{pmatrix} U  \]
	
	for some invertible matrix $ U $, and so 
	
	\[ p_{1,1}(n) = A + B\left(  - \frac{1}{4} \right)^{n} + C \left(  - \frac{1}{12} \right)^{n}  \]
	
	Using $ p_{1,1}(0) = 1 $, $ p_{1,1}(1) = 0 $, $ p_{1,1}(2) = 0 $
	
	This gives $ A = 1/65 $, $ B = - 2/5 $ and $ C = 18/13 $
	
	\item (Check, this is incorrect)  For $ p = 1/6 $, the characteristic equation $ \det(P - \kappa \iota) = 0 $  gives roots $ \kappa_{1} = 1 $, $ \kappa_{2}, \kappa_{3} = -1/6 \pm i/6 = \frac{1}{\sqrt{18}} e^{\pm i \pi / 4} $, so similarly,
	
	\[ p_{1,1}(n) = A + \left(  18^{-1/2} \right)^{n} B \cos ( \pi n / 4) + \left(  18^{-1/2} \right)^{n} C \sin ( \pi n / 4)    \]
	
	yielding $ A = 1/13,  B = 12/13 $, $ C = - 18/13 $
	
	\item 		\[ P^{n} = U^{-1}  \begin{pmatrix}
	1 & 0 & 0 \\
	0 & (-1/6)^{n} & 0 \\
	0 & 0 & (- 1/6)^{n} 
	\end{pmatrix} U  \]
	
	Hence, $ p_{1,1}(n) = A + B \left(  - \frac{1}{6} \right)^{n} + C n \left(  - \frac{1}{6} \right)^{n}     $
	
	Check, should get
	
	\[ \frac{1}{49} + \left(  - \frac{1}{6} \right)^{n} \left(  \frac{48}{49}  - \frac{6}{7} n  \right)    \]
	

	
	
 	
	
\end{enumerate}




\section{QUESTION 10}

Possible transitions of the chain are illustrated below:
  \begin{center}
	\begin{tikzpicture}
	\node [mstate] (1) at (0, 0) {$1$};
	\node [mstate] (5) at (2, 0) {$5$};
	\node [mstate] (3) at (3, -1.4) {$3$};
	\node [mstate] (4) at (4, 0) {$4$};
	\node [mstate] (2) at (6, 0) {$2$};
	\draw (1) edge [loop below, ->] (1);
	\draw (2) edge [bend left, ->] (4);
	\draw (4) edge [bend left, ->] (2);
	\draw (4) edge [loop above, ->] (4);
	\draw (5) edge [loop below, ->] (6);
	\draw (1) edge [bend left, ->] (5);
	\draw (5) edge [bend left, ->] (1);
	\draw (4) edge [->] (5);
	\draw (4) edge [->] (3);
	\draw (3) edge [loop below, ->] (3);
	\end{tikzpicture}
\end{center}

The communicating classes are $ C_{1} = \{ 1,5 \}, C_{2} = \{ 3 \} $ and $ C_{3} = \{ 2,4 \} $. The classes $ C_{1} $ and $ C_{3} $ are not closed, but $ C_{2} $ is closed.

\section{QUESTION 11}

Suppose $ S $ contains no closed communicating classes. 

Pick some $ i_{0} \in S $. If $ i $ is a singleton communicating class, then $ \{i_{0} \} $ is closed. So $ \exists \; i_{1} \in S $ st. $ i_{0} \to i_{1} $. Similarly, $ \exists \; i_{2} \in S $ st. $ i_{1} \to i_{2} $. ($ i_{2} \neq i_{0} $, or $ \{i_{0},i_{1}\} $ is a closed communicating class). Continuing in this fashion, we eventually hit the `last' element in the state space $ i_{n} $, as it is finite. Then $ S = \{ i_{0},\cdots,i_{n} \} $ itself is a closed communicating class.

For a transition matrix with no communicating classes a simple example is:

\[ \begin{pmatrix}
0 & 1 & 0 & 0 &\cdots \\
0 & 0 & 1 & 0 & \cdots \\
0 & 0 & 0 & 1 & \ddots \\
\vdots &&\vdots && \ddots 
\end{pmatrix} \]

ie. $ i_{0} \to i_{1} \to i_{2} \to \cdots $


\section{QUESTION 12}

See Weber's Right Hand Equations

$ \alpha = P \alpha $, $ k = 1 + P k $

\begin{align*}
\alpha_{2} & =  \frac{1}{2} \cdot 0 + \frac{1}{2} \alpha_{4}  \\
\alpha_{4} & =  \frac{1}{2} \cdot 0 + \frac{1}{2} \alpha_{8}  \\
\alpha_{6} & =  \frac{1}{2} \alpha_{6} + \frac{1}{2} 1  \\
\alpha_{8} & =  \frac{1}{2} \alpha_{2} + \frac{1}{2} 1  \\
\alpha_{2} & =  \frac{1}{2}\left(   \frac{1}{2} \left(  \frac{1}{2} + \frac{1}{2} \left(    \frac{1}{2} + \frac{1}{2} \alpha_{2} \right)   \right)  \right)   \\
& = \frac{1}{8} + \frac{1}{16} + \frac{1}{16} \alpha_{2}
\end{align*}

Hence \[ \frac{15}{16} \alpha_{2} = \frac{3}{16} \qquad \alpha_{2} = \frac{1}{5}  \]

Could also argue it has to be $ 1/5 $ since it's a fair game, and the placement of $ 2 $ between 0 and 10.

Similarly, 

\begin{align*}
k_{2} & = 1 + \frac{1}{2} \cdot 0 + \frac{1}{2} k_{4} \\
k_{4} & = 1 + \frac{1}{2} \cdot 0 + \frac{1}{2} k_{8}
\end{align*}

But since $ k_{6} = k_{4} $, and $ k_{8} = k_{2} $, we have 

\[ k_{2} = 1 + \frac{1}{2} \left(  1 + \frac{1}{2} k_{2} \right)  \]

and we recover $ k_{2} = 2 $, as expected. 

\section{QUESTION 13}


$ \alpha_{0} = 1 $

\[ \alpha_{i}  = \frac{(i+1)^{2}}{i^{2}  + (i + 1)^{2} } \alpha_{i+1} + \frac{i^{2}}{i^{2}  + (i + 1)^{2}  } \alpha_{i-1}   \]

This is equivalent to

\[ (i+1)^{2}  (\alpha_{i+1}  - a_{i} ) = i^{2} ( \alpha_{i} - \alpha_{i-1})  \]

Iterating down,

\[ \cdots= 1^{2}  ( \alpha_{1} - \alpha_{0})  = \alpha_{1} - 1  \]


ie.

\[ \alpha_{i+1} - \alpha_{i} = \frac{1}{(i+1)^{2}}(\alpha_{1} - 1) \]

Summing,

\[ \alpha_{k+1} - \alpha_{0} = \left[ \frac{1}{1^{2}}  + \cdots ? \frac{1}{(k+1)^{2}}\right]  (\alpha_{1} - 1) \]

The result follows, taking $ k \to \infty $.



\section{QUESTION 14}


Again, expand on the right side,

\begin{align*}
\phi(s) & = \E s^{H_{0}} \\
& = \frac{1}{2} s^{1} + \frac{1}{2} s^{1} \underbrace{\E s^{H_{0}' + H_{0}''}}_{= \phi(s)^{2}}
\end{align*}

where $ H_{0}',H_{0}'' $ are independent variables.

\[ \phi(s) = \frac{1 \pm \sqrt{1 - s^{2}}}{s} \]

where we pick the $ - $ from the $ \pm $, as terms in the expansion need to be positive. 

Thus

\[ \phi(s) = \frac{s}{2}+\frac{s^3}{8}+\frac{s^5}{16}+O\left(s^6\right) \]

Next we have the same idea,

\begin{align*}
\phi(s) & = \frac{1}{2} s^{1} + \frac{1}{2} s^{1}\phi(s)^{3}  \\
\end{align*}

Can use mathematica to solve this. The smallest root of this equation, interestingly, is the golden mean!





\end{document}